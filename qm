\section{Quantum Mechanics}
\subsection{Polarisation of light}
We will start our discussion of quantum mechanics in a slightly unusual place: The polarisation of classical light.
We will see that this reproduces a lot of the maths of quantum mechanics in an entirely classical context.

A scalar plane wave (examples for scalar waves are sound waves in a gas) has the general equation
$$p = p_0 \cos(\vec k \cdot \vec r - \omega t + \phi_0).$$
For each value of $\vec k$ there is one type of plane wave (modulo phase and amplitude), hence there is no polarisation.
It is helpful to write the equation in complex notation as
$$p = p_0 e^{\im (\vec k \cdot \vec r - \omega t)},$$
where we implicitly take the real part to get a physical quantity.
Note how this absorbs $\phi_0$ into the now complex $p_0$.

We can now generalise the equation for a vector wave, such as light, as
$$\vec E = \vec E_0 e^{\im (\vec k \cdot \vec r - \omega t)}.$$
Naively we can now form three types of plane waves (one for each component of $\vec E_0$), but there is a further constraint from electrodynamics that states that $\vec k \cdot \vec E = 0$ (in quantum terms this constraint will come from the fact that the photon is massless).
This eliminates the polarisation in the direction of propagation (the ``longitudinal'' mode).

If we pick two vectors $\vec E_1$ and $\vec E_2$ that are equal in magnitude, perpendicular to each other and to $\vec k$, then we can write
$$\vec E_0 = J_1 \vec E_1 + J_2 \vec E_2.$$
The two components of $J$ now form a 2D vector, called the \emph{Jones vector} $\ket{J} = (J_1, J_2)$.
The unconventional notation $\ket{J}$ for a vector is called \emph{Dirac} notation and it's pervasive throughout quantum mechanics (I'm introducing it now to acquaint you with the notation; right now it's really just strange notation for 2D vectors).
A vector $\ket{J}$ is also called a \emph{ket}.
For each ket there is also a \emph{bra} $\bra{J} = ({J_1}^*, {J_2}^*)$.
Note that in matrix notation $\ket{J}$ is a column vector, whereas $\bra{J}$ is a row vector.
We can thus form scalar products
$$\braket{J}{K} = \bra{J} \ket{K} = {J_1}^* K_1 + {J_2}^* K_2,$$
where two adjacent bars can be merged into one.
The operation of converting a ket into a bra is given the symbol ${}^\dagger$ and called \emph{Hermitian conjugation}.
In matrix notation it's equivalent to transposing, followed by complex conjugation.
From
$$(\braket{J}{K})^\dagger = \braket{K}{J} = (\ket{K})^\dagger (\bra{J})^\dagger$$
we see that $(ab)^\dagger = b^\dagger a^\dagger$.

\begin{question}
What kind of object is $\ket{K} \bra{J}$? (It's not a scalar.)
What are its components?
\hint{Try applying bra or ket vectors to the appropriate side.}
\solution{
$$(\ket{K} \bra{J}) \ket{L} = \ket{K} \braket{J}{L}$$
is a ket and
$$\bra{ L} (\ket{K} \bra{J}) = \braket{L}{K} \bra{J}$$
is a bra.
This implies that $\ket{K} \bra{J}$ is a matrix.
In fact, from the matrix definition as $\ket{K}$ as a row and $\bra{J}$ as a column we see, using matrix multiplication, that
$$\ket{K} \bra{J} = \begin{pmatrix}{K_1}^* J_1 & {K_2}^* J_1\\{K_1}^* J_2 & {K_2}^* J_2\end{pmatrix}.$$
}
\end{question}

Given $\ket{J} = (J_1, J_2)$ we will call $J_1$ the horizontal and $J_2$ the vertical component.
If $\vec E_1$ and $\vec E_2$ have the dimension of $\vec E$, then $\ket{J}$ is dimensionless.
The magnitude squared $\braket{ J}{J }$ is still proportional to intensity, but we can redefine the constant of proportionality arbitrarily.
Here we don't really care about absolute intensity and we will just call $\braket{ J}{J }$ the intensity.


Note how $\ket{J}$ evolves with a factor $e^{\im (\vec k \cdot \vec r - \omega t)}$ and we can thus treat $\ket{J}$ and $e^{\im\phi} \ket{J}$ as equivalent -- they correspond to the same polarisation state, just at a different point in space and time.

\begin{question}
What Jones vector $\ket{\theta}$ corresponds to light linearly polarised at an angle $\theta$ (to the horizontal axis)?
\result{$(\cos \theta, \sin \theta)$.}
\end{question}

Technically the text between $|$ and $\rangle$ is an uninterpreted label and so something like $\ket{\theta+\pi}$ should be considered entirely unrelated to $\ket{\theta}$.
However, insisting on this is cumbersome and we will usually use $\ket{\theta+\pi}$ to mean $(\cos(\theta+\pi), \sin(\theta+\pi))$.
A better notation, perhaps, would be $\ket{\theta=\theta+\pi}$, but the shorter, ambiguous notation is very common in the literature.
We will also define $\ket{\rm H}$ and $\ket{\rm V}$ for the $\theta=0$ and $\theta=\pi/2$ special cases, i.e.\ purely horizontal and purely vertical light.
Here there is no ambiguity because $\rm H$ is just a label and not a variable and $\ket{{\rm H}^2}$, etc., can be unambiguously considered as an entirely unrelated vector.

Now we can define operations on light -- such as polarisation filters (polaroids) -- in terms of $2 \times 2$-matrices.
\begin{question}
Write down the matrix and a bra-ket expression for\\
1. a horizontal polaroid (i.e.\ one that lets through only horizontal light),\\
2. a vertical polaroid,\\
3. a polaroid at an angle $\theta$,\\
4. a rotation by an angle $\theta$,\\
5. a ``quarter-wave plate'' that leaves horizontal light unchanged and delays vertical light by $\pi/2$.
\hint{The first column of the matrix is the effect of the operation on $\ket{\rm H}$ and the second is the effect on $\ket{\rm V}$.}
\hint{To find the bra-ket expressions, consider the effect of $\ket{x} \bra{\rm H}$ on $\ket{\rm H}$ and $\ket{\rm V}$.}
\result{
1. $\begin{pmatrix}1 & 0\\0 & 0\end{pmatrix} = \ket{\rm H} \bra{\rm H}$,\\
2. $\begin{pmatrix}0 & 0\\0 & 1\end{pmatrix} = \ket{\rm V} \bra{\rm V}$,\\
3. $\begin{pmatrix}\cos^2 \theta & \cos \theta \sin \theta\\\cos \theta \sin \theta & \sin^2 \theta\end{pmatrix} = \ket{\theta } \bra{\theta}$,\\
4. $\begin{pmatrix}\cos \theta & -\sin\theta\\\sin \theta & \cos \theta\end{pmatrix} = \ket{\theta}\bra{H} + \ket{\theta+\pi/2}\bra{V}$,\\
5. $\begin{pmatrix}1 & 0\\0 & \im\end{pmatrix} = \ket{\rm H} \bra{\rm H} + \im \ket{\rm V} \bra{\rm V}$.

}
\end{question}

\begin{question}
What's the effect of a quarter-wave plate on light polarised at $\pm \pi/4$ to the horizontal axis?
Calculate the Jones vector for the result and physically interpret it (how does the real $\vec E$ evolve with time?).
\solution{The resulting Jones vector is $(1, \pm \im)/\sqrt{2}$. It corresponds to \emph{circularly polarised} light, i.e.\ $\vec E$ spins around in a circle, in either direction. We call $\ket{\rm LCP} = (1,\im)/\sqrt{2}$ \emph{left-handed circular polarisation} and $\ket{\rm RCP} = (1,-\im)/\sqrt{2}$ as \emph{right-handed circular polarisation} (note: some authors define it the other way around).}
\end{question}

\begin{question}
1. What's the effect of a rotation on LCP and RCP light?\\
2. What about a polaroid at angle $\theta$?
\solution{
1. $(\cos \theta \mp \im \sin \theta, \sin \theta \pm \im \cos \theta)/\sqrt{2} = e^{\mp\im \theta} (1, \pm \im)/\sqrt{2}$ (i.e.\ the state is unaffected).\\
2. $(\cos^2 \theta \pm \im \cos \theta \sin \theta, \cos \theta \sin \theta \pm \im \sin^2 \theta) = e^{\pm \im \theta}/\sqrt{2}~\ket{\theta}$ (i.e.\ linearly polarised light at half the intensity).
}
\end{question}

Note that there is something peculiar about the LCP and RCP states.
We cannot separate them by a linear polaroid -- they seem like ``mixtures'' of all linear polarisation states.
The following question develops some terminoloy to describe this more formally.

\begin{question}
Calculate the effect of a polaroid at angle $\theta$ on an arbitrary state $\ket{J}$, followed by an intensity detector (i.e.\ a device that measures $\braket{ J}{J}$).
Write the result using the dot product of a bra with $\ket{J}$.
\result{$|\braket{ \theta}{J}|^2$.}
\solution{The effect of the polaroid is to yield $(J_1 \cos^2 \theta + J_2 \cos \theta \sin \theta, J_1 \cos \theta \sin \theta + J_2 \sin^2 \theta)$.
The $\braket{ J}{J}$ corresponding to this is
$$|J_1 \cos \theta + J_2 \sin \theta|^2 \cos^2 \theta + |J_1 \cos \theta + J_2 \sin \theta|^2 \sin^2 \theta$$
$$= |J_1 \cos \theta + J_2 \sin \theta|^2 = |\braket{ \theta}{J}|^2.$$
}
\end{question}

Note that this operation amounts to measuring how much intensity is in the $\ket{\theta}$ state.
\begin{question}
What happens if you add the intensity in the $\ket{\theta}$ state to the intensity in the $\ket{\theta+\pi/2}$ state?
\solution{
$$|(\cos \theta, \sin \theta)^\dagger \ket{J}|^2 + |(-\sin \theta, \cos \theta)^\dagger \ket{J}|^2$$
$$= |J_1|^2 \cos^2 \theta + 2 J_1 J_2^* \cos \theta \sin \theta + |J_2|^2 \sin^2 \theta + |J_1|^2 \sin^2 \theta - 2 J_1 J_2^* \cos \theta \sin \theta + |J_2|^2 \cos^2 \theta$$
$$= |J_1|^2 + |J_2|^2 = \braket{J}{J}.$$
The result is the total intensity.
}
\end{question}

\begin{question}
Can you generalise the result from the last question by replacing the $\ket{\theta}$ and $\ket{\theta+\pi/2}$ states with arbitrary states $\ket{K}$ and $\ket{L}$? What properties do they need to have for this to work?
\hint{Try setting $\ket{J} = \ket{K}$ and $\ket{J} = \ket{L}$.}
\solution{
The equation we desire is
$$|\braket{ K}{J}|^2 + |\braket{L}{J}|^2 = \braket{J}{J}.$$
Setting $\ket{J} = \ket{K}$ and $\ket{J} = \ket{L}$ we get
$$|\braket{K}{K}|^2 + |\braket{L}{K}|^2 = \braket{K}{K},\qquad |\braket{L}{L}|^2 + |\braket{K}{L}|^2 = \braket{ L}{L}.$$
You can convince yourself that this works if
$$\braket{ K}{K}  = 1, \quad \braket{L}{L} = 1, \quad \braket{K}{L} = 0.$$
We call $\ket{K}, \ket{L}$ an \emph{orthonormal set} in this case.
}
\end{question}

Note that, given the last question, we can express every $\ket{J}$ as $\alpha \ket{K} + \beta \ket{L}$ and we have $\braket{J}{J} = |\alpha|^2 + |\beta|^2$.
\begin{question}
What is $\ket{K} \bra{K} + \ket{L} \bra{L}$?
\solution{
Writing an arbitrary state as $\ket{J} = \alpha \ket{K} + \beta \ket{L}$ and note
$$(\ket{K} \bra{K} + \ket{L} \bra{L}) (\alpha \ket{K} + \beta \ket{L}) = \alpha \ket{K} + \beta \ket{L} = \ket{J},$$
hence $\ket{K} \bra{K} + \ket{L} \bra{L} = 1$.
}
\end{question}

Using our new terminology we can say there is $1/2$ intensity in every $\theta$ state for LCP/RCP light.
The question is now, can we distinguish LCP/RCP states in some other way?

\begin{question}
1. What mathematical expression corresponds to the amount of intensity in the LCP/RCP states?\\
2. Can you find a series of operations that will effect the measurement?\\
3. What about the sum of the intensities in the LCP and RCP states?
\solution{
1. $|\braket{\rm LCP}{J}|^2$ and $|\braket{\rm RCP}{J}|^2$.
2. The trick is to convert it to linear light using a quarter-wave plate. After applying the matrix
$$\begin{pmatrix}1 & 0\\0 & \im\end{pmatrix}$$
we get $(1, \pm 1)/\sqrt{2}$, which can be distinguished by measuring in the $\pm \pi/4$ directions.\\
3. $(1,\pm \im)/\sqrt{2}$ form an orthonormal set and so the intensities add to $\braket{J}{J}$.
}
\end{question}

\begin{question}
Find a procedure to measure the parameters $a, b, \phi$ for an arbitrary state $(a, b e^{\im \phi})$ (often called elliptical polarisation).
Note that $a > 0$, $b > 0$ and $-\pi < \phi < \pi$.
\solution{
Measuring the horizontal and vertical components gives $a^2$ and $b^2$.
Measuring the LCP component gives
$$I_L = |\braket{\rm LCP}{J}|^2 = |a - \im b e^{\im \phi}|^2/2 = \frac{a^2 + b^2 + 2 a b \sin \phi}{2}$$
and hence $\phi = \arcsin (I_L - (a^2 + b^2)/2)/(ab)$.

}
\end{question}

%So far we have always measured the intensity in some state (some Jones vector).
%We now want to generalise the notion and we want to ``measure a matrix''.
%
%\begin{question}
%Write the expression $|\braket{K}{J}|^2$ in terms of a matrix and $\ket{J}$.
%\hint{Remember $|a|^2 = a a^* = a a^\dagger$.}
%\result{$\bra{J} M \ket{J}$, where $M=\ket{K} \bra{K}$.}
%\end{question}
%
%\begin{question}
%(optional)
%Show that an arbitrary matrix $M$ can be written as a sum of terms of the form $\alpha \ket{K} \bra{K}$.
%\solution{
%Suppose that $M = \left(\begin{smallmatrix}M_1 & M_2\\M_3 & M_4\end{smallmatrix}\right)$.
%Note that
%$$\ket{\rm LCP} \bra{\rm LCP} = \frac{1}{2} \begin{pmatrix}1 & \im\\-\im & 1\end{pmatrix},\quad \ket{\pi/4} \bra{\pi/4} = \frac{1}{2} \begin{pmatrix}1 & 1\\1 & 1\end{pmatrix},$$
%and hence
%$$(M_2 + M_3) \ket{\pi/4} \bra{\pi/4} + (M_2 - M_3) (-\im) \ket{\rm LCP} \bra{\rm LCP} = \begin{pmatrix}M_2 (1 - \im) + M_3 (1 + \im) & M_2\\M_3 & M_2 (1 - \im) + M_3 (1 + \im)\end{pmatrix}.$$
%We can now easily fix the diagonal elements to find that
%$$M = (M_2 + M_3) \ket{\pi/4} \bra{\pi/4} + (M_2 - M_3) (-\im) \ket{\rm LCP} \bra{\rm LCP}$$
%$$+ (M_1 - M_2 (1 - \im) - M_3 (1 + \im)) \ket{\rm H} \bra{\rm H} + (M_4 - M_2 (1 - \im) - M_3 (1+\im)) \ket{\rm V} \bra{\rm V}.$$
%}
%\end{question}
%
%%\begin{question}
%%Define the ``measured value'' $\mathcal{M}(M)$ for a matrix $M$ in a state $\ket{J}$, both as a mathematical expression and as a procedure to determine its value in practice.
%%We want $\mathcal{M}(\ket{K} \bra{K}) \braket{J}{J}$ to be the intensity in state $\ket{K}$.
%%Since total intensity is arbitrary, define it so that it is independent of $\braket{J}{J}$.
%%\result{$\bra{J} M \ket{J} / \braket{J}{J}$.}
%%\solution{The previous question showed that we can write $M = \sum \alpha_i \ket{i} \bra{i}$.
%%We can define the measured value of $M$ now as follows.
%%Measure the intensity for each $\ket{i}$.
%%Multiply the result of each by $\alpha_i$ and sum them to get a final value $v$.
%%Divide that value by $\braket{J}{J}$ to remove the dependence on total intensity.
%%The mathematical expression for this value is
%%$$v = \sum_i \alpha_i |\braket{i}{J}|^2 = \sum_i \alpha_i \bra{J} \ket{i} \bra{i} \ket{J} = \bra{J} M \ket{J}.$$
%%and hence the measured value is $\bra{J} M \ket{J}/\braket{J}{J}.$
%%}
%%\end{question}
%%
%%I admit, this notion of a measured value of a matrix is rather esoteric and artificial, but we will soon find this notion to be very useful when developing quantum theory.
%%As bizarre as it is, always remember that we can define it precisely in terms of physical measurements.
%%
%%\begin{question}
%%Find the measured value of a rotation matrix $R_\phi$ for the states $\ket{\theta}$, $\ket{\rm LCP}$ and $\ket{\rm RCP}$.
%%\solution{
%%Note that all three states are ``normalised'' ($\braket{J}{J} = 1$).
%%We know that
%%$$R_\phi \ket{\theta} = \ket{\theta+\phi}, \quad R_\phi \ket{\rm LCP} = e^{-\im \phi} \ket{\rm LCP}, \quad R_\phi \ket{\rm RCP} = e^{\im \phi}$$
%%and hence the measured values are $\cos \phi$, $e^{-\im \phi}$ and $e^{\im \phi}$, respectively.
%%}
%%\end{question}
%%
%%\begin{question}
%%1. How do the measured values of $M$ and $M^\dagger$ relate?\\
%%2. Under what condition are the measured values of $M$ always real?
%%\solution{
%%1. They are complex conjugate, as follows from
%%$$\left(\sum \alpha_i \ket{i} \bra{i}\right)^\dagger = \sum {\alpha_i}^* \ket{i} \bra{i}.$$
%%2. $M = M^\dagger$.
%%}
%%\end{question}

\subsection{Unpolarisation of light}
So far we have only considered light from a single source.
We found that we could always ignore overall phase factors since all physical quantities were of the form $|\braket{a}{b}|^2$.
If we have light from two sources we expect (from the linearity of Maxwell's equation) that the result can be described by a new vector $\ket{c} = \ket{a} + \ket{b}$.
\begin{question}
Replace $\ket{a}$ and $\ket{b}$ by phase shifted versions $e^{\im \alpha} \ket{a}$ and $e^{\im \beta} \ket{b}$.
Find an expression for the total intensity $\braket{c}{c}$ and show that it depends on the relative phase $\alpha - \beta$.
\solution{
$$\braket{c}{c} = \left(e^{-\im \alpha} \bra{a} + e^{-\im \beta} \bra{b}\right)\left(e^{\im \alpha} \ket{a} + e^{\im \beta} \ket{b}\right)$$
$$= \braket{a}{a} + \braket{b}{b} + e^{\im (\alpha - \beta)} \braket{b}{a} + e^{\im (\beta - \alpha)} \braket{a}{b}$$
$$= \braket{a}{a} + \braket{b}{b} + 2 \Re(e^{\im (\alpha - \beta)} \braket{b}{a})$$
}
\end{question}

We thus have to be very careful to determine the relative phase before adding the Jones vectors.
In particular, if the two light sources are independent, the relative phase will most likely be random and change with time.
The two sources are then said to be \emph{mutually incoherent}.
We should thus average all predictions for measurements over all possible phase differences.

\begin{question}
Perform the averaging for $\braket{c}{c}$.
\result{$\braket{a}{a} + \braket{b}{b}$.}
\end{question}

Note that we have to be very careful here: We must not average the vector.
After all, $\ket{a} + e^{\im \delta} \ket{b}$ would average to $\ket{a}$!
In fact, it is impossible in general to describe the resulting light by a single Jones vector, as can be seen from considering \emph{unpolarised light}.
Unpolarised light is the result of combining light of two orthogonal polarisations (e.g.\ horizontal and vertical) incoherently.
From earlier we see that we can write this as, e.g.\
$$\ket{u_\delta} = \frac{1}{\sqrt{2}} \left(\ket{\rm H} + e^{\im \delta} \ket{\rm V}\right),$$
with the caveat that all results have to be averaged over $\delta$ (the $1/\sqrt{2}$ factor normalises the intensity $\braket{u_\delta}{u_\delta} = 1$).

\begin{question}
Show that the intensity of $\ket{u_\delta}$ in any normalised ket $\ket{K}$ is $1/2$.
\solution{
$$|\braket{K}{u_\delta}|^2 = \frac{1}{2} |K_1 + K_2 e^{\im \delta}|^2 = \frac{1}{2} (|K_1|^2 + |K_2|^2 + 2 K_1 K_2 \cos \delta) = \frac{1}{2} + K_1 K_2 \cos \delta,$$
after averaging the $\cos \delta$ disappears and we're left with $1/2$.
}
\end{question}

\begin{question}
We saw earlier that the intensity of $\ket{\rm LCP}$ in any $\ket{\theta}$ state is $1/2$. What then is the difference between $\ket{\rm LCP}$ and unpolarised light?
\solution{
Unpolarised light has the same intensity in \emph{both} LCP and RCP states, but $\ket{\rm LCP}$ obviously does not.
}
\end{question}

\begin{question}
Show that unpolarised light (of non-zero intensity) cannot be described by \emph{any} single ket $\ket{J}$ (with no dependence on $\delta$).
\solution{
Write $\ket{J} = \alpha \ket{K} + \beta \ket{L}$ for two orthonormal states $\ket{K}$ and $\ket{L}$ and define $\ket{H} = -\beta \ket{K} + \alpha \ket{L}$.
If $\ket{J}$ is unpolarised, then $\braket{H}{J} = \braket{J}{J} \ne 0$.
But $\braket{H}{J} = -\beta \alpha + \beta\alpha = 0$.
Hence $\ket{J}$ cannot be unpolarised.
}
\end{question}

\begin{question}
What is the effect of (a) a linear polariser, (b) a quarter-wave plate and (c) a rotation on unpolarised light?\\
\solution{(a) $\ket{\theta} \braket{\theta}{u_\delta} = (\cos \theta + e^{\im \delta} \sin \theta)/\sqrt{2}~\ket{\theta}$.
Note that $\left|\cos \theta + e^{\im \delta} \sin \theta\right| = \cos^2 \theta + \sin^2 \theta + 2 \cos \delta \cos \theta \sin \theta = 1 + \cos \delta \sin(2\theta)$.
We thus get linearly polarised light with an average intensity of $1/2$.\\
(b) $(1, \im e^{\im \delta})/\sqrt{2}$ is also unpolarised light, since we can redefine $\delta \to \delta - \pi/2$.\\
(c) Intuitively, the result is still unpolarised light. Mathematically it's a bit trickier since
$$R_\theta \ket{u_\delta} = \frac{1}{\sqrt{2}} (\cos \theta + \sin \theta e^{\im \delta}, -\sin \theta + \cos \theta e^{\im \delta}).$$
It is not at all obvious that the result is unpolarised.
We can calculate the intensity in an arbitrary $\ket{K}$ state as
$$|\bra{K} R_\theta \ket{u_\delta}|^2 = \frac{1}{2} \left|K_1 (\cos \theta + \sin \theta e^{\im \delta}) + K_2 (-\sin \theta + \cos \theta e^{\im \delta})\right|^2.$$
$$= \frac{1}{2} \left(|K_1|^2 |\cos \theta + \sin \theta e^{\im \delta}|^2 + |K_2|^2 |-\sin \theta + \cos \theta e^{\im \delta}|^2 + 2 \Re(K_1^* K_2 (\cos \theta + \sin \theta e^{-\im \delta}) (-\sin \theta + \cos \theta e^{\im \delta}))\right)$$
$$= \frac{1}{2} |K_1|^2 (1 + 2 \cos \theta \sin \theta \cos \delta) + \frac{1}{2} |K_2|^2 (1 - 2 \cos \theta \sin \theta \cos \delta) + \Re(K_1^* K_2 (\cos^2 \theta e^{\im \delta} - \sin^2 \theta e^{-\im \delta}))$$
which averages to
$$= \frac{1}{2} (|K_1|^2 + |K_2|^2) = \frac{1}{2}.$$
}
\end{question}

We saw that in the last question (especially (c)) that it's very difficult to manipulate unpolarised light using the expression $(1,e^{\im \delta})/\sqrt{2}$.
We found a lot of different ways to express the same unpolarised state in terms of $\delta$, which makes it tricky to show that two states are actually physically the same.
We would thus like a different formalism that doesn't use $\delta$.
The key insight is to take the expression for the intensity $\mathcal{A}\left(|\braket{K}{u_\delta}|^2\right)$, where $\mathcal{A}$ denotes averaging over $\delta$.
We can then write it as
$$\mathcal{A}\left(\bra{K} \ket{u_\delta} \bra{u_\delta} \ket{K}\right) = \bra{K} \mathcal{A}\left(\ket{u_\delta} \bra{u_\delta}\right) \ket{K} = \bra{K} \varrho \ket{K},$$
where the object
$$\varrho = \mathcal{A}\left(\ket{u_\delta} \bra{u_\delta}\right)$$
does not depend on $\delta$ (since the dependence is averaged out); yet since the above expression is general, it contains enough information to predict the result to \emph{any} intensity measurement.
$\varrho$ is called a \emph{density matrix}.
We can generalise the definition to arbitrary polarised and even to mixed polarised/unpolarised light, with the general formula $\varrho = \mathcal{A}\left(\ket{K_\delta} \bra{K_\delta}\right)$, where $\ket{K_\delta}$ is an arbitrary state that may or may not depend on $\delta$.
We will call a general $\varrho$ state a \emph{mixed state} and a state with no unpolarised component (i.e.\ can be written with $\ket{K_\delta}$ independent of $\delta$) a \emph{pure state}.

\begin{question}
Calculate $\varrho$ for\\
(a) unpolarised light,\\
(b) horizontally polarised light,\\
(c) vertically polarised light,\\
(d) light that is polarised in the $\theta$ direction,\\
(e) LCP/RCP light.
\result{
(a) $1/2 = \begin{pmatrix}1/2 & 0\\0 & 1/2\end{pmatrix}$\\
(b) $\ket{\rm H} \bra{\rm H} = \begin{pmatrix}1 & 0\\0 & 0\end{pmatrix}$\\
(c) $\ket{\rm V} \bra{\rm V} = \begin{pmatrix}0 & 0\\0 & 1\end{pmatrix}$\\
(d) $\ket{\theta} \bra{\theta} = \begin{pmatrix}\cos^2 \theta & \cos \theta \sin \theta\\\cos \theta \sin \theta & \sin^2 \theta\end{pmatrix}$\\
(e) $\ket{\rm L/RCP} \bra{\rm L/RCP} = \begin{pmatrix}1/2 & \pm\im/2\\\mp\im/2 & 1/2\end{pmatrix}$
}
\end{question}

\begin{question}
(a) Light from two uncorrelated sources $\varrho_1$ and $\varrho_2$ is mixed. What is the density matrix of the resulting light?\\
(b) If the two sources emit light in two orthonormal pure states, what is the result?
\hint{(a) Write $\varrho_1$ in terms of $\ket{K_\delta}$ and $\varrho_2$ in terms of $\ket{L_\varepsilon}$ and add the states $\ket{K_\delta}$ and $\ket{L_\varepsilon}$ states. Think about the phase relationship between $\ket{K_\delta}$ and $\ket{L_\varepsilon}$ (even if they are pure states).}
\solution{
(a) The total state is
$$\varrho = \mathcal{A}((\ket{K_\delta} + \ket{L_\varepsilon}) (\bra{K_\delta} + \bra{L_\varepsilon})) = \mathcal{A}(\ket{K_\delta} \bra{K_\delta}) + \mathcal{A}(\ket{L_\varepsilon} \bra{L_\varepsilon}) + 2 \operatorname{Re} \mathcal{A}(\ket{L_\varepsilon} \bra{K_\delta}).$$
Since the two sources are uncorrelated, there is a random phase factor between the two and so $\mathcal{A}(\ket{L_\varepsilon} \bra{K_\delta}) = 0$.
Hence $\varrho = \varrho_1 + \varrho_2$.

(b) We have $\varrho = \ket{K} \bra{K} + \ket{L} \bra{L} = 1,$
hence the result is unpolarised.
}
\end{question}

\begin{question}
Find an expression for the density matrix of light which has been manipulated with a matrix $M$ and originally had a density matrix $\varrho$.
\result{$M \varrho M^\dagger$.}
\solution{We have $\ket{K} \to M \ket{K}$ and $\bra{K} \to \bra{K} M^\dagger$.
Hence
$$\varrho = \mathcal{A}\left(\ket{J} \bra{J}\right) \to \mathcal{A}\left(M \ket{J} \bra{J} M^\dagger\right) = M \varrho M^\dagger.$$
}
\end{question}

\begin{question}
Now using $\varrho$: What is the effect on the density matrix of originally unpolarised light that passed through (a) a linear polariser, (b) a quarter-wave plate and (c) a rotation?
\solution{
Note that in all cases $\varrho = 1/2$ and so $M^\dagger \varrho M = M^\dagger M / 2$.\\
(a) $\frac{1}{2} \ket{\theta} \bra{\theta} \ket{\theta} \bra{\theta} = \frac{1}{2} \ket{\theta} \bra{\theta}$.\\
(b) $\frac{1}{2} \begin{pmatrix}1 & 0\\0 & \im (-\im)\end{pmatrix} = \frac{1}{2}$.\\
(c) $\frac{1}{2} R_{-\theta} R_\theta = \frac{1}{2}$.
}
\end{question}

%\begin{question}
%The trace $\tr(M)$ is defined to be linear in $M$ and to obey $\tr(\ket{A} \bra{B}) = \braket{B}{A}$.
%Show that $\tr(\ket{A} \bra{B} C) = \bra{B} C \ket{A}$ and $\tr(C D) = \tr(D C)$ for two matrices $C$ and $D$.
%\hint{Remember that $M = \sum_i \alpha_i \ket{i} \bra{i}$.}
%\solution{
%Write $C = \sum_i \gamma_i \ket{i} \bra{i}$ and note that
%$$\tr(\ket{A} \bra{B} C) = \sum_i \gamma_i \tr(\ket{A} \bra{B} \ket{i} \bra{i}) = \sum_i \gamma_i \braket{B}{i} \tr(\ket{A} \bra{i}) = \sum_i \gamma_i \braket{B}{i} \braket{i}{A} = \bra{B} C \ket{A}.$$
%With $D = \sum_j \delta_j \ket{j} \bra{j}$ we have
%$$\tr(CD) = \sum_{i,j} \gamma_i \delta_j \tr(\ket{i} \braket{i}{j} \bra{j}) = \sum_{i,j} \gamma_i \delta_j \tr(\ket{j} \bra{i}) \tr(\ket{i} \bra{j}),$$
%the result is symmetric in $i$ and $j$ and so equal to $\tr(DC)$.
%}
%\end{question}

%\begin{question}
%Find an expression for the measured value of a matrix $M$ in a mixed state $\varrho$.
%Express the result using the trace.
%\hint{Remember the definition of measured value in terms of physical measurements.}
%\solution{
%Write $M = \sum_i \alpha_i \ket{i} \bra{i}$.
%The measured value is defined as
%$$\mathcal{M} = \frac{1}{I_0} \sum_i \alpha_i \times \mbox{intensity in the $\ket{i}$ state},$$
%where $I_0$ is the total intensity.
%For a mixed state we have the intensity in the $\ket{i}$ state as $\bra{i} \varrho \ket{i}$ 
%$$\frac{1}{I_0} \sum_i \alpha_i \bra{i} \varrho \ket{i} = \frac{1}{I_0} \sum_i \alpha_i \tr(\ket{i} \bra{i} \varrho) = \frac{1}{I_0} \tr\left(\left(\sum_i \alpha_i \ket{i} \bra{i}\right) \varrho\right) = \frac{\tr(\varrho M)}{I_0}.$$
%To find $I_0$ note that we want the measured value of $1$ to be $1$ and so $I_0 = \tr(\varrho)$. Hence
%$$\mathcal{M}(M) = \frac{\tr(\varrho M)}{\tr(\varrho)}.$$
%}
%\end{question}

\begin{question}
The trace $\tr(M)$ is defined to be linear in $M$ and to obey $\tr(\ket{A} \bra{B}) = \braket{B}{A}$.

Show that\\
1. $\tr(\varrho) \ge 0$,\\
2. $\varrho = \varrho^\dagger$,\\
3. $|\braket{A}{B}|^2 \le \braket{A}{A} \braket{B}{B},$\\
4. $\tr(\varrho^2) \le (\tr(\varrho))^2$ (with equality for perfectly polarised light).
\hint{3. Find a vector $\ket{C}$ so that $\ket{A} \bra{A} + \ket{C} \bra{C} = \braket{A}{A}$ (remember we are in 2D). Then manipulate that equation by applying bras and kets.}
\solution{
1. $\tr(\varrho) = \mathcal{A}(\tr(\ket{K_\delta}\bra{K_\delta})) = \mathcal{A}(\braket{K_\delta}{K_\delta}) \ge 0$, since $\braket{K_\delta}{K_\delta} \ge 0$.\\
2. $\varrho^\dagger = \mathcal{A}((\ket {K_\delta} \bra{K_\delta})^\dagger) = \mathcal{A}(\ket{K_\delta} \bra{K_\delta}) = \varrho$.\\
3. If $\ket{C}$ is orthogonal to $\ket{A}$ and $\braket{C}{C} = \braket{A}{A}$ then $\ket{A} \bra{A} + \ket{C} \bra{C} = \braket{A}{A}$ and
$$\braket{A}{A} \braket{B}{B} = \bra{B} \braket{A}{A} \ket{B} = \braket{B}{A} \braket{A}{B} + \braket{B}{C} \braket{C}{B} \ge |\braket{B}{A}|^2.$$
4. For perfectly polarised light write $\varrho = \ket{J} \bra{J}$, then
$$\tr(\varrho^2) = \tr(\ket{J} \braket{J}{J} \bra{J}) = (\braket{J}{J})^2 = \tr(\varrho)^2.$$

In the general case, we can introduce a second ``$\delta$'' called $\varepsilon$ and use it to write $\varrho^2$ using a single average over both variables as
$$\varrho^2 = \mathcal{A}(\ket{K_\delta} \braket{K_\delta}{K_\varepsilon} \bra{K_\varepsilon}).$$
It follows that
$$\tr(\varrho^2) = \mathcal{A}(\tr(\ket{K_\delta} \braket{K_\delta}{K_\varepsilon} \bra{K_\varepsilon})) = \mathcal{A} (\braket{K_\varepsilon}{K_\delta} \braket{K_\delta}{K_\varepsilon}).$$
Now we can use the result from 3.\ to show that
$$\tr(\varrho^2) \le \mathcal{A}(\braket{K_\varepsilon}{K_\varepsilon}) \mathcal{A}(\braket{K_\delta}{K_\delta}) = (\tr(\varrho))^2$$
(note that the average ``splits'' like that because each expression averages over a different variable).
}
\end{question}

\begin{question}
Show that $\varrho$ is measurable, i.e.\ can be uniquely determined by measurements.
In particular, two states with different $\varrho$ can be distinguished.
\solution{
Remember that $\varrho = \varrho^\dagger$ and so $\varrho$ can be written as
$$\begin{pmatrix}a & b + \im c\\b - \im c & d\end{pmatrix}.$$
Measuring the intensity in $\ket{\rm H}$ and $\ket{\rm V}$ gives $a$ and $d$, respectively.
The intensity in $\ket{\rm LCP}$ gives
$$\bra{\rm LCP} \varrho \ket{\rm LCP} = \frac{1}{2} (1, \mp \im) \begin{pmatrix}a + (b \im - c)\\b - \im c + \im d\end{pmatrix} = \frac{1}{2} (a + (b\im - c) - \im (b - \im c + \im d)) = \frac{1}{2} (a + d) + c,$$
whereas the intensity in $\ket{\pi/4}$ gives
$$\bra{\pi/4} \varrho \ket{\pi/4} = \frac{1}{2} (1, 1) \begin{pmatrix}a + b + c \im\\b - \im c + d\end{pmatrix} = \frac{1}{2} (a + b + c \im + b - c \im + d) = \frac{1}{2} (a + d) + b.$$
Hence
$$a = \bra{\rm H} \varrho \ket{\rm H}, d = \bra{\rm V} \varrho \ket{\rm V},$$
$$b = \bra{\pi/4} \varrho \ket{\pi/4} - \tr(\rho)/2,$$
$$c = \bra{\rm LCP} \varrho \ket{\rm LCP} - \tr(\rho)/2.$$
}
\end{question}

\subsection{From photons to quantum mechanics}
We now know that light is not a classical wave and is, in fact, made of discrete excitations called photons, each carrying energy $\hbar \omega$ where $\hbar$ is the (reduced) Planck's constant (we will see later where this formula comes from).
Reconciling this with our previous results is not easy.
We might naively expect that photons come in two polarisations, say $\ket{\rm H}$ and $\ket{\rm V}$, and that light in an arbitrary state is really a mixture of both.
What do we mean by mixture?
The simplest would be that the polarisation of each photon is random with probability $\alpha$ to be $\ket{\rm H}$.
However, this would imply that the density matrix is proportional to
$$\begin{pmatrix}\alpha & 0\\0 & 1-\alpha\end{pmatrix},$$
which means it cannot represent states such as $\ket{\theta}$ or $\ket{\rm LCP}$ correctly.

While we could attempt to build more complex models, it is actually possible to prepare photons in arbitrary polarisation states, i.e.\ to prepare a stream of photons so that \emph{all of them} will pass through a filter for that state.
This implies that the polarisation vector $\ket{J}$ is a \emph{property of the photon}.
Previously $\braket{J}{J}$ had meaning as an intensity, but now the intensity is fixed and so it's now completely meaningless.
Also, previously $0$ was a sensible, but boring, state (corresponding to no light, i.e.\ zero intensity).
But we can't have a photon and no intensity and so zero is now no longer an actual state.

Now suppose photons polarised in state $\ket{J}$ enters a filter for photons of state $\ket{K}$.
We expect the intensity after the filter to be proportional to $|\braket{K}{J}|^2$.
We can recover this result by assuming that each photon passes through the filter with probability proportional to $|\braket{K}{J}|^2$.
\begin{question}
Find the actual probability that the photon passes through the filter.
\hint{What is the ratio of intensities?}
\result{$|\braket{K}{J}|^2/\braket{J}{J}$.}
\end{question}

Note that photons leaving a filter for state $\ket{K}$ are in state $\ket{K}$.
Thus, a second measurement will agree with the first one 100\% of the time.

The idea behind quantum mechanics is now to assume that \emph{all} systems behave like this.
Right now we can't justify such a bold leap but let's see where it leads us.
Take an \emph{arbitrary system} and assume that it's described by a state vector $\ket{J}$.
Since the system is most likely a lot more complicated the state vectors will have more dimensions than two (often infinitely many!).
We assume that we can still form bra vectors $\bra{J}$, scalar products $\braket{K}{J}$ etc.
We also assume that, just like photons, $\alpha \ket{J}$ (with $\alpha \ne 0$) is the same physical state as $\ket{J}$ and that $0$ is not a physical state.

In the photon case we had the equation
$$\ket{K} \bra{K} + \ket{L} \bra{L} = 1,$$
for two orthonormal vectors $\ket{K}$ and $\ket{L}$.
In the general case this generalises to
$$\sum_i \ket{i} \bra{i} = 1.$$
A set of states is called \emph{complete} if it satisfies this ``completeness relation''.
An alternative term is \emph{basis}.
This is a by no means trivial statement: It means that the set of state covers all the physics of the system.

Measurement in quantum mechanics turns out to work just like it did in the photon case.
In the photon case a measurement told us whether the photon was in state $\ket{K}$ or the orthogonal state $\ket{L}$.
Additionally, it \emph{disturbed the state} and changed it to be in line with whichever state it told us it was.
The simplest measurements in general quantum mechanics are very similar:
A measurement for state $\ket{K}$ will tell us the system is in state $\ket{K}$ or whether it's orthogonal to $\ket{K}$ (we assume that $\braket{K}{K} = 1$).
It will find the former with probability $|\braket{K}{J}|^2/\braket{J}{J}$.
It will also change the state of the system to agree with the result it gave.

As a simple example, consider an atom that can sit in one of $N$ sites in a material.
Each site corresponds to a state $\ket{1}$, $\ket{2}$, \dots, $\ket{N}$.
If the atom is in a general state $\ket{\psi}$ and we measure whether it's in $\ket{1}$, the answer will be ``yes'' with probability $|\braket{1}{\psi}|^2/\braket{\psi}{\psi}$.
In that case, it will end up in site 1.
If the answer is ``no'', it's now in a new state $\ket{\psi_1}$ which obeys $\braket{1}{\psi_1} = 0$ (i.e.\ it's orthogonal to $\ket{1}$).

In the photon case this is really the most general type of measurement:
The states are two dimensional and there is only one direction orthogonal to the first state.
If the states are higher dimensional, we can imagine a more complex type of measurement that tells us whether the system is in state $\ket{1}$, $\ket{2}$, \dots, $\ket{n}$ or whether it's orthogonal to all of them.
It will find each state with probability $|\braket{i}{\psi}|^2/\braket{\psi}{\psi}$.

This idea of measurements giving random results and changing the state of the system is rather bizarre.
We will later give it a more precise footing in terms of underlying physics.
For now, think of measurements as coupling a quantum system with a large classical system.
The classical system must end up in a definite state which needs to be consistent with the way the coupling is done.
In our $N$ lattice site model the classical system could be a person operating a detector that finds the atom in one of the sites.
If the atom is in some weird quantum state, like $(\ket{1} + 2 \ket{2} + 3 \ket{3})/\sqrt{13}$, to do the coupling we necessarily have to pick one of the three states.
The simplest solution is a random choice.
The fact that the system ends up in either $\ket{1}$, $\ket{2}$ or $\ket{3}$ is necessary to ensure that a second measurement will give the same result (if the atom hasn't moved).

We now want to derive a formalism to describe measurements.
Assume $n=N$, i.e.\ the measurement identifies one of the $\ket{i}$ states.
Assign each site a number $q_i$.
We can then say that our measurement ``measures a quantity $q$'' where each individual measurement will yield one of the $q_i$.
\begin{question}
Find an expression for the average value $\langle q \rangle$.
Write the result as an equation in terms of a matrix $q$.
Here the average is over many measurements of identically prepared states (an ``ensemble average'').
\hint{To get the matrix remember that $|a|^2 = a a^*$ and $\braket{a}{b}^* = \braket{b}{a}$.}
\solution{
Using the standard formula for expectation value
$$\langle q \rangle = \sum_i q_i \times \mbox{prob. of measuring $q_i$} = \sum q_i \frac{\left|\braket{i}{\psi}\right|^2}{\braket{\psi}{\psi}}.$$
We can write this as
$$\langle q \rangle = \frac{1}{\braket{\psi}{\psi}} \sum_i q_i \braket{\psi}{i} \braket{i}{\psi} = \frac{1}{\braket{\psi}{\psi}} \bra{\psi} \left(\sum_i q_i \ket{i} \bra{i}\right) \ket{\psi} = \frac{\bra{\psi}q\ket{\psi}}{\braket{\psi}{\psi}},$$
where $q = \sum_i q_i \ket{i} \bra{i}$.
}
\end{question}

\begin{question}
Measurable quantities are usually real.
Assuming that all the $q_i$ are real, show that $q$ from the last question is \emph{Hermitian}, i.e.\ $q = q^\dagger$.
\solution{
$$q^\dagger = \left(\sum_i q_i \ket{i} \bra{i}\right)^\dagger = \sum_i {q_i}^* \ket{i} \bra{i} = \sum_i q_i \ket{i} \bra{i} = q.$$
}
\end{question}

Note that the vaguely defined ``quantity $q$'' is now precisely defined as an operator.
An ``operator'' generalises the concept of a matrix to an arbitrary, possible infinite, number of dimensions.
An operator is formally defined as a mapping from vectors $\ket{i}$ to other vectors $q \ket{i}$.
We will always demand operators to be linear, which means $q (\alpha \ket{i} + \beta \ket{j}) = \alpha (q \ket{i}) + \beta (q \ket{j})$.
Take good note of the fact that operators can operate on bars as well as kets, mapping $\bra{i}$ to $\bra{i} q$.
Taking the conjugate of this equation we have
$$(\bra{i} q)^\dagger = q^\dagger \ket{i}.$$
The operator $q^\dagger$ thus describes the operation of $q$ on bra-vectors.
An expression such as $\bra{i} q \ket{j}$ is deliberately ambiguous -- it can mean either $(\bra{i} q) \ket{j}$ or $\bra{i} (q \ket{j})$.
The two are guaranteed to give the same result.

It turns out that this is the most general type of measurement.
We thus have the principle:
 \emph{Any measurable quantity (an ``observable'') is associated with a Hermitian operator $q$ in the space of states. The expectation value (written as $\langle q \rangle$) of a measurement in state $\ket{\psi}$ is given by $\bra{\psi} q \ket{\psi} / \braket{\psi}{\psi}$.}

The $q_i$ and $\ket{i}$ are particularly important -- each individual measurement produces one of the $q_i$ and leave the state in the corresponding $\ket{i}$ state.
Note that in terms of $q$ we have the equation
$$q \ket{i} = q_i \ket{i}$$
(remember, $q$ is an operator whereas $q_i$ is a number).
We can use this to define the $q_i$ and $\ket{i}$ in terms of $q$ as a solution of this equation (excluding the trivial solution $\ket{i} = 0$).
For an arbitrary operator $q$ the solutions $q_i$ for this equation are called \emph{eigenvalues}.
They are in general complex numbers.
The corresponding $\ket{i}$ are called \emph{eigenvectors} (also called \emph{eigenstates} in quantum mechanics).
The process of obtaining the eigenvalues and eigenvectors is often called \emph{diagonalisation} (for reasons that will be obvious later).

We can now formulate an addendum to the principle: \emph{Each individual measurement results in an eigenvalue $q_i$ of $q$, with probability $|\braket{q_i}{\psi}|^2$, and leaving the state as an eigenvector of $q$.}
Note that the last step is identical to applying the operator $\ket{i} \bra{i}$.

\begin{question}
$q$ is an operator with eigenvalues $q_i$ and eigenvectors $\ket{i}$.
Show that\\
(a) $\bra{i} q^\dagger = \bra{i} {q_i}^*$ (note that $\bra{i} q \ne \bra{i} q_i$ in general),\\
(b) if $q$ is Hermitian ($q = q^\dagger$), $q_i$ must be real,\\
(c) if $q$ is anti-Hermitian ($q = -q^\dagger$), $q_i$ must be imaginary,\\
(d) $q^n \ket{i} = {q_i}^n \ket{i}$,\\
(e) if $\langle q \rangle \ge c$ (for some real number $c$) always, then all $q_i \ge c$.
\solution{
(a) Take the Hermitian conjugate of $q \ket{i} = q_i \ket{i}$.\\
(b) (c)
$${q_i}^* \braket{i}{i} = \bra{i} q^\dagger \ket{i} = \pm \bra{i} q \ket{i} = \pm q_i \braket{i}{i}$$
and hence ${q_i}^* = \pm q_i$, which implies $q_i$ is real/imaginary.\\
(d) Apply $q$ to $q \ket{i} = q_i \ket{i}$ repeatedly.\\
(e) Apply $\langle q \rangle \ge c$ to $\ket{i}$ to get
$$q_i = \frac{\bra{i} q \ket{i}}{\braket{i}{i}} \ge c.$$
}
\end{question}

\begin{question}
(optional)
For finite dimensional $q$ use the determinant to eliminate $\ket{i}$ from $q \ket{i} = q_i \ket{i}$.
\solution{
$(q - q_i) \ket{i} = 0$ is a matrix equation that can only be solved for non-zero $\ket{i}$ if
$$\det(q - q_i) = 0.$$
}
\end{question}

\begin{question}
If $q_i$ and $q_j$ are identical (``degenerate''), are $\ket{i}$ and $\ket{j}$ uniquely determined from $q$?
\solution{No, take, e.g., the rotated states
$$\ket{a} = \cos \theta \ket{i} + \sin \theta \ket{j}, \qquad \ket{b} = -\sin \theta \ket{i} + \cos \theta \ket{j}.$$
Since
$$q_i (\ket{a} \bra{a} + \ket{b} \bra{b}) = q_i (\ket{i} \bra{i} + \ket{j} \bra{j})$$
we can replace $\ket{i}$ and $\ket{j}$ by $\ket{a}$ and $\ket{b}$ and still get the same operator $q$.
}
\end{question}

This leads to a slight correction to the principle if $q_i = q_j = \dots$: We project not with $\ket{i} \bra{i}$ but with $\ket{i} \bra{i} + \ket{j} \bra{j} + \dots$
Note that this entire construction would blow up if it were not for the following theorem.

\begin{question}
Given that $q$ is a Hermitian operator, show that for two eigenvalues $q_i$, $q_j$, the eigenvectors are either orthogonal or can be chosen to be orthogonal.
\hint{Consider $q_i = q_j$ and $q_i \ne q_j$ separately. In the latter case remember that you can apply bra vectors and ${}^\dagger$ to equations.}
\solution{
We know that
$$q \ket{i} = q_i \ket{i}, \quad q \ket{j} = q_j \ket{j},$$
and if we apply $\bra{j}$ to the first and $\bra{i}$ to the second equation we get
$$\bra{j} q \ket{i} = q_i \braket{j}{i}, \qquad \bra{i} q \ket{j} = q_j \braket{i}{j},$$
hence
$$q_i \braket{j}{i} = (q_j \braket{i}{j})^* = q_j \braket{j}{i},$$
or
$$(q_i - q_j) \braket{j}{i} = 0.$$
Hence $\braket{j}{i} = 0$, if $q_i \ne q_j$.

If $q_i = q_j$ then we just saw that we can replace $\ket{i}$ and $\ket{j}$.
Replace $\ket{j}$ with $\ket{j} - \braket{i}{j} \ket{i}$, which is orthogonal to $\ket{i}$ (we assume $\braket{i}{i} = 1$).
}
\end{question}

\begin{question}
Assume that we are in $N$ dimensions.
Define $\ket{1} = (1,0,0,\dots)$, $\ket{2} = (0,1,0,0,\dots)$, \dots, $\ket{N} = (0, 0, \dots, 1)$.
Note that this is a basis.
Given a matrix
$$M = \begin{pmatrix}M_{11} & M_{12} & M_{13} & \dots\\M_{21} & M_{22} & M_{23} & \dots\\M_{31} & M_{32} & M_{33} & \dots\\\vdots & \vdots & \vdots & \ddots\end{pmatrix}$$
write $M$ in terms of bra and ket vectors.

If you determined a matrix $M$ to have coefficients $M'_{i,j}$ in terms of another basis $\ket{i'}$, can you express $M_{i,j}$ in terms of $M'_{i,j}$?
If you use the $M'_{i,j}$ with the original $\ket{i}$ vectors you can define another matrix $M'$. Relate $M$ and $M'$ via a matrix equation.
\hint{Remember that you can insert the completeness relation $\sum \ket{i} \bra{i} = 1$ into expressions.}
\solution{
Applying $M$ to $\ket{i}$ gives the $i$-th column
$M \ket{i} = \sum_j M_{j,i} \ket{j}$
Hence
$$M = \sum_i M \ket{i} \bra{i} = \sum_{i,j} M_{j,i} \ket{j} \bra{i}.$$

Now given
$M = \sum_{i,j} M'_{j,i} \ket{j'} \bra{i'}$
we can apply $\sum \ket{k}\bra{k} = 1$ to both sides to get
$$M = \sum_{i,j,k,\ell} M'_{j,i} \braket{k}{j'} \braket{i'}{\ell} \ket{k} \bra{\ell},$$
by comparing with our earlier expression
$$M_{k,\ell} = \sum_{i,j} M'_{j,i} \braket{k}{j'} \braket{i'}{\ell}.$$

If $M' = \sum_{i,j} M'_{j,i} \ket{j} \bra{i}$
then
$$M = \sum_{i,j} \ket{j'} \bra{j} M' \ket{i} \bra{i'} = U M' U^\dagger,$$
where $U = \sum_i \ket{i'} \bra{i}$.
}
\end{question}

\begin{question}
In the last question take $\ket{i'}$ to be the eigenvectors with eigenvalues $\lambda_i$ (assume that they form a basis).
Show that $M'$ is diagonal, i.e.\ ${M'}_{ij}$ is zero if $i \ne j$.
\solution{
We can substitute $M = \sum_{i,j} {M'}_{j,i} \ket{j'} \bra{i'}$ in $\bra{j'} M \ket{i'}$ to find
$${M'}_{j,i} = \bra{j'} M \ket{i'} = \bra{j'} \lambda_j \ket{i'} = \lambda_j \braket{j'}{i'}.$$
This is zero if $j \ne i$ (and equal to $\lambda_j$ otherwise).
}
\end{question}

\subsection{Classical and quantum probability}
We assumed that photons have a polarisation \emph{vector} $\ket{J}$, but we showed earlier that the most general polarisation state of light is a \emph{density matrix} $\varrho$.
We already hinted how this works out, in our analysis of the naive two-polarisation-state photon model.
A density matrix is the result of a random sequence of photons.
What do we mean by random?
Here, we used the classical definition of randomness, which reflects \emph{imperfect knowledge}.
We can roll a die and get a ``random'' number even though the number rolled is a perfectly determistic function of the die's position and momentum when it was thrown!
However this function is so complicated and we know so little about the position and momentum that the result \emph{appears} random.

This is very different from the quantum world.
We can know with 100\% certainty that the state of the photon is $\ket{\rm LCP}$.
There are no further degrees of freedom so we know everything there is to know about the system.
Yet we cannot predict whether the photon will pass through a linear polarisation filter -- we can only calculate the probability.
This is the essence of \emph{quantum probability}.

In reality we often don't even know the state $\ket{\psi}$.
That is, we have a situation with \emph{both classical and quantum probability}.
We have a probability distribution for the state $\ket{\psi}$ -- reflecting incomplete knowledge -- \emph{and} we have the fundamental quantum probability that comes with every measurement.
\begin{question}
We know the system is in either $\ket{\psi}$ or $\ket{\phi}$ and both occur with equal chance.
Assuming that $\braket{\psi}{\psi} = \braket{\phi}{\phi} = 1$, find an expression for $\langle q \rangle$.
(we assume that both states are normalised).
Show that the ``quantum superposition'' state $(\ket{\psi} + \ket{\phi})/\sqrt{2}$ gives a different answer.
\solution{
For the classical mixture of two states we have
$$\langle q \rangle = \frac{1}{2} \bra{\psi} q \ket{\psi} + \frac{1}{2} \bra{\phi} q \ket{\phi}$$
For the quantum superposition state
$$\langle q \rangle = \frac{1}{2} \bra{\psi} q \ket{\psi} + \frac{1}{2} \bra{\phi} q \ket{\phi} + \operatorname{Re} \bra{\phi} q \ket{\psi}.$$
}
\end{question}

Note that in the quantum superposition state there is an additional ``interference'' term involving both states.
The presence of interference is another fundamental difference between classical and quantum probability.
As a result, classical mixing is irreversible -- states only become more and more impure as you mix them.
Quantum ``mixing'' (superposition) is reversible. Given the two superposition states $(\ket{A} \pm \ket{B})/\sqrt{2}$ you can recover $\ket{A}$ and $\ket{B}$ (note the parallels to unpolarised light and LCP/RCP states).

It is this scenario of both classical and quantum probability that density matrices appear in.
A system in an unknown quantum state, also called a mixed or impure state, is described by a density matrix $\varrho$.
$\varrho$ is defined just as it is for photons
$$\varrho = \mathcal{A}(\ket{K_\delta} \bra{K_\delta}).$$
Note that $\varrho$ can also be defined for a ``pure'' state $\ket{K}$, for which we just have $\varrho = \ket{K} \bra{K}$.

\begin{question}
Assume that the system is in one of the (normalised) states $\ket{K_1}$, $\ket{K_2}$, \dots with probability $P_1$, $P_2$, \dots respectively.
Find an expression for $\varrho$.
\solution{
The average can be expressed using the standard expectation value formula
$$\varrho = \sum_i P_i \ket{K_i} \bra{K_i}.$$
}
\end{question}

Note that just like $\ket{K_i}$, $\varrho$ need not be normalised.
Just like light, $\varrho$ obeys\\
1. $\tr(\varrho) \ge 0$.\\
2. $\varrho = \varrho^\dagger$.\\
3. $\tr(\varrho^2) \le \tr(\varrho)^2$ (with equality for a pure state).

\begin{question}
Show that $\tr(\ket{A} \bra{B} C) = \bra{B} C \ket{A}$ and $\tr(C D) = \tr(D C)$ for two matrices $C$ and $D$.
\hint{Remember that $M = \sum_i \alpha_i \ket{i} \bra{i}$.}
\solution{
Write $C = \sum_i \gamma_i \ket{i} \bra{i}$ and note that
$$\tr(\ket{A} \bra{B} C) = \sum_i \gamma_i \tr(\ket{A} \bra{B} \ket{i} \bra{i}) = \sum_i \gamma_i \braket{B}{i} \tr(\ket{A} \bra{i}) = \sum_i \gamma_i \braket{B}{i} \braket{i}{A} = \bra{B} C \ket{A}.$$
With $D = \sum_j \delta_j \ket{j} \bra{j}$ we have
$$\tr(CD) = \sum_{i,j} \gamma_i \delta_j \tr(\ket{i} \braket{i}{j} \bra{j}) = \sum_{i,j} \gamma_i \delta_j \tr(\ket{j} \bra{i}) \tr(\ket{i} \bra{j}),$$
the result is symmetric in $i$ and $j$ and so equal to $\tr(DC)$.
}
\end{question}

\begin{question}
Find an expression for the expectation value of an observable $q$ in a mixed state $\varrho$.
\solution{
To find $\langle q \rangle$ we perform first the quantum average $\bra{K_\delta} q \ket{K_\delta}$ and then the classical average $\mathcal{A}$.
To do this we need to introduce a normalisation factor $\alpha$.
We get
$$\langle q\rangle = \alpha \mathcal{A}(\bra{K_\delta} q \ket{K_\delta}) = \alpha \mathcal{A}(\tr(q \ket{K_\delta} \bra{K_\delta})) = \alpha \tr(q \mathcal{A}(\ket{K_\delta} \bra{K_\delta})) = \alpha \tr(q \rho).$$
To find $\alpha$ note that $\langle 1 \rangle = \alpha \tr(\rho)$, but it should be 1, and so $\alpha = 1/\tr(\rho)$ and hence
$$\langle q \rangle = \frac{\tr(\rho q)}{\tr(\rho)}.$$
}
\end{question}

\subsection{Infinite dimensions}
So far we have worked with finite dimensional states -- i.e.\ with a finite number of basis states.
We can also generalise all concepts so far to infinite dimensional state space.

In infinite dimensions we have to explicitly assume that all observables have a ``complete set of eigenstates'', i.e.\ that the eigenstates obey
$$\sum_i \ket{i} \bra{i} = 1.$$
This additional axiom can be proved in the finite dimensional case.
Note that even in finite dimension it's not true for all non-Hermitian operators, for instance it's not true for
$$\begin{pmatrix}1 & 0\\1 & 1\end{pmatrix}.$$

With infinite dimensions there are two fundamental possibilities (which are not mutually exclusive).
We can have either discrete states or continuous states.
An example for discrete states would be an atom in one of infinitely many sites $\ket{1}$, $\ket{2}$, $\ket{3}$, \dots
An example of continuous states would be an atom in a 1D universe -- we now have a state $\ket{x}$ for every real number $x$.
In the discrete case we simply use infinite sums, e.g. the completeness relation is\
$$\sum_{i=1}^\infty \ket{i} \bra{i} = 1.$$
In the continuous case we have to use integrals and we get a completeness relation
$$\int \ket{x} \bra{x} \D{x} = 1.$$

\begin{question}
Derive the normalisation condition for the discrete $\ket{i}$ and the continuous $\ket{x}$ states, i.e.\ find an equation for $\braket{j}{i}$ and $\braket{x}{y}$.
\hint{Apply $\ket{j}$ / $\ket{y}$ to the completeness relation.}
\solution{
In the discrete case we have
$$\sum_{i=1}^\infty \ket{i} \braket{i}{j} = \ket{j},$$
which implies
$$\braket{i}{j} = \delta_{i,j}.$$

In the continuous case we have
$$\int \ket{x} \braket{x}{y} \D{x} = \ket{y}$$
This equation is solved by the $\delta$-function
$$\braket{x}{y} = \delta(x - y).$$
}
\end{question}

By virtue of the completeness relation we can expand an arbitrary state $\ket{\psi}$ in terms of $\ket{x}$, i.e.\
$$\ket{\psi} = \int \ket{x} \braket{x}{\psi} \D{x}.$$
We can then define the \emph{wavefunction}
$$\psi(x) = \braket{x}{\psi}.$$

\begin{question}
Find an expression for $\braket{\phi}{\psi}$ in terms of wavefunctions.
In particular express the normalisation condition $\braket{\psi}{\psi} = 1$ in terms of $\psi(x)$.
\hint{Use the completeness relation.}
\solution{
Insert the completeness relation between $\bra{\phi}$ and $\ket{\psi}$ to get
$$\braket{\phi}{\psi} = \int \braket{\phi}{x} \braket{x}{y} \braket{y}{\psi} \D{x} \D{y} = \int \phi^*(x) \delta(x - y) \psi(y) \D{x} \D{y} = \int \phi^*(x) \psi(x) \D{x}.$$
The normalisation condition is obtained by setting $\phi=\psi$, i.e.\
$$\int |\psi(x)|^2 \D{x} = 1.$$
}
\end{question}

\begin{question}
(a) Given an operator $q$, write $\ket{\phi} = q \ket{\psi}$ in terms of wavefunctions and a ``matrix'' $q(x,y)$.\\
(b) Define a position operator by $q \ket{x} = x \ket{x}$ and find $q(x,y)$.\\
(c) Find an expression for the expectation value of an operator and apply it to the position operator.
\solution{
Using the completeness relation
$$\braket{x}{\phi} = \int \bra{x} q \ket{y} \braket{y}{\psi} \D{y}.$$
Hence if $q(x,y) = \bra{x} q \ket{y}$ then
$$\phi(x) = \int q(x,y) \psi(y) \D{y}.$$
(b) 
$$q(x,y) = \bra{y} q \ket{x} = x \braket{y}{x} = x\,\delta(y - x).$$
(c)
The expectation value is
$$\bra{\psi} q \ket{\psi} = \int \psi^*(x) q(x,y) \psi(y) \D{x} \D{y}$$
For the position operator we have
$$\bra{\psi} q \ket{\psi} = \int x |\psi(x)|^2 \D{x}.$$
}
\end{question}

\begin{question}
Find the eigenvalues and eigenstates of the position operator $\ket{x}$.
\solution{
Since $q \ket{x} = x \ket{x}$ for every $x$ we conclude that every real number $x$ is an eigenvalue and that $\ket{x}$ is the corresponding eigenstate.
}
\end{question}

In finite dimensions we can always normalise a state $\ket{\psi}$, i.e.\ we can redefine it so that $\braket{\psi}{\psi} = 1$ using
$$\ket{\psi} \to \frac{\ket{\psi}}{\sqrt{\braket{\psi}{\psi}}}.$$
In infinite dimension some states are not normalisable since $\braket{\psi}{\psi}$ is infinite.
For instance, we just showed earlier that $\braket{x}{x} = \delta(0)$.
This generally implies that the state is unphysical.
For instance, we will later see that the $\ket{x}$ state has infinite kinetic energy.
Such unphysical state are often still useful, either as approximations of physical states or as mathematical constructs.
Note that states that depend continuously on a parameter can be described by a relationship such as $\braket{x}{y} = \delta(x - y)$ which can be considered a normalisation condition.

\subsection{Symmetry}
So far we found ways to describe systems and observables in general terms but we had little physical content (except for photons).
To ameliorate this situation we need a way to identify -- and relate -- observables.
For instance, for the particle in the one-dimensional space, we identified a position operator $q$, but to do physics we also need a momentum operator $p$, an energy operator $H$, etc.
Additionally, we need a way to include time in our scheme.
It turns out we can solve both these problems with just one additional principle.

To understand this principle we need to understand the profound effect that symmetry has on physical systems.
A complete discussion of the effect of symmetry requires Lagrangian mechanics, but there are a few simple cases we can discuss without.
\begin{question}
Consider a classical 1D particle in a potential $V(t,x)$
$$m \ddot x = -\pfrac{V}{x}.$$
(a) Show that if the system is symmetric with respect to spatial displacements, i.e.\ $V(t, x+\delta x) = V(t, x)$, then momentum $p$ is conserved (i.e.\ does not change with time).\\
(b) Show that if the system is symmetric with respect to temporal displacements, i.e.\ $V(t+\delta t, x) = V(t, x)$, then energy $H$ is conserved.
\hint{(b) Calculate $\D{H}/\D{t}$.}
\solution{
(a) If $V(t, x+\delta x) = V(t,x)$, then $V$ is independent of $x$ and $\partial V/\partial x = 0$ and hence
$$\dot p = m \ddot x = 0.$$
(b) Note that $H = m \dot x^2/2 + V$ and hence by the chain rule
$$\Dfrac{H}{t} = m \ddot x \dot x + \pfrac{V}{x} \dot x + \pfrac{V}{t} = \pfrac{V}{t}$$
by applying the equation of motion.
Hence if $\partial V/\partial t = 0$, then $H$ is constant.
}
\end{question}

\begin{question}
(optional)
Generalise the results from the last question to $n$ particles as follows.
Given a potential $V(t, x_1, x_2, \dots, x_n)$ the equation of motion is
$$m_i \ddot x_i = - \pfrac{V}{x_i}.$$
Show that \\
(a) if $V(t, x_1 + \delta x, x_2 + \delta x, \dots, x_n + \delta x) = V(t, x_1, \dots, x_n)$, then total momentum $P$ is conserved,\\
(b) if $\partial V/\partial t = 0$, then total energy $H$ is conserved.
\solution{
(a) Taking derivatives with respect to $\delta x$ and setting $\delta x = 0$ gives
$$0 = \sum_i \pfrac{V}{x_i} = \sum_i (-m_i \ddot x_i) = -\Dfrac{}{t} \sum_i m_i \dot x_i = -\Dfrac{P}{t}.$$
(b)
Now $H = \sum_i m_i \dot x_i^2/2 + V$ and hence
$$\Dfrac{H}{t} = \sum_i m_i \ddot x_i \dot x_i + \sum_i \pfrac{V}{x_i} \dot x_i + \pfrac{V}{t} = \sum_i \dot x_i \left(m_i \ddot x_i + \pfrac{V}{x_i}\right) + \pfrac{V}{t} = \pfrac{V}{t}$$
and $H$ is again conserved if $\partial V/\partial t = 0$.
}
\end{question}

To apply these ideas to quantum systems we start by defining an operator $T_\varepsilon$ that maps state to transformed states.
$T_\varepsilon$ depends on a real number $\varepsilon$, i.e.\ we are dealing with \emph{continuous symmetries} (mirror symmetry is an example of a symmetry that's not continuous).
For instance, spatial displacement corresponds to an operator
$$T_\varepsilon \ket{x} = \ket{x + \varepsilon}.$$
We assume that in general $T_0 = 1$.

\begin{question}
We want $T_\varepsilon$ to preserve scalar products, i.e.\ if $\ket{x} \to \ket{x'}$ and $\ket{y} \to \ket{y'}$ then
$$\braket{x}{y} = \braket{x'}{y'}$$
(we say that $T_\varepsilon$ must be \emph{unitary}).
Find an equation that $T_\varepsilon$ must obey.
\solution{
$\ket{y'} = T_\varepsilon \ket{y}$ implies that $\bra{y'} = \bra{y} {T_\varepsilon}^\dagger$.
Hence
$\braket{y'}{x'} = \bra{y} {T_\varepsilon}^\dagger T_\varepsilon \ket{x}$, which only works for all states if
$${T_\varepsilon}^\dagger T_\varepsilon = 1.$$
}
\end{question}

We can define a ``derivative'' in $\varepsilon$ via the limit
$$G = \lim_{\varepsilon \to 0} \frac{T_\varepsilon - 1}{\varepsilon}.$$
The resulting operator $G$ is called the \emph{generator} of the transformation.
We can then write transformations for $\varepsilon \approx 0$ as
$$T_\varepsilon = 1 + \varepsilon G + O(\varepsilon^2).$$

\begin{question}
What does $T_\varepsilon$ being unitary imply about $G$?
\solution{
$$(1 + \varepsilon G)^\dagger (1 + \varepsilon G) = 1 + \varepsilon (G + G^\dagger) + O(\varepsilon^2) = 1$$
and hence $G = -G^\dagger$, i.e.\ $G$ is \emph{anti-Hermitian}.
}
\end{question}

\begin{question}
(optional)
If $T_a T_b = T_{a+b}$ we can actually recover $T_\varepsilon$ from $G$ for arbitrary $\varepsilon$.
Find an equation to do so.
\result{$T_\varepsilon = \exp(\varepsilon G).$}
\hint{Find an expression for the derivative $\D{T_\varepsilon}/\D{\varepsilon}$ for arbitrary $\varepsilon$.}
\solution{
$$\Dfrac{T_\varepsilon}{\varepsilon} = \lim_{\delta \to 0} \frac{T_{\varepsilon+\delta} - T_\varepsilon}{\delta} = T_\varepsilon \lim_{\delta \to 0} \frac{T_\delta - 1}{\delta} = T_\varepsilon G$$
This is a differential equation with the solution $T_\varepsilon = \exp(\varepsilon G)$.
One way to show this is to integrate to get
$$T_\varepsilon = 1 + \int_0^\varepsilon T_\delta G \D{\delta}$$
and to then substitute this equation into itself repeatedly to find a power series in $G$,
$$T_\varepsilon = 1 + G \varepsilon + \frac{1}{2!} G^2 \varepsilon^2 + \dots = \exp(G).$$
}
\end{question}

We can now express the principle we alluded to earlier.
It states that \emph{if a continuous symmetry in the classical system conserves $Q$, then the quantum operator $Q$ is related to the generator of the transformation via the equation $Q = \im \hbar G$.}
Here $\hbar$ is a constant of nature called the reduced Planck's constant.
We can consider the principle the definition of $\hbar$.

\begin{question}
Why do we need $\im$ in $Q = \im \hbar G$?
\solution{
$\im$ ensures that $Q$ is Hermitian, since $(\im G)\dagger = \im^* G^\dagger = (-\im) (-G) = \im G$.
}
\end{question}

To apply the principle we need to precisely identify which classical conserved quantity is associated with a symmetry.
Note that the simple approach from above identifies it, at best, only up to a constant factor, since if $Q$ is conserved, so is $\alpha Q$ for any constant $\alpha$.
Using Lagrangian mechanics one can give the following, more, precise definition:
If a transformation parametrised by $\varepsilon$ is such that $L=T-V$ (where $T$ is kinetic and $V$ is potential energy) is invariant under the transformation, then
$$Q = \sum_i \pfrac{L}{\dot x_i} \Dfrac{x_i}{\varepsilon}$$
is the corresponding conserved quantity (more generally, if a symmetry does not leave $L$ invariant but rather changes $L$ to $L + \D{\Lambda}/\D{t}$ then the quantity has an extra $-\D{\Lambda}/\D{\varepsilon}$).

Of course, the principle still has a problem: we can only apply it to symmetric systems!
We will usually work around that by assuming that the operator for the quantity remains unchanged even when the system is not symmetric (even though it no longer corresponds to a classical conserved quantity).

\begin{question}
(optional) Show that $\hbar$ has units of energy $\times$ time.
\solution{
$T_\varepsilon$ is dimensionless and $G$ has units of $1/\varepsilon$.
$Q$ has units of
$$\frac{{\rm energy}}{{\rm length/time}} \frac{\rm length}{\varepsilon} = \frac{{\rm energy} \times {\rm time}}{\varepsilon}$$
Since $G=\im \hbar Q$ we have $\hbar$ having units of ${\rm energy} \times {\rm time}$.
}
\end{question}

\begin{question}
(optional) Another result from Lagrangian mechanics is the \emph{Euler-Lagrange equation}
$$\Dfrac{}{t} \pfrac{L}{\dot x_i} = \pfrac{L}{x_i}.$$
Use this equation to show that $Q$ as defined above is conserved in classical mechanics.
Note that $L$ is a function of $t$, $x_i$ and $\dot x_i$.
\hint{Show that $\D{Q}/\D{t} = \D{L}/\D{\varepsilon}$.}
\solution{
$$\Dfrac{Q}{t} = \sum_i \left(\left(\Dfrac{}{t} \pfrac{L}{\dot x_i}\right) \Dfrac{x_i}{\varepsilon} + \pfrac{L}{\dot x_i} \Dfrac{\dot x_i}{\varepsilon}\right) = \sum_i \left(\pfrac{L}{x_i} \Dfrac{x_i}{\varepsilon} + \pfrac{L}{\dot x_i} \Dfrac{\dot x_i}{\varepsilon}\right)$$
Using the chain rule this is equal to $\D{L}/\D{\varepsilon}$, which is zero since $L$ is invariant under the transformation.
}
\end{question}

\begin{question}
Apply the principle to show that the momentum operator is $p = -\im \hbar\,\D{}/\D{x}$ (in one dimension).
\hint{Use an appropriate symmetry transformation with $p$ as the classical conserved quantity. Calculate $\bra{x} G \ket{\psi}$ for a state $\ket{\psi}$ and express the result in terms of $\psi(x)$.}
\hint{Also note that $\bra{x} T_\varepsilon = \bra{x-\varepsilon}$.}
\solution{
Consider spatial displacements $x \to x+\varepsilon$.
We have $\D{x}/\D{\varepsilon} = 1$ and so
$$Q = \pfrac{(T - V)}{\dot x} = \pfrac{}{\dot x} \frac{1}{2} m \dot x^2 = m \dot x$$
as expected.

In the quantum realm we have $T_\varepsilon \ket{x} = \ket{x+\varepsilon}$.
Since $T_\varepsilon$ is unitary we have $T_{-\varepsilon} = {T_\varepsilon}^{-1} = {T_\varepsilon}^\dagger$ and so $T_{-\varepsilon} \ket{x} = \ket{x-\varepsilon}$ implies $\bra{x} T_\varepsilon = \bra{x-\varepsilon}$ and
$$\bra{x} T_\varepsilon \ket{\psi} = \braket{x-\varepsilon}{\psi} = \psi(x - \varepsilon).$$
We then have
$$\bra{x} G \ket{\psi} = \lim_{\varepsilon \to 0} \frac{\bra{x} T_\varepsilon \ket{\psi} - \braket{x}{\psi}}{\varepsilon} = \lim_{\varepsilon \to 0} \frac{\psi(x-\varepsilon) - \psi(x)}{\varepsilon} = -\psi'(x)$$
and hence $G = -\D{}/\D{x}$.
Applying the symmetry principle then gives $p=Q=-\im \hbar\,\D{}/\D{x}$.
}
\end{question}

\begin{question}
Find the eigenvalues and eigenstates of the momentum operator.
\solution{
We can write the eigenstate equation as $\bra{x} p \ket{\psi} = P \braket{x}{\psi}$, which implies
$$-\im \hbar \psi'(x) = P \psi(x).$$
Hence
$$\Dfrac{}{x} \ln \psi(x) = \im P/\hbar$$
and $\psi(x) = e^{\im P x/\hbar}$ is an eigenstate of eigenvalue $P$.
Since this works for every $P$, all real numbers are eigenvalues.
}
\end{question}

Note that momentum eigenstates cannot be normalised, just like position eigenstates.

You may be familiar with the fact that matrices in general don't commute, i.e.\ $AB\ne BA$.
The same is true for operators, in particular position and momentum operators don't commute!
To manipulate expressions with non-commuting objects it's useful to define the \emph{commutator}
$$[A,B] = AB - BA.$$

To make manipulations of expression a bit less tedious we will be sloppy with notation and equivocate between $\ket{\psi}$ and $\psi(x)$ and write, e.g., $T \psi(x)$ instead of $\bra{x} T \ket{\psi}$.
The effect of the position operator $q$ can then be written $q \psi(x) = x \psi(x)$.
Note that we have to be very careful in expressions with $\D{}/\D{x}$.
$\Dfrac{}{x} q$ is a sequence of operators that is equivalent to neither $0$ or $1$, as can be seen from applying it to $\psi(x)$ to get
$$\Dfrac{}{x} \left(q \psi(x)\right) = \Dfrac{}{x} \left(x \psi(x)\right).$$
To be sure you don't get confused, apply a state $\psi(x)$ on the right hand side before simplifying expressions.

\begin{question}
Calculate the commutator $[q,p]$ of the position and momentum operators.
\result{$\im \hbar$.}
\solution{
$$[q,p] \psi(x) = (-\im \hbar) \left(x \Dfrac{}{x} \psi(x) - \Dfrac{}{x} x \psi(x)\right) = (-\im \hbar) \left(x \Dfrac{}{x} \psi(x) - \psi(x) - x \Dfrac{}{x} \psi(x)\right) = \im \hbar \psi(x)$$
and hence $[x,p] = \im \hbar$.
}
\end{question}

This seemingly trivial calculation is a profound result, as can be seen from the following.

\begin{question}
Show that no state can be an eigenstate of both $q$ and $p$.
Hence in every state either position or momentum (or both!) are not perfectly defined, i.e.\ a measurement can give one of multiple values.
\solution{
If $q \ket{\psi} = q_0 \ket{\psi}$ and $p \ket{\psi} = p_0 \ket{\psi}$ then
$$[q,p] \ket{\psi} = (q p - p q) \ket{\psi} = (q_0 p_0 - p_0 q_0) \ket{\psi} = 0,$$
in contradiction with $[q,p] \psi(x)=\im \hbar \psi(x)$.
}
\end{question}

This is a hint towards the uncertainty principle, which is often (somewhat incorrectly) stated as that you can't measure both position and momentum to perfect accuracy.
We see from the derivation that the more correct expression is that it is impossible to even have a state that has both well-defined position and momentum to begin with!
We can define the ``spread'' of an observable in a state using the standard deviation:
$$\sigma_A = \sqrt{\langle (A - \langle A \rangle)^2\rangle}.$$
We now want to derive a bound on $\sigma_A \sigma_B$ but we need an intermediate result first.

\begin{question}
(Cauchy-Schwartz inequality)
Show that for any states $\ket{\psi}$ and $\ket{\phi}$ we have the inequality
$$|\braket{\psi}{\phi}|^2 \le \braket{\psi}{\psi} \braket{\phi}{\phi}.$$
\hint{Consider the operator $P = \ket{\phi} \bra{\phi}$. What are its eigenvalues?}
\solution{
The operator $P = \ket{\phi} \bra{\phi}$ has eigenvalues 0 and $\braket{\phi}{\phi}$.
The expectation value of $P$ is an average of the two and hence
$$0 \le \langle P \rangle \le \braket{\phi}{\phi}.$$
The expectation value in state $\psi$ is given by
$$\langle P \rangle = \frac{\bra{\psi} P \ket{\psi}}{\braket{\psi}{\psi}}$$
Hence
$$\frac{\braket{\psi}{\phi} \braket{\phi}{\psi}}{\braket{\psi}{\psi}} \le \braket{\phi}{\phi}.$$
}
\end{question}

\begin{question}
(Heisenberg uncertainty principle)
Find a lower bound on $\sigma_A \sigma_B$ for two Hermitian operators $A$ and $B$.
\hint{Define new operators that have zero mean (i.e.\ $\langle a \rangle = 0$) and evaluate $(\sigma_A \sigma_B)^2$.}
\hint{Use Cauchy-Schwartz and remember that $|x|^2 = (\Re x)^2 + (\Im x)^2$.}
\result{$|\langle [A,B] \rangle/2|.$}
\solution{
Define $a = A - \langle A \rangle$ and $b = B - \langle B \rangle$.
$${\sigma_A}^2 {\sigma_B}^2 = \langle a^2 \rangle \langle b^2 \rangle = \bra{\psi} a~a \ket{\psi} \bra{\psi} b~b \ket{\psi} \ge |\bra{\psi} a b \ket{\psi}|^2$$
$$\ge (\Im \bra{\psi} a b \ket{\psi})^2 = \frac{1}{4} |\bra{\psi} a b\ket{\psi} - (\bra{\psi} a b \ket{\psi})^*|^2 = \frac{1}{4} |\bra{\psi} (a b - b a) \ket{\psi}|^2 = \frac{1}{4} |\langle [a,b]\rangle|^2.$$
Note that $[a,b] = [A,B]$ and hence
$$\sigma_A \sigma_B \ge \left|\frac{\langle [A,B]\rangle}{2}\right|.$$
}
\end{question}

In particular, setting $a=q$ and $b=p$ we get the famous momentum-position uncertainty principle
$$\sigma_q \sigma_p \ge \frac{\hbar}{2}.$$
The similarity with the Fourier uncertainty principle $\sigma_t \sigma_\omega \ge 1/2$ is no coincidence since $q$ and $p$ are actually related by a Fourier transform!
\begin{question}
Express the ``momentum wavefunction'' $\phi(q) = \braket{p}{\psi}$ (where $\ket{p}$ is a momentum eigenstate of eigenvalue $p$) in terms of $\psi(x)$ and vice versa.
\solution{
The answer depends on the normalisation chosen for $\ket{p}$.
We choose $\braket{x}{p} = e^{\im p x/\hbar}$.
Then
$$\int \braket{x}{p} \braket{p}{y} \D{p} = \int e^{\im p (x-y)/\hbar}\D{p} = 2\pi \hbar\delta(x-y)$$
which we can state as a completeness relation
$$\int \ket{p} \bra{p} \frac{\D{p}}{2\pi \hbar} = 1.$$

We can now show that
$$\phi(p) = \braket{p}{\psi} = \int \braket{p}{x} \braket{x}{\psi} \D{x} = \int e^{-\im p x/\hbar} \psi(x) \D{x},$$
$$\psi(x) = \braket{x}{\psi} = \int \braket{x}{p} \braket{p}{\psi} \frac{\D{p}}{2\pi\hbar} = \int e^{\im p x/\hbar} \phi(p) \frac{\D{p}}{2\pi\hbar}.$$
}
\end{question}

The following properties of the commutator are very useful and worth remembering:
\begin{question}
Show that\\
1. $[A,BC]=[A,B]C+B[A,C]$ and $[AB,C] = [A,C]B+A[B,C]$,\\
2. $[f(q),p]=\im \hbar f'(q)$,\\
3. $[q, f(p)]=\im \hbar f'(p)$.

If you're unhappy with functions of operators, think of them as power series $f(q) = a_0 + a_1 q + a_2 q^2 + \dots$.
\solution{
1.
$$[A,BC]=ABC-BCA = ABC-BAC+BAC-BCA = [A,B]C + B[A,C]$$
$$[AB,C]=ABC-CAB = ACB-CAB+ABC-ACB = [A,C]B+A[B,C]$$

2.
Writing
$$f(q) = a_0 + a_1 q + a_2 q^2 + a_3 q^3 + \dots$$
implies that $f(q) \psi(x) = f(x) \psi(x)$.
Hence
$$[f(q),p] \psi(x) = (-\im \hbar) \left(f(x) \Dfrac{}{x} \psi(x) - \Dfrac{}{x} f(x) \psi(x)\right) = \im \hbar f'(x) \psi(x)$$
which implies $[f(q),p] = \im \hbar f'(q)$.

3.
$$f(p) = a_0 + a_1 p + a_2 p^2 + a_3 p^3 + \dots$$
It's easiest to show this using
$$[q,p^n] = [q,p p^{n-1}] = [q,p] p^{n-1} + p [q,p^{n-1}] = \im \hbar p^{n-1} + p [q,p^{n-1}],$$
after applying the relation $n$ times we get
$$[q,p^n] = \im \hbar n p^{n-1} + p [q,1] = \im \hbar n p^{n-1}.$$
Hence
$$[q,f(p)] = \im \hbar (a_1 + 2 a_2 p + 3 a_3 p^2 + \dots) = \im \hbar f'(p).$$
}
\end{question}

\subsection{Time}
We now want to introduce time.
We will do this by adding time dependence to our states: $\ket{\psi}$ becomes $\ket{\psi(t)}$.
We define a time translation (also called time evolution) operator $U(t)$ by
$$U(\varepsilon) \ket{\psi(t)} = \ket{\psi(t+\varepsilon)}.$$
Since $U(0) = 1$ we can again define the generator $G$ just like before.
\begin{question}
Show that the generator $G$ is $\pfrac{}{t}$.
\hint{Apply $G$ to a state $\ket{\psi}$.}
\solution{
$$G \ket{\psi(t)} = \lim_{\varepsilon \to 0} \frac{\ket{\psi(t+\varepsilon)} - \ket{\psi(t)}}{\varepsilon} = \pfrac{}{t} \ket{\psi(t)}.$$
}
\end{question}
\begin{question}
(optional)
Using the earlier formula, show that, for the classical one dimensional particle in a potential $V(x)$, time translation corresponds to total energy $H$.
\hint{Note that $L$ is \emph{not} invariant but rather changes to $L+\D{\Lambda}/\D{t}$.}
\solution{
The formula for one dimension is
$$Q = \pfrac{L}{\dot x} \Dfrac{x}{\varepsilon} - \Dfrac{\Lambda}{\varepsilon}.$$
Note that $\D{x}/\D{\varepsilon} = \dot x$ and $L \to L + \D{L}/\D{t}~\varepsilon$, hence $\Lambda = L$.
This gives
$$Q = m \dot x^2 - L = \frac{1}{2} m \dot x^2 + V.$$
This is just the formula for total energy $H$.
}
\end{question}

We can now apply our principle $Q=\im \hbar G$ to find the \emph{time-dependent Schr\"odinger equation}
$$H \ket{\psi(t)} = \im \hbar \pfrac{}{t} \ket{\psi(t)}.$$
Here $H$ is an operator called \emph{Hamiltonian} that evaluates to the total energy\footnote{There are actually some situation where this analysis fails and the Hamiltonian is not equal to the total energy. In this case one has to derive it from Hamiltonian mechanics. We will not encounter this problem.}.

\begin{question}
Show that if $\ket{\psi}$ is an eigenstate of $H$ the time evolution is particularly simple.
\solution{
If $H \ket{\psi} = E \ket{\psi}$ then the equation is
$$E \ket{\psi(t)} = \im \hbar \pfrac{}{t} \ket{\psi(t)}$$
with the solution 
$$\ket{\psi(t)} = \ket{\psi(0)} e^{-\im E t/\hbar}.$$
}
\end{question}

Note that this result means that the state remains unchanged except for a phase factor.
We thus call the eigenstates of $H$ \emph{stationary states}.

Since we assume that the eigenstates of an observable (such as $H$) form a complete set, we can expand \emph{any} state in terms of stationary state and find the time evolution this way.

The equation for stationary states
$$H \ket{\psi} = E \ket{\psi}$$
is often called the \emph{time-independent Schr\"odinger equation}.

To get an expression for $H$ in terms of other operators, we need to determine what the physics of our system is.
For instance for a 1D particle in a potential $V(x)$ we know that classically
$$H = \frac{p^2}{2m} + V(x).$$
We can transfer this directly into quantum mechanics by replacing $p$ with the operator $p=-\im \hbar~\D{}/\D{x}$.
In general they may be some operator ordering ambiguity (e.g.\ should $pq$ in a classical Hamiltonian by replaced by $pq$ or $qp$?).
In that case there may be no unique quantum system corresponding to the classical system and it's ultimately up to experiment to distinguish the two.

\begin{question}
Write the two Schr\"odinger equations in this case in terms of the wavefunction $\psi(x,t)$.
\solution{
The time-independent Schr\"odinger equation is
$$-\frac{\hbar^2}{2m} \Dfrac{^2}{x^2} \psi(x) + (V(x) - E) \psi(x) = 0.$$
The time-dependent Schr\"odinger equation is
$$-\frac{\hbar^2}{2m} \pfrac{^2}{x^2} \psi(t,x) + V(x) \psi(t,x) = \im \hbar \pfrac{}{t} \psi(t,x).$$
}
\end{question}

When working with wavefunctions it's important to remember that we want physical states.
This usually means that they must be normalisable, i.e.\ that $\braket{\psi}{\psi}$ is finite.
We already saw that we sometimes make exceptions for this. States such as $\ket{x}$ and $\ket{p}$ are useful because they let us expand physical states in terms of them.
They can also be approximated arbitrarily well by physical states.
However if you find a stationary state where $\psi(x)$ grows exponentially as $x \to \infty$, feel free to discard it -- it's so different from a physical state it's not relevant.

\begin{question}
Show that the eigenstates for a free particle ($V(x) = 0$) are just momentum eigenstates and that the time evolution corresponds to the propagation of waves.
Find equations for the wavevector $k$, the frequency $\omega$, the propagation speed $c_p$.
\solution{
$H = p^2/(2m)$ depends only on one operator, viz. $p$, and so has the same eigenstates as $p$.
A $p$ eigenstate is
$$\psi(x) = e^{\im P x/\hbar}.$$
The time evolution is thus just
$$\psi(x) = e^{\im (P x-E t)/\hbar},$$
with $E=P^2/2m$.
This is just the equation for a plane wave.
Comparing with $e^{\im (k x - \omega t)}$ we conclude that
$$P=\hbar k\quad\mbox{(de-Broglie relation),}$$
$$E=\hbar \omega \quad\mbox{(Planck-Einstein relation),}$$
$$c_p = \frac{\omega}{k} = \frac{E}{P} = \frac{P}{2m}.$$
Note that this is just 1/2 of the classical speed $v = P/m$.
}
\end{question}

Note that we thus naturally find that particles can actually act like waves!
This ``wave-particle duality'', while very bizarre classically, has actually been observed with electrons and even with larger objects such as ``buckyballs'' (sphere-shaped $\rm C_{60}$ molecules).
(How do you observe wave behaviour? By double-slit experiments of course!)
% TODO: Add reference.

However, we found that the wave propagation velocity was \emph{not} equal to the classical propagation speed.
This is a bit puzzling but we shouldn't be too surprised because momentum eigenstates are not physical states.
$|\psi|^2$ is a constant and so the particle is actually \emph{completely delocalised}.
To construct real physical states we need to construct ``wavepackets'', by multiplying the waves with a localised function, such as a Gaussian, to arrive at
$$\psi(x,0) = e^{-x^2/(2\sigma^2)} e^{\im p_0 x/\hbar}.$$
This wavepacket consists of many plane wave states, each moves at the ``phase velocity'' $\omega/k = E/p = v/2$.
The packet's overall shape on the other hand moves with $\D{\omega}/\D{k} = \D{E}/\D{p} = v$, in agreement with classical mechanics.
This speed is called the ``group velocity'' and it's the speed that energy and information travel through waves.
The two following questions prove this for quantum mechanical waves, first (tediously but precisely) for Gaussian wavepackets and then (approximately) for arbitrary wavepackets.

\begin{question}
(optional, requires Fourier transform, tedious)
We want to find the time evolution of a Gaussian wavepacket
$$\psi(x,0) = e^{-x^2/(4\sigma^2)} e^{\im p_0 x/\hbar}.$$
Find the Fourier transform $\phi(p,0)$ and the time evolution $\phi(p, t)$.
Calculate $\psi(x,t)$ and find an expression for $|\psi(x,t)|^2$.
Interpret the result physically.
\hint{To interpret the result, remember the position-momentum uncertainty relation.}
\result{
$$\psi(x,t) \propto \exp\left(-\frac{(x - p_0 t/m)^2}{2(\sigma^4 + t^2 \hbar^2/(4m^2\sigma^2))}\right)$$
}
\solution{
We will drop overall normalisation factors throughout the derivation.
For $\phi(p,0)$ we find that
$$\phi(p,0) = e^{-\sigma^2 (p-p_0)^2/\hbar^2}$$
$\phi(p,t)$ has an extra $e^{-\im Et/\hbar}$ factor, i.e.\
$$\phi(p,t) = \exp\left(-\frac{\sigma^2 (p - p_0)^2}{\hbar^2} - \frac{\im p^2 t}{2m\hbar}\right) = \exp\left(-\left(\frac{\sigma^2}{\hbar^2} + \frac{\im t}{2m\hbar}\right) p^2 + \frac{2 \sigma^2 p p_0}{\hbar^2}  - \frac{\sigma^2 p_0^2}{\hbar^2}\right)$$
We showed earlier that
$$\int e^{\im \omega p - \varepsilon p^2} \D{p} = \sqrt{\frac{\pi}{\varepsilon}} e^{-\omega^2/(4 \varepsilon)}.$$
The inverse Fourier transform now corresponds to $\varepsilon = \sigma^2/\hbar^2 + \im t/(2m\hbar)$ and $\omega = x/\hbar - 2 \im \sigma^2 p_0/\hbar^2$.
The result is
$$\psi(x,t) = \exp\left(-\frac{\left(\frac{x}{\hbar} - \frac{2\im \sigma^2 p_0}{\hbar^2}\right)^2}{4\left(\frac{\sigma^2}{\hbar^2} + \frac{\im t}{2m\hbar}\right)}\right) = \exp\left(-\frac{(\hbar x - 2 \im \sigma^2 p_0)^2}{4 \hbar^2 (\sigma^2 + \im t \hbar/(2m))}\right).$$
The absolute value is
$$|\psi(x,t)|^2 = \exp\left(-\Re \frac{(\hbar x - 2 \im \sigma^2 p_0)^2}{\hbar^2 (\sigma^2 + \im t \hbar/m)}\right) = \exp\left(-\frac{\Re (x\hbar - 2\im \sigma^2 p_0)^2 (\sigma^2 - \im t \hbar/(2m))}{\hbar^2 (\sigma^4 + t^2 \hbar^2/m^2)}\right).$$
$$=\exp\left(-\frac{\sigma^2 x^2 \hbar^2 - 4\sigma^6 p_0^2 - x t \hbar^2 \sigma^2/m}{2\hbar^2(\sigma^4 + t^2 \hbar^2/(2m^2))}\right) = \exp\left(-\frac{(x - p_0 t/m)^2}{2(\sigma^4 + t^2 \hbar^2/(4m^2\sigma^2))} + \mbox{const}\right).$$
Note that the result is a Gaussian with a centre moving at the classical velocity $v = p_0/m$.
The spread of the Gaussian increases according to
$$\sigma(t)^2 = \sigma(0)^2 + \left(\frac{t \hbar}{2m \sigma}\right)^2.$$
To interpret this, remember that the uncertainty principle implies that that the initial momentum is uncertain by $\sigma_p = \hbar/(2\sigma)$.
This contributes $\sigma_p t/m$ to the uncertainty in the final position.
Since the two contributions are independent they add in quadrature to give the result.
}
\end{question}

\begin{question}
(optional, requires Fourier transform)
Consider an arbitrary wavepacket $\psi(x,t)$.
We assume it has a well-defined momentum, i.e.\ $\phi(p,t)$ is approximately zero unless $p \approx p_0$.
Write down the time evolution of $\phi(p,t)$ and expand the term in the exponential around $p-p_0$ to first order.
Use this to show that $\psi(x,t)$ moves at $p_0/m$.
\solution{
$$\phi(p,t) = \phi(p, 0) e^{-\im p^2 t/(2m \hbar)} = \phi(p, 0) e^{-\im p_0^2 t/(2m\hbar) - \im (p - p_0) p_0 t/(m \hbar) + O((p-p_0)^2)}$$
$$= \phi(p, 0) e^{-\im p p_0 t/(m \hbar)} e^{\im p_0^2 t/(2m\hbar)} + O((p-p_0)^2)$$
The inverse Fourier transform of the first two terms is the convolution of $\psi(x,0)$ with $\delta(x - p_0 t/m).$
We thus get
$$\psi(x,t) = \psi(x - p_0 t/m, 0) e^{\im p_0^2 t/(2m\hbar)},$$
i.e.\ the particle moves with velocity $p_0 t/m$.
}
\end{question}

\begin{question}
(optional)
Special relativity does not, in general, play well with the basic ideas of quantum mechanics discussed so far.
However it turns out that one can still talk about plane wave wavefunctions of the form $e^{\im (P x - E t)/\hbar}$ with the energy replaced using the relativistic expression $E^2 = p^2 c^2 + m^2 c^4$.
Find the phase and group velocity in this case.
You may find the formula $p = m v/\sqrt{1-v^2/c^2}$ useful.
\solution{
The phase velocity is
$$\frac{E}{p} = \sqrt{\frac{m^2 c^4 (1 - v^2/c^2) + m v^2 c^2}{m^2 v^2}} = \frac{c^2}{v}.$$
Note that since $v < c$, the phase velocity is \emph{always faster than the speed of light}.
The group velocity is
$$\Dfrac{E}{p} = \frac{p c^2}{\sqrt{p^2 c^2 + m^2 c^4}} = \frac{p}{E} c^2 = v.$$
}
\end{question}

\begin{question}
(optional)
A free particle has an initial momentum-space wavefunction $\phi_0(p)$.
Show that for very large times the position-space wavefunction $\psi(x)$ is proportional to $\phi_0(\alpha x)$ for some $\alpha = \alpha(t)$.
\hint{Integrals of the form $\int f(x) e^{\im \theta(x)} \D{x}$ with highly variable $\theta(x)$ can be approximated by the method of stationary phase: only the points where $\D{\theta}/\D{x} = 0$ contribute to the integral.}
\solution{
We again have
$$\phi(p,t) = \phi_0(p) e^{-\im p^2 t/(2m \hbar)}.$$
The position-space wavefunction is given by
$$\psi(x,t) = \int \phi_0(p) e^{-\im p^2 t/(2m \hbar) + \im p x/\hbar} \frac{\D{\vec p}}{2\pi\hbar}.$$
For large $t$ the $p^2 t$ is highly oscillatory and so we can now approximate the integral by the method of stationary phase: 
We assume that the fast oscillation cancel the integral everywhere, except when the phase is stationary (because, e.g., it goes through an extremum).
Here this happens when $p t/(m) = x$ and so we conclude
$$\psi(x,t) \sim \phi_0(x m/t).$$
Note that this result makes a lot of intuitive sense -- at large $t$ a particle that originally had position $x_0$ and velocity $v_0$ will be found at position $v_0 t \gg x_0$.
The equation for the densities -- in quantum mechanical notation $|\psi(x,t)|^2 \sim |\phi_0(xm/t)|^2$ -- thus holds for a cloud of classical non-interacting particles as well.
}
\end{question}

Before we move on to some specific examples of simple quantum 1D systems, there are some general theorems we can prove about time evolution.

\begin{question}
Find an equation for the time derivative of the expectation value
$$\Dfrac{}{t} \langle Q(t) \rangle,$$
where the state evolves according to the Schr\"odinger equation.

Apply your result to the position operator and to the momentum operator for a particle moving in a potential $V(x)$ in one dimension.
\solution{
$$\Dfrac{}{t} \langle Q(t) \rangle = \pfrac{\bra{\psi}}{t} Q(t) \ket{\psi} + \bra{\psi} \pfrac{Q(t)}{t} \ket{\psi} + \bra{\psi} Q(t) \pfrac{\ket{\psi}}{t}$$
$$= - \bra{\psi} \frac{H}{\im \hbar} Q(t) \ket{\psi} + \bra{\psi} Q(t) \im \hbar \frac{H}{\im \hbar} \ket{\psi} + \left\langle \pfrac{Q}{t}\right\rangle$$
$$= \frac{1}{\im \hbar} \left\langle [Q(t), H]\right\rangle + \left\langle \pfrac{Q}{t}\right\rangle.$$

For the position operator we have
$$[q, H] = \im \hbar \pfrac{H}{p}$$
by an earlier theorem.
For $H=p^2/(2m) + V$ we get $[q,H] = \im \hbar p/m$ and so
$$\Dfrac{}{t} \langle q(t) \rangle = \frac{\langle p \rangle}{m}$$
(which agrees with both classical mechanics and our earlier wave calculations).

For the momentum operator we have
$$[p, H] = -\im \hbar \pfrac{H}{q} = -V'(q)$$
and hence
$$\Dfrac{}{t} \langle p(t) \rangle = -\langle V'(q)\rangle,$$
which is very similar to Newton's second law.
}
\end{question}

\begin{question}
What does the last question imply about conserved quantities?
In particular, show that operators corresponding to symmetry operations and the generators of symmetries are conserved.
\hint{For the second part: What do you get when you apply a symmetry transformation to a stationary state?}
\solution{
A quantity is conserved if $\D{\langle Q\rangle}/\D{t} = 0$.
If $Q$ itself does not depend on time this implies that $[Q,H] = 0$, i.e.\ $Q$ commutes with the Hamiltonian.

If $\ket{\psi}$ is a stationary state, then, if the system is truly symmetric, $T \ket{\psi}$ has to be a stationary state, too (not necessarily a different one).
They also need to have the same energy, i.e.\
$$H \ket{\psi} = E \ket{\psi}, \qquad H T \ket{\psi} = E T \ket{\psi}.$$
Applying $T$ to the first equation gives $T H \ket{\psi} = E T \ket{\psi} = H T \ket{\psi}$.
Hence $[T,H]\ket{\psi} = 0$ for stationary states.
But \emph{any} state can be expanded in stationary states and so $[T,H] = 0$.

For a generator $G$ we have
$$[G,H] = \lim_{\varepsilon \to 0} \frac{1}{\varepsilon} [T_\varepsilon-1,H] = \lim_{\varepsilon \to 0} \frac{1}{\varepsilon} [T_\varepsilon,H] = 0.$$

}
\end{question}

(There is a slight subtlety in terminology: An operator can be conserved without being an observable!)

\begin{question}
Assume that $A$ and $B$ commute and that $\ket{a}$ is a nondegenerate eigenstate of $A$, i.e.\ that $\ket{a}$ is the only eigenstate of eigenvalue $a$.
Show that it's an eigenstate of $B$.
\hint{Show that $B \ket{a}$ is an eigenstate of $A$ of eigenvalue $a$.}
\solution{
$$A B \ket{a} = B A \ket{a} = a B \ket{a}.$$
This means $B \ket{a}$ is an eigenstate of $A$ of eigenvalue $a$.
Since $\ket{a}$ is assumed non-degenerate, $B \ket{a} = b \ket{a}$ for some $b$.
}
\end{question}

\begin{question}
(optional)
Show that if (and only if) two observables $A$ and $B$ commute, they permit a simultaneous basis of eigenstates, i.e.\ there is a basis where each state is an eigenstate of both operators.
\hint{To prove ``if'', show that $\bra{a'} B \ket{a} = 0$ if $\ket{a}$ and $\ket{a'}$ are $A$ eigenstates of different eigenvalue.}
\solution{
``Only if'': If there is a simultaneous basis of eigenstates, then for every eigenstate $A \ket{\psi} = a \ket{\psi}, B \ket{\psi} = b \ket{\psi},$
hence $AB \ket{\psi} = ab \ket{\psi} = B A \ket{\psi}$.
Since the $\ket{\psi}$ states form a basis, we can expand any state in them and $A$ and $B$ commute for all states.

``If'': Take a basis of eigenstates of $A$ with $A \ket{a} = a \ket{a}$.
Do the following for all the different eigenvalues $a$.
You can use the same argument as in the last question to show that $B \ket{a}$ is an eigenstate of $A$ of eigenvalue $a$.
Since $\braket{a}{a'} = 0$ if $a \ne a'$, this means that we can expand $B \ket{a}$ in eigenstates of eigenvalue $a$.
By doing this for all eigenstates $\ket{a}$ of that eigenvalue we get a matrix $\bra{a'} B \ket{a}$.
We can find eigenvectors of this matrix, i.e.\ vectors $\ket{ab}$ that obey $B \ket{ab} = b \ket{ab}$.
But since we worked entirely with vectors that obey $A \ket{a} = a \ket{a}$ we also have $A \ket{ab} = a \ket{ab}$.
}
\end{question}

This result is important (even though the proof is not).
In particular, we can use it to show the following.

\begin{question}
Show that we can label energy eigenstates with the eigenvalues of symmetry operations.
As an example, apply it to a system which obeys $V(x) = V(-x)$.
\solution{
$H$ commutes with $T$ and so $T$ and $H$ have a simultaneous eigenbasis.
Hence we can find a basis of energy eigenstates where each eigenstate is an eigenstate of $T$ with an eigenvalue.

For the example use the operator $P$ that maps $x$ to $-x$.
The system is invariant under it and so $P$ commutes with $H$.
Hence we can choose all energy eigenstates to be eigenvalues of $P$.
To find the eigenvalues of $P$ note that $P^2 = 1$ and therefore the eigenvalues are $\pm 1$.
We can thus form a basis of eigenstates, all either even or odd.

This is sometimes stated as ``all eigenstates are either even or odd'', which is only true for non-degenerate eigenstates.
If $\psi_1$ is even, $\psi_2$ is odd and both have the same energy, then $\psi_1 + \psi_2$ is neither even nor odd, but is still an energy eigenstate.
}
\end{question}

One final thing.
Rememember that we can interpret $|\psi|^2$ as a probability density $\rho$.
There is a useful equation for the time evolution of this probability density.

\begin{question}
Use the time-dependent Schr\"odinger equation to find an expression for $\partial{\rho}/\partial{t}$ with $\rho = |\psi|^2$.
Your result should be of the form
$$\pfrac{\rho}{t} + \pfrac{J}{x} = 0.$$
Integrate this equation from $a$ to $b$ to interpret the result physically.
What's the meaning of $J$?
\solution{
$$\pfrac{\psi^* \psi}{t} = \pfrac{\psi^*}{t} \psi + \psi^* \pfrac{\psi}{t} = \frac{1}{\im \hbar} \frac{\hbar^2}{2m} \pfrac{^2 \psi^*}{x^2} \psi - \frac{1}{\im \hbar} V \psi^* \psi - \frac{1}{\hbar} \frac{\hbar^2}{2m} \psi^* \pfrac{\psi}{x^2} + \frac{1}{\im \hbar} V \psi^* \psi$$
$$= \pfrac{}{x} \frac{-\im \hbar}{2m} \left(\pfrac{\psi^*}{x} \psi - \psi^* \pfrac{\psi}{x}\right)$$
Hence
$$J = \frac{-\im \hbar}{2m} \left(\psi^* \pfrac{\psi}{x} - \psi \pfrac{\psi^*}{x}\right).$$
Integrating the equation gives us
$$\pfrac{}{t} \int_a^b \rho \D{t} = J(a) - J(b),$$
i.e.\ the amount of probability density in a region changes at rate $J(a) - J(b)$.
$\rho$ is then a conserved quantity (we have the same equation for charge density in electrodynamics or mass density in fluid mechanics).
We can interpret $J$ as a ``probability current''.
}
\end{question}

\begin{question}
Find $J$ for a momentum eigenstate $\ket{p}$.
Normalise the state so that $\rho = \rho_0$.
\solution{
Use $\psi(x) = \sqrt{\rho_0} e^{\im p x/\hbar}$.
$$J = \frac{-\im \hbar}{2m} \left(\sqrt{\rho_0} e^{-\im p x/\hbar} \sqrt{\rho_0} \frac{\im p}{\hbar} e^{\im p x/\hbar} - \sqrt{\rho_0} e^{\im p x/\hbar} \sqrt{\rho_0} \frac{-\im p}{\hbar} e^{\im k x}\right) = \rho_0 \frac{p}{m},$$
i.e.\ the current is just density times velocity (just like in fluid mechanics!).
}
\end{question}

\subsection{Stationary states}
We now want to study the time-independent Schr\"odinger equation
$$-\frac{\hbar^2}{2m} \pfrac{^2 \psi}{x^2} + (V - E) \psi = 0$$
for different potentials $V$.
The simplest potential we can actually solve is the ``particle in a box'' with $V(x) = 0$ for $0 < x < a$ and $V(x) \to \infty$ otherwise.
\begin{question}
Find the stationary states (and their energies) for the particle in a box.
\solution{Note that we need $\psi(x) = 0$ in the region $V(x) \to \infty$.
Note that we need $\psi(x) = 0$ in the region $V(x) \to \infty$.
Within the ``box'' we have $(-\hbar^2/(2m))\psi''(x) = E \psi(x)$ which is solved by
$$\psi_n(x) = \sin\left(\frac{\pi n x}{a}\right)$$
with integer $n \ge 1$ and energy
$$E_n = \frac{\hbar^2 \pi^2 n^2}{ a^2}.$$
}
\end{question}

Note that even though $V$ was highly discontinuous, $\psi$ was continuous. This is a general feature of $\psi$.
\begin{question}
(a) We expect physical states to have finite kinetic energy $\langle p^2/(2m)\rangle$.
Show that this implies that $\psi$ cannot be discontinuous.\\
(b) Show that for a stationary state $\psi'(x)$ can only be discontinuous where $V(x)$ is infinite.
\hint{(a) Put the discontinuity near 0 and write $\psi$ of the form $a + \Theta(x) b$ where $\Theta(x)$ is the Heaviside step function.}
\solution{
(a) Note that
$$\bra{\psi} \frac{p^2}{2m} \ket{\psi} \propto \left(\pfrac{}{x} \bra{\psi}\right) \left(\pfrac{}{x} \ket{\psi}\right) = \int (\psi'(x))^2 \D{x}.$$
If $\psi$ was discontinuous around 0 we could write, near 0, $\psi(x) = a + \Theta(x) b$. Hence $\psi'(x) = \delta(x) b$.
Then the kinetic energy has a contribution
$$\int b^2 \delta^2(x) \D{x} = b^2 \delta(0),$$
which is infinite unless $b = 0$.

(b)
The Schr\"odinger equation has a term proportional to $\psi''$.
If $\psi'$ was discontinuous at 0, this term would contribute a term proportional to $\delta(x)$.
We already know that $\psi$ must be continuous and hence $\psi(0)$ must be finite.
For the left-hand side to equal zero, there must then be term infinite near zero in $V(x)$ to cancel the $\delta(x)$.
}
\end{question}

The second solvable potential is kind of an inverse situation -- an infinitely narrow well $V(x) = \alpha \delta(x)$ ($\alpha$ is real but can have either sign).

\begin{question}
Find the stationary states for the $\delta$-function potential.
Make sure to consider both positive and negative energy states.
\hint{Write down a general solution. Note that $\psi'$ is discontinuous and you need to treat two regions separately.}
\hint{Integrate from $-\varepsilon$ to $\varepsilon$ for small $\varepsilon$ to get rid of the $\delta$.}
\result{
For $E>0$ there are two independent states for each value of $E$. For $E < 0$ there is one localised state at $E=-m\alpha^2/(2\hbar^2)$ if $\alpha < 0$.
}
\solution{
In this case
$$-\frac{\hbar^2}{2m} \psi''(x) + (\alpha \delta(x) - E) \psi(x) = 0.$$
Since $V(x)$ is infinite near zero $\psi'(x)$ can be discontinuous at $x=0$ ($\psi(x)$ is still continuous).
For $x \ne 0$ we again have plane waves and we can write
$$\psi(x) = \left\{\begin{matrix}a e^{\im k x} + b e^{-\im k x} & x < 0\\c e^{\im k x} + d e^{-\im k x} & x > 0\end{matrix}\right.$$
with $\hbar^2 k^2/(2m) = E$ and $a+b=c+d$.
To solve the equation we can integrate around 0 to get
$$-\frac{\hbar^2}{2m} (\psi'(+\varepsilon) - \psi'(-\varepsilon)) + \alpha \psi(0) + O(\varepsilon) = 0.$$
In the limit $\varepsilon \to 0$ this gives
$$-\frac{\im E}{k} (c - d - a + b) + \alpha (a + b) = 0.$$
We thus have two equations for four parameters.
We can solve for $b$ and $c$ to get
$$b = \frac{2E d-\im k\alpha a}{2E+\im k\alpha}, \qquad c = \frac{-\im k \alpha d + 2 E a}{2E+\im k\alpha}.$$
For every energy $E>0$ there are then two basis states -- similar to how we have left and right going states for a free particle, which we can mix to get more complex states.
In particular we can form physical wavepacket states by mixing states of similar energies.

If $d=0$ we have a wave incoming from the left that is both reflected and transmitted (similar to light at an interface), with reflection coefficient $r = -\im k \alpha/(2E+\im k \alpha)$ and transmission coefficient $t = 2E/(2E+\im k \alpha)$.

For energies $E<0$ things get more interesting.
To make a normalisable state we cannot allow states that grow exponentially.
If we choose $k/\im$ to be positive, then $a=0$ and $d=0$.
The above solutions now seem to imply that $b=c=0$, meaning there is no solution.
There is however one ``loophole'': the denominator is zero if $2E=-\im k \alpha = |k| \alpha$.
This works only if $\alpha < 0$ and 
$$E = -\frac{m \alpha^2}{2\hbar^2}.$$
We can write the wavefunction as
$$\psi(x) = e^{-m \alpha |x|/\hbar^2}.$$
}
\end{question}

The last question showed the first glimpse of a general pattern.
Note that $V(x)$ was finite as $x \to \pm \infty$, in particular it approached $V_0 = 0$.
Potentials that obey this condition show a dichotomy of states: There are \emph{bound} states ($E < V_0$) and \emph{scattering} states ($E > V_0$) (the lowest energy bound state is usually called the ``ground state'', the one above it the ``first excited state'' etc.).

\begin{question}
Assume that $V(x) \to 0$ at infinity.
Show that\\
1. bound states are localised, i.e.\ $\psi$ approaches zero at infinity, whereas scattering states approach plane wave states,\\
2. if $V(x)$ has a lower bound $V_{\rm min}$, then the energy of the ground state is $\ge V_{\rm min}$,\\
3. all the bound states can be chosen to be real ($V(x)$ is real).\\
(optional) Also make qualitative arguments to argue that\\
4. bound states are discrete in energy, whereas scattering states are continuous,\\
5. if $V(x)$ is finite except for some number of poles (e.g.\ $V(x)=-1/|x|$), the ground state energy is still bounded from below,\\
6. if $V(x)$ has no poles, the ground state has no nodes ($x$ for which $\psi(x)=0$) and the $n$-th excited state has more nodes than the states below it.\\
\hint{4. Think a bit intuitively about what parameters determine $\psi$ for a given potential and what needs to happen to get a physical state. What does the set of valid parameters look like in the $n$-dimensional parameter space?}
\hint{5. Remember the uncertainty principle.}
\hint{6. Think about the effect of nodes on the kinetic energy.}
\solution{
1. We can neglect $V$ for large $|x|$. Then the Schr\"odinger equation is simply the free equation
$$-\frac{\hbar^2}{2m} \psi'' = E \psi.$$
If $E<0$ this implies $\psi$ grows or falls exponentially.
A physical state can't grow exponentially, hence it must fall.
For $E>0$ the solution is a plane wave.

2. Write $H=T+V$. $\langle T \rangle \ge 0$ and so $\langle H \rangle \ge \langle V \rangle$.
If $V \ge V_{\rm min}$, then (if $\psi$ is normalised)
$$\langle V \rangle = \int V(x) |\psi|^2 \D{x} \ge V_{\rm min} \int |\psi|^2 \D{x} = V_{\rm min}.$$
Hence $\langle H \rangle \ge V_{\rm min}$.

3. Take an arbitrary bound state $\psi$. Taking the complex conjugate of the Schr\"odinger equation shows that $\psi^*$ is a solution of the same energy. If $\psi = \psi^*$ then $\psi$ is real and we are done.
Otherwise $(\psi + \psi^*)/2$ and $(\psi - \psi^*)/(2\im)$ are both real, non-zero and linear combinations of bound states of the same energy, hence they are bound states themselves and can replace $\psi$ and $\psi^*$.

4. The Schr\"odinger equation has one parameter $E$ and is a second order equation.
We thus have three parameters to vary, $E$, $\psi(0)$ and $\psi'(0)$.
For $E < 0$ we showed in 1.\ that the wavefunction must fall exponentially at infinity, which implies two conditions (one for $+\infty$ and another for $-\infty$).
We can also normalise states, which implies another condition.
With three conditions for three unknowns, the best we can hope for is a set of points in the solution space.
Hence the energies are discrete.
For $E > 0$ the wavefunction is largely arbitrary at infinity.
No matter what we set $\psi(0)$ and $\psi'(0)$ to we will get a valid state at infinity.
Hence we have a continuous set of states.

5.
To get very low potential energy we would have to confine the wavefunction very narrowly near the poles.
But that implies, by the uncertainty principle, that $\sigma_p$ is very high.
Since $\langle T \rangle = \hbar^2/(2m) \langle p^2 \rangle = \hbar^2 {\sigma_p}^2/(2m)$ this implies the kinetic energy is very high, which cancels out the negative potential energy.

6.
The Schr\"odinger equation implies that $\psi'' = 0$ if $\psi = 0$.
Hence $\psi'$ is extremal at a node.
Because $\psi \to 0$ at $\pm \infty$, this means $|\psi'|^2$ is maximal at a node.
Thus, each node contributes a ``peak'' to the kinetic energy integral $\int |\psi'|^2 \D{x}$.
A state with more nodes then has more kinetic energy.
In particular we can take the ground state and ``excise'' any nodes by flipping one half and smoothing the cusp at the node.
This new state has basically unchanged potential energy, since that is $\int V(x) |\psi|^2 \D{x}$ but lower kinetic energy because there no longer is a maximum of $|\psi'|^2$.
We have then a state of lower energy, which is impossible (you might think that there is a catch in the argument since the new state might not be a stationary state, but see the variational principle below).
}
\end{question}

\begin{question}
(optional)
Assume again that $V(x) \to 0$ at infinity.
We are interested in scattering states.
Write $\psi(x)$ at infinity in terms of ingoing and outgoing waves and show that, for fixed energy $E$, there is a matrix relationship
$$\begin{pmatrix}a_{L-}\\a_{R-}\end{pmatrix} = S \begin{pmatrix}a_{L+}\\a_{R+}\end{pmatrix},$$
where $a_{L-}$ is the outgoing coefficient on the left (at $x \to -\infty$) etc.
Show that $S$ is unitary.
$S$ is (unimaginatively) called the $S$-matrix.
Find $S$ for the $\delta$-function potential.
Can you see, in this special case, any way that $S$ tells you something about the bound state?

\hint{To show that $S$ is unitary use the probability current $J$.}
\solution{
Write
$$\psi(x) = \left\{\begin{matrix} a_{L-} e^{-\im k x} + a_{L+} e^{\im k x} & x \to -\infty\\a_{R-} e^{\im k x} + a_{R+} e^{-\im k x} & x \to \infty\end{matrix}\right.$$
Since a solution to $\psi$ is uniquely determined by the values $\psi(0)$ and $\psi'(0)$, there must be two equations relating the four coefficients.
Since the wavefunction is linear in $\psi$, these equations are linear, too, and we can hence solve them to find a matrix $S$.

If $J$ is the probability current, then
$$\Dfrac{\rho}{t} + \Dfrac{J}{x} = 0.$$
But we have a stationary state and so the first term is zero, hence $J = \mbox{const}$.
Calculating $J$ at $-\infty$ and $+\infty$ gives 
$$|a_{L+}|^2 - |a_{L-}|^2 = |a_{R-}|^2 - |a_{R+}|^2$$
or
$$|a_{L+}|^2 + |a_{R+}|^2 = |a_{L-}|^2 + |a_{L-}|^2,$$
which means that $S$ conserves the lengths of vectors, hence it must be unitary.

For the $\delta$ potential we have
$$S = \frac{1}{2 E + \im k \alpha} \begin{pmatrix}-\im k \alpha & 2E\\2E & -\im k \alpha\end{pmatrix}.$$
Note that $S$ has a pole for the bound state $k=-\im m \alpha/\hbar^2$.
This is to be expected -- $k$ has to be imaginary and either the ingoing or outgoing waves must be zero, while the other set is non-zero.
}
\end{question}

\begin{question}
(Variational method)
Show that the energy expectation value $\langle H \rangle$ for \emph{any} state (not necessarily an energy eigenstate) is $\ge$ than the energy of the ground state $E_0$.
We can then approximate the ground state by using a trial wavefunction with parameters and minimising $\langle H \rangle$.
\solution{
We can expand $\ket{\psi}$ (assumed to be normalised) in energy eigenstates $\ket{n}$ to get
$$\langle H \rangle = \sum_n E_n \left|\braket{\psi}{n}\right|^2 \ge E_0 \sum_n \left|\braket{\psi}{n}\right|^2 = E_0.$$
}
\end{question}

\begin{question}
(optional)
Find a way to apply the variational method to discover the excited states.
\hint{Define a new Hamiltonian $H'$ as a function of $H$ such that the ground state of $H'$ is an excited state of $H$.}
\solution{
Apply the method to find the ground state of $H' = (H - \lambda)^2$.
If $\lambda$ is close to the energy of an excited state of $H$, $H'$ will have the state as its ground state.
}
\end{question}

\begin{question}
Consider the potential
$$V(x) = \left\{\begin{matrix}V_0 & x > 0\\0 & x < 0\end{matrix}\right.$$
Find the reflection and transmission coefficients for an incoming wave from the left.
Are there any bound (i.e.\ localised) states?
\hint{Solve the Schr\"odinger equation in each region separately and apply the condition that $\psi$ and $\psi'$ must be continuous.}
\solution{
Consider $E>0$.
The particle is free in either region ($V_0$ just shifts the energy) and we get
$$\psi(x) = \left\{\begin{matrix}
e^{\im k x} + r e^{-\im k x} & x < 0\\
t e^{\im k' x} & x > 0
\end{matrix}\right.$$
with $\hbar^2 k^2/(2m) = E$ and $\hbar^2 k'^2/(2m) = E+V_0$.
We need $\psi$ and $\psi'$ to be continuous and we then get equations
$$1 + r = t,\qquad k (1 - r) = k' t.$$
Hence
$$r = \frac{k - k'}{k + k'},\qquad t = \frac{2k}{k+k'}.$$

We can't form a bound state, which would have to be exponentially decaying in either region.
But there is no way to match two exponential decays without a ``cusp'' at the interface, hence $\psi'$ would be discontinuous.
}
\end{question}

Note that in the last question the transmission coefficient was non-zero when $0 < E < V_0$, leading to an exponentially decaying wavefunction for $x>0$.
Classically, the particle would be forbidden from entering here, but quantum mechanically there is a non-zero chance that the particle is found in this forbidden region.
This effect is called \emph{quantum tunneling} and it's crucial to many phenomena that seem classically impossible, such as $\alpha$ decay.
It's even used technologically in flash memory and tunnel diodes.

\begin{question}
(optional)
Consider the potential (called the finite potential well)
$$V(x) = \left\{\begin{matrix}V_0 & -L \le x \le L\\0 & \mbox{otherwise}\end{matrix}\right.$$
Find an equation for the energy of the bound states (note that it can't be solved algebraically).\\
(optional) Show that there is always at least one bound state.
\hint{Solve for even and odd states separately.}
\solution{
We need $V_0 \le E \le 0$ to get bound states, which can be chosen to be either even or odd.
To get an even state we need
$$\psi(x) = \left\{\begin{matrix}
a e^{k x} & x \le -L\\
b \cos(k' x) & -L \le x \le L\\
a e^{-k x} & x \ge L
\end{matrix}\right.$$
with $\hbar^2 k^2/(2m) = -E$ and $\hbar^2 k'^2/(2m) = E - V_0$.
From the continuity conditions we get
$$a e^{k L} = b \cos(k' L),\qquad -k a e^{k L} = -k' b \sin(k' L).$$
Dividing the two we get $k = k' \tan(k' L),$
which is shorthand for
$$\sqrt{-E} = \sqrt{E - V_0} \tan(\sqrt{2 m (E - V_0)} L/\hbar).$$

The odd case proceeds in the same way to give $k = -k' \cot(k' L),$ or
$$\sqrt{-E} = -\sqrt{E - V_0} \cot(\sqrt{2 m (E - V_0)} L/\hbar).$$

We know that the ground state has no nodes, hence it must be even.
The equation for that case can be rewritten as
$$\sqrt{\frac{-E}{E - V_0}} - \tan(\sqrt{2m (E - V_0)} L/\hbar) = 0.$$
For $E\to V_0$ the left-hand side goes to $+\infty$.
For $E=0$ it reads $-\tan(\sqrt{2m (E-V_0)} L/\hbar)$.
If the $\tan$ has no pole between $V_0 < E < 0$, then $\tan(\sqrt{2m(-V_0)} L/\hbar) > 0$ and the left-hand side is negative at $E=0$ -- hence it must cross through zero and there must be a solution.
If $\tan$ does have a pole then between $E=V_0$ and the first pole the function continuously goes from $+\infty$ to $-\infty$ -- and must pass through zero somewhere inbetween.
}
\end{question}

\begin{question}
(optional)
Use the result from the last question to show that every continuous potential $V$ that goes to 0 at $\pm \infty$ and that is negative somewhere must admit a bound state (this is only true in one dimension).
\hint{Take a bound state for an appropriate finite potential well and put an upper bound on $\langle T+V \rangle$ for that state.}
\solution{
Since $V$ is continuous and negative somewhere, there must be a region $[x_0-\varepsilon,x_0+\varepsilon]$ for which $E < V_0$ for some negative $V_0$.
Define
$$V_1(x) = \left\{\begin{matrix}V_0 & x_0-\varepsilon < x < x_0 + \varepsilon\\0 & \mbox{otherwise}\end{matrix}\right.;$$
by the last question it must have a bound state $\ket{\psi}$ (which we can normalise) of energy $E_0 < 0$.
Now note that $V_1(x) \ge V(x)$ everywhere and hence
$$\bra{\psi} H \ket{\psi} = \bra{\psi} T \ket{\psi} + \int V(x) |\psi(x)|^2 \D{x} \le \bra{\psi} T \ket{\psi} + \int V_1(x) |\psi(x)|^2 \D{x} = E_0 < 0.$$
This implies at least one eigenvalue of $H$ is less than 0, which implies that there is at least one bound state.
}
\end{question}

\subsection{The harmonic oscillator}
Of particular importance is the potential $V(x) = \frac{1}{2} m \omega^2 x^2$, known as the \emph{harmonic oscillator}.
\begin{question}
Find the classical solutions.
\solution{
The equations of motion are
$$m \ddot x = -m \omega^2 x,$$
with the solution
$$x = a \cos(\omega t) + b \sin(\omega t).$$
}
\end{question}

We could now plunge in and solve the Schr\"odinger equation for the stationary states
$$-\frac{\hbar^2}{2m} \psi''(x) + \left(\frac{1}{2} m \omega^2 x^2 - E\right) \psi(x) = 0$$
directly, but it's quite tedious and there is a much more elegant way due to Dirac.
On a most basic level, the insight is to reduce the complex ``back-and-forth'' motion of the real $x$ and $p$ to circular motion in the complex plane.

\begin{question}
Note that the classical solutions in \emph{phase space} $(x,p) = (x, m \dot x)$ are just ellipses.
Define a dimensionless complex quantity $a = \alpha x + \beta p$ (with $\alpha > 0$) so that $a$ rotates clockwise in a circle in the complex plane.
Show that the magnitude $|a|^2$ is proportional to the total energy and define $a$ to get the simplest possible proportionality constant.

(Even though this problem is entirely classical, you need to use $\hbar$ to make $a$ dimensionless).
\result{$a = \sqrt{m \omega/(2\hbar)} (x + \im p/(m\omega))$.}
\solution{
Circular motion in the complex plane means $\dot a = -\im \omega a$.
Calculate
$$\dot a = \alpha \dot x + \beta \dot p = \frac{1}{m} \alpha p - \beta m \omega^2 x \stackrel{!}{=} -\im \omega (\alpha x + \beta p).$$
Comparing the coefficient of $x$ we see that $\beta = \im \alpha/(m \omega)$.
To get a dimensionless result $\alpha$ needs to be units of inverse length.
$\hbar$ has units $\rm kg \cdot m^2/s$, $\omega$ has units $\rm 1/s$ and $m$ has units $\rm kg$.
Hence we need $\alpha \propto \sqrt{m \omega/\hbar}$ which leads to
$$a = \gamma \sqrt{\frac{m \omega}{\hbar}} \left(x + \frac{\im p}{m \omega}\right)$$
(where $\gamma$ is dimensionless).
This leads to
$$|a|^2 = \gamma^2 \frac{m \omega x^2}{\hbar} + \gamma^2 \frac{p^2}{m \omega} = 2\gamma^2 \hbar \omega \left(\frac{p^2}{2m} + \frac{1}{2} m \omega^2 x^2\right).$$
The simplest proportionality constant is $\hbar \omega$ which means $\gamma=1/\sqrt{2}$ and thus
$$a = \sqrt{\frac{m \omega}{2 \hbar}} \left(x + \frac{\im p}{m \omega}\right).$$
}
\end{question}

\begin{question}
Now on to the quantum problem.
$x$ and $p$ are now operators and $a$ can be made a (non-Hermitian!) operator by substituting the $x$ and $p$ operators in the equation you derived for $a$.
Evaluate $[a,a^\dagger]$ and write $H = p^2/(2m) + m \omega^2 x^2/2$ in terms of $a$ and $a^\dagger$ (be very mindful that $[x,p] \ne 0!$).
\hint{To write down $H$, evaluate an equivalent to $|a|^2$.}
\result{$[a,a^\dagger] = 1$ and $H = \hbar \omega (a^\dagger a + 1/2)$.}
\solution{
$$[a,a^\dagger] = \frac{m \omega}{2\hbar} \left[x + \frac{\im p}{m \omega}, x - \frac{\im p}{m \omega}\right] = \frac{m \omega}{2\hbar} \frac{-\im}{m \omega} ([x,p] - [p,x]) = \frac{m \omega}{2\hbar} \frac{\im}{m \omega} 2\im \hbar = 1$$
In the classical case we had $H = \hbar \omega a a^*$.
Trying $\hbar \omega a^\dagger a$ we find
$$\hbar \omega a^\dagger a = \hbar \omega \frac{m \omega}{2\hbar} \left(x- \frac{\im p}{m \omega}\right) \left(x + \frac{\im p}{m \omega}\right) = \frac{m \omega^2}{2} \left(x^2 + \frac{p}{m^2 \omega^2} + \frac{\im}{m \omega} [x,p]\right) = H - \frac{1}{2} \hbar \omega.$$
Hence
$$H = \hbar \omega a^\dagger a + \frac{1}{2} \hbar \omega.$$
}
\end{question}

\begin{question}
Show that there is a (up to a phase factor) unique normalised state $\ket{0}$ obeying $a \ket{0} = 0$ and determine its wavefunction.
Is it a stationary state? If so, what is its energy?
\hint{Replace $p$ with $-\im \hbar\;\partial/\partial x$ and solve the resulting differential equation.}
\result{$\psi(x) = (m \omega/(\pi \hbar))^{1/4} \exp(-m \omega x^2/(2\hbar))$ and energy $\hbar \omega/2$.}
\solution{
$a \psi = 0$ implies
$$x \psi(x) + \frac{\hbar \psi'(x)}{m \omega} = 0,$$
which can be rearranged to $\pfrac{}{x} \ln \psi(x) = - m \omega x/\hbar$.
The (normalised) solution to this is a Gaussian
$$\psi(x) = \left(\frac{m \omega}{\pi \hbar}\right)^{1/4} \exp\left(-\frac{m \omega x^2}{2 \hbar}\right).$$

Applying $H$ to this state results in
$$H \ket{0} = \hbar \omega a^\dagger a \ket{0} + \frac{1}{2} \hbar \omega \ket{0} = \frac{1}{2} \hbar \omega \ket{0},$$
hence $\ket{0}$ is a stationary state of energy $\hbar \omega/2$.
}
\end{question}

\begin{question}
Show that $\bra{\psi} a^\dagger a \ket{\psi}$ is positive for any (normalised) state $\psi$.
What does this imply about the eigenvalues of $H$ and about $\ket{0}$?
\hint{Use $\braket{\phi}{\phi} \ge 0$ for any state $\ket{\phi}$.}
\solution{
Define $\ket{\phi} = a \ket{\psi}$.
Then $0 \le \braket{\phi}{\phi} = \bra{\psi} a^\dagger a \ket{\psi}$.
Since $H = \hbar \omega a^\dagger a + \hbar \omega/2$ this means that all eigenvalues are $\ge \hbar \omega/2$.
Since $\ket{0}$ has energy $\hbar \omega/2$ this implies it's the ground state.
}
\end{question}

\begin{question}
Show that if $\ket{\psi}$ is a stationary state of energy $E$, then $a \ket{\psi}$ and $a^\dagger \ket{\psi}$ are as well and find their energy (you can, optionally, show that $a^\dagger \ket{\psi}$ is non-zero for any physical state).
Use this insight to find an expression for the $n$-th stationary state $\ket{n}$ and its energy.
What is the effect of the $a^\dagger a$ operator on $\ket{n}$?
Normalise $\ket{n}$.
\hint{Use $[a,a^\dagger] = 1$ to derive an expression for $[H,a]$ and remember that $\ket{0}$ is the ground state.}
\hint{To normalise $\ket{n}$ evaluate $\bra{n} a a^\dagger \ket{n}$.}
\solution{
$$[H,a] = \hbar \omega [a a^\dagger, a] = \hbar \omega a [a^\dagger, a] = -\hbar \omega$$
and hence
$$H a \ket{\psi} = a H \ket{\psi} + [H,a] \ket{\psi} = (E - \hbar \omega) a \ket{\psi}.$$
Similarly
$$H a^\dagger \ket{\psi} = (E + \hbar \omega) a \ket{\psi}.$$
$a$ then decreases the energy by $\hbar \omega$ and $a^\dagger$ increases it by the same amount.

We can find $\ket{n}$ by starting from $\ket{0}$ and applying $a^\dagger$ $n$ times.
The energy is then $(n+1/2)\hbar \omega$.
$a^\dagger a = (H - 1/2)/(\hbar \omega)$ and so $a^\dagger a \ket{n} = n \ket{n}$.
To normalise the state write $\alpha_n \ket{n} = a^\dagger \ket{n-1}$ and note that
$$|\alpha_n|^2 \braket{n}{n} = \bra{n-1} a a^\dagger \ket{n-1} = \bra{n-1} (a^\dagger a + 1) \ket{n-1} = n \braket{n-1}{n-1}.$$
Hence the states will be normalised if we define $\ket{n} = n^{-1/2}~a^\dagger \ket{n-1}.$
By applying this relation repeatedly we can show that
$$\ket{n} = \frac{1}{\sqrt{n!}} (a^\dagger)^n \ket{0}.$$

Remember the relations
$$a \ket{n} = \sqrt{n} \ket{n-1},$$
$$a^\dagger \ket{n} = \sqrt{n+1} \ket{n+1};$$
they are very useful.
They also make it obvious why $a$ and $a^\dagger$ are called the ``lowering'' and ``raising'' operator, respectively.
Collectively they are known as ``ladder operators''.
}
\end{question}

\begin{question}
(optional)
Show that if $\ket{\psi}$ is a stationary state, then $a^n \ket{\psi}$ must be 0 for some integer $n$.
Hence argue that the previously found $\ket{n}$ states exhaust all the stationary states of the oscillator.
\hint{What is the energy of $a^n \ket{\psi}$?}
\solution{
$a^n \ket{\psi}$ is either zero or a stationary state of energy $E - n \hbar \omega$.
Since all stationary states have energies larger than $\hbar \omega/2$, the latter can't possibly be true for all $n$ and hence it must be 0 for at least one $n$.
If $a^{n-1} \ket{\psi} \ne 0$ but $a^n \psi = 0$, then $a (a^{n-1} \ket{\psi}) = 0$ which means that $a^{n-1} \ket{psi} \propto \ket{0}$.
Applying $(a^\dagger)^{n-1}$ one can now show that $\ket{\psi} \propto (a^\dagger)^n \ket{0} \propto \ket{n}$.
}
\end{question}

\begin{question}
(optional, Rodrigues' formula)
Derive an expression for the wavefunction $\psi_n(x)$ of $\ket{n}$.
It should be of the form
$$\psi_n(x) = f(x) \Dfrac{^n}{x^n} g(x),$$
where $f(x)$ and $g(x)$ are simple functions with no $(\D{}/\D{x})^n$ operators in them.
\hint{Calculate $a^\dagger (h(x) e^{m \omega x/(2\hbar)})$ for some generic function $h(x)$ (note that the exponent has a $+$ sign).}
\solution{
You can show that
$$\left(x - \frac{\hbar}{m \omega} \Dfrac{}{x}\right) h(x) e^{m \omega x^2/(2 \hbar)} = -e^{m \omega x^2/(2\hbar)} \frac{\hbar}{m \omega} \Dfrac{}{x} h(x).$$
We can thus ``pull out'' $e^{m\omega x^2/(2\hbar)}$ from $a^\dagger$ if we drop the $x$ term.
Hence $(a^\dagger)^n \ket{0}/\sqrt{n!}$ can be written as
$$\psi_n(x) = \frac{(-1)^n}{\sqrt{n! \sqrt{\pi} 2^n}} \left(\frac{m \omega}{\hbar}\right)^{1/4-n/2} \exp\left(\frac{m \omega x^2}{2\hbar}\right) \left(\Dfrac{}{x}\right)^n \exp\left(-\frac{m \omega x^2}{\hbar}\right).$$
}
\end{question}

\begin{question}
Write $x$ and $p$ in terms of $a$ and $a^\dagger$.
Show that $\langle x \rangle = \langle p \rangle = 0$ and find $\langle x^2 \rangle$ and $\langle p^2 \rangle$ for $\ket{n}$.
Verify the uncertainty principle.

The ``virial theorem'' in classical mechanics states that $2 T = \beta V$ for a potential $V(x) \propto x^\beta$.
Verify that it holds for the quantum harmonic oscillator for a stationary state (taking expectation values of both sides).
\solution{
Inverting the definitions of $a$ and $a^\dagger$ gives
$$x = \sqrt{\frac{\hbar}{2m \omega}} \left(a + a^\dagger\right), \qquad p = -\im \sqrt{\frac{m \omega \hbar}{2}} \left(a - a^\dagger\right).$$
We know that
$$\bra{n} a \ket{n} = \sqrt{n} \braket{n}{n-1}$$
and so $\langle a \rangle = 0$ (similarly, $\langle a^\dagger \rangle = 0$).
Since $\langle x \rangle$ and $\langle p \rangle$ are just linear combinations of these two, they must also be zero.
The squared equivalents can be found from
$$\langle x^2 \rangle = \frac{\hbar}{2m \omega} \left\langle a^2 + a a^\dagger + a^\dagger a + (a^\dagger)^2 \right\rangle = \frac{\hbar}{2m \omega} (\langle a^\dagger a + 1\rangle + \langle a^\dagger a \rangle) = \frac{\hbar}{2m \omega} (2n + 1)$$
$$\langle p^2 \rangle = -\frac{m \omega \hbar}{2} \left\langle a^2 - a a^\dagger - a^\dagger a + (a^\dagger)^2 \right\rangle = \frac{\hbar m \omega}{2} (2n + 1)$$
Hence
$$\sigma_x \sigma_p = \sqrt{\langle x^2 \rangle} \sqrt{\langle p^2 \rangle} = \frac{\hbar}{2} (2n+1) \ge \frac{\hbar}{2}.$$
Note that only the ground state has minimum uncertainty.

The virial theorem just simplifies to $\langle T \rangle = \langle V \rangle$, or
$$\frac{\langle p^2 \rangle}{2m} = \frac{1}{2} m \omega^2 \langle x^2 \rangle,$$
substituting the previous results gives $(\hbar \omega/4) (2n+1)$ on both sides.
}
\end{question}

There is a fun interpretation of the harmonic oscillator and the ladder operators.
We say that $a^\dagger$ and $a$ create particles of energy $\hbar \omega$!
This may seem a bit strange -- no doubt partly because these are in fact the most boring particles possible, since they have absolutely no degrees of freedom (the state $\ket{n}$ with $n$ particles is a \emph{unique} state after all).
Nonetheless, we will later see that, in a nutshell, this is how \emph{all} particles work (using the framework of second quantisation).

We can make things a little bit more interesting by coupling two oscillators, in the hope that particles can then ``travel'' from one oscillator to the other.
But first, we need to study how to represent the combination of two oscillators as a single quantum system!

\subsection{Combining systems and measurements}
Consider a system of \emph{two} oscillators.
Intuitively, we expect that there are now states described by \emph{two} numbers, corresponding to the number of the particles in each oscillator.
Hence we can write the stationary states as $\ket{n,m}$.
Writing the ladder operators for the first oscillator as $a,a^\dagger$ and as $b,b^\dagger$ for the second oscillator we expect
$$a \ket{n,m} = \sqrt{n} \ket{n-1,m}, \qquad b \ket{n,m} = \sqrt{m} \ket{n,m-1}.$$
This shows a general feature of combined systems: We write the state as $\ket{\psi_a} \ket{\psi_b}$ (this notation is called a ``tensor product'') and the operators that originally belonged to system $a$ act only on the $\ket{\psi_a}$ part of the state and the operator that originally belonged to system $b$ act only on the $\ket{\psi_b}$ part of the state.
Mathematically,
$$a (\ket{\psi_a} \ket{\psi_b}) = (a \ket{\psi_a}) \ket{\psi_b}, \qquad b (\ket{\psi_a} \ket{\psi_b}) = \ket{\psi_a} (b \ket{\psi_b}).$$

The tensor product notation is a bit strange at first (note that is not commutative, since $\ket{\psi_b} \ket{\psi_a}$ has the $a$ system in $\ket{\psi_b}$ and the $b$ system in $\ket{\psi_a}$; the order is reversed for a bra state, so that $\ket{\psi_a} \ket{\psi_b}$ corresponds to $\bra{\psi_b} \bra{\psi_a}$).
But it turns it has even stranger physical implications!
We are still in a linear space and so we can form states such as
$$\ket{\psi_a} \ket{\psi_b} + \ket{\phi_a} \ket{\phi_b}$$
that cannot be written as a tensor product of $a$ and $b$ states (they are called ``entangled'' states).
If we act on this with the operator $\ket{\psi_a} \bra{\psi_a}$ (corresponding to a measurement finding the $a$ system in the $\ket{\psi_a}$ state) we get
$$\ket{\psi_a} \ket{\psi_b}.$$
The $b$ system is now in the $\ket{\psi_b}$ state.
This may not seem particularly disturbing, but consider that we have included \emph{no explicit interaction between $a$ and $b$} and that this works \emph{regardless of how much distance is between the two systems}.
It works even if the $a$ system is on earth and the $b$ system is near Alpha Centauri and it does so instantly!
Einstein termed this effect ``spooky action at a distance''.

Before you get too freaked out by this, it may be comforting to know that there is no way transmit information solely by this mechanism.
Whoever is left at Alpha Centauri still has to measure the $b$ system and they will find it in state $\psi_b$ -- which would have happened with 50\% chance even if we didn't do anything on earth.
Further, to create the real-life entangled state in the first place requires interacting the systems (which implies an explicit interaction and won't work faster than the speed of light).

We can now do a fascinating thought experiment to shed light on the nature of quantum measurement.
This will involve the often mentioned (and widely misunderstood) ``Schr\"odinger's cat''.
A particularly cruel experimentalist has put a cat in a box with a vial of poison, which is released by a mechanism triggered by some quantum process (such as radioactive decay).
The usual interpretation in popular science is that this puts the cat in a state $\ket{\rm alive} + \ket{\rm dead}$, which is mystical and confusing.
Further, it is stated, the state only collapses once the conscious observer opens the box, which implies consciousness is somehow distinct from other physical processes.
Reality is less glamorous.
No matter how much care the experimenter takes he will couple the cat and the box with his laboratory, and in particular his brain.
We should thus be thinking of this as a combined system.
The naive interpretation corresponds to a state
$$(\ket{\rm alive} + \ket{\rm dead}) \ket{\mbox{experimentalist confused}}.$$
Opening the box then somehow resolves this into a state such as $\ket{\rm alive} \ket{\mbox{experimentalist happy}}$ or $\ket{\rm dead} \ket{\mbox{experimentalist sad}}.$
But the inadvertent coupling enforces that the cat state is consistent with the experimentalist state even before he opens the box, hence the true state is
$$\ket{\rm alive} \ket{\mbox{experimentalist will be happy}} + \ket{\rm dead} \ket{\mbox{experimentalist will be sad}}.$$
Now, there are ``interpretations'' of quantum mechanics that assign meaning to this combination.
For our purposes it's really only important to realise that it doesn't really matter since \emph{from the experimentalist's point of view}, his local ``universe'' has a cat that is either dead or alive, not in a weird superposition state (in the ``multiverse'' interpretation this is made most explicit by stating that each branch of the wavefunction corresponds to a separate universe, all of which exists in parallel).

(As a historical sidenote, Schr\"odinger actually introduced the cat experiment to expose the inconsistencies of the ``Copenhagen interpretation'', which involves the ``collapse of the wavefunction'' due to a conscious observer.)

We can extend this idea to arbitrary measurements.
Measuring an observable $q$ involves an operator
$$\sum_i \ket{i} \ket{\mbox{exp.\ records $q_i$}} \bra{\mbox{exp.\ clueless}} \bra{i}.$$
Here the ``exp.'' states include not just the experimentalist and his brain but the entire laboratory and whatever else happens to interact with it.
Measuring a superposition such as $(\ket{1} + 2 \ket{2})/\sqrt{5}$ then leads to a state
$$\ket{\psi} = \frac{1}{\sqrt{5}} \ket{1} \ket{\mbox{exp.\ records $q_1$}} + \frac{2}{\sqrt{5}} \ket{2} \ket{\mbox{exp.\ records $q_2$}}.$$

Now, this is a pure state (it completely specifies the state of the universe).
It corresponds to a density matrix $\varrho = \ket{\psi} \bra{\psi}$.
Suppose we only care about the experimentalist state.
We can then define a new density matrix $\varrho'$ that reflects our ``ignorance'' (indifference).
Mathematically this is done by a ``partial trace''.
Recall that the trace converts $\ket{i} \bra{j}$ to $\braket{j}{i}$.
The partial trace does this only for the state corresponding to one part of the system, i.e.\ $\tr_S(\ket{i} \bra{j}) = \braket{j}{i}$ but $\tr_S$ has no effect on $\ket{\mbox{exp.\ records $q_i$}} \bra{\mbox{exp. records $q_i$}}$.
We can then calculate
$$\tr_S(\ket{\psi} \bra{\psi}) = \frac{1}{5} \ket{\mbox{exp.\ records $q_1$}} \bra{\mbox{exp.\ records $q_1$}} + \frac{4}{5} \ket{\mbox{exp.\ records $q_2$}} \bra{\mbox{exp.\ records $q_2$}}.$$
This means there is a $1/5$ chance that the experimenter recorded $q_1$ -- exactly the same result that we postulated earlier, but now there is nothing special about the ``measurement''.

There is an alternative scenario that works very similarly.
Suppose the experimenter carefully prepared the state $(\ket{1}+2 \ket{2})/\sqrt{5}$.
Now, some coupling he couldn't shield, let's say it's a stray $\alpha$-particle, entangles the system with the particle to produce a state
$$\ket{\psi} = \frac{1}{\sqrt{5}} \ket{1} \ket{\alpha_1} + \frac{2}{\sqrt{5}} \ket{2} \ket{\alpha_2}.$$
Now in this case the experimenter only cares about the state of the system -- he isn't even aware of the $\alpha$-particle.
We should thus use the partial trace to remove it from $\psi$ to get the state that the experimenter cares about.
Note that now we remove the \emph{other} part of the state.
We get
$$\tr_\alpha(\ket{\psi} \bra{\psi}) = \frac{1}{5} \ket{1} \bra{1} + \frac{4}{5} \ket{2} \bra{2}.$$
Effectively, the $\alpha$-particle has converted the pure state to an impure state.
This is called \emph{decoherence}.

\subsection{Coupled oscillators}
Let's now return to our original problem -- coupling oscillators.
We can now identify our states $\ket{m,n}$ as just tensor products $\ket{m} \ket{n}$.
Operators such as $a$ and $b$ operate on either half of the tensor product.
Two operators that operate on different parts of the state obviously commute.
If there is no coupling, the total energy is just $H_a + H_b$, i.e.
$$\frac{{p_a}^2}{2m} + \frac{1}{2} m \omega^2 {x_a}^2 + \frac{{p_b}^2}{2m} + \frac{1}{2} m \omega^2 {x_b}^2 = \hbar \omega (a^\dagger a + b^\dagger b + 1).$$
Note that this Hamiltonian commutes with the number operators $N_a = a^\dagger a$ and $N_b = b^\dagger b$, hence they are conserved.
The total number $N = N_a + N_b$ is also conserved.
Of course to get an interesting theory we need to add an explicit coupling term.
Generally speaking, systems with coupling are usually no longer solvable (even if they are, it's rarely easy).
We can, however, get an idea of the effect of the interaction by studying which conserved quantities are still conserved. 

\begin{question}
Show that\\
1. the Hamiltonian with an extra term $\lambda (a b^\dagger + a^\dagger b)$ no longer conserves $N_a$ and $N_b$, but conserves $N$.\\
2. $\lambda x_a x_b$ does not conserve $N_a$, $N_b$ or $N$.

\hint{Remember the commutation relations
$$[a,a^\dagger] = 1, [b, b^\dagger] = 1,$$
and all other commutators (e.g.\ $[a,b]$) are zero (but of course $[a^\dagger, a]$ counts as $-[a,a^\dagger]$). Also remember that $[A,BC]=[A,B]C+B[A,C]$.}

\solution{
1. We already know that the ``free'' Hamiltonian conserves all three so we only need to look at the new terms.
Calculate
$$[a b^\dagger, a^\dagger a] = [a, a^\dagger] a b^\dagger = a b^\dagger, \qquad [a b^\dagger, b^\dagger b] = a b^\dagger [b^\dagger, b] = -a b^\dagger,$$
$$[b a^\dagger, a^\dagger a] = b a^\dagger [a^\dagger, a] = -b a^\dagger, \qquad [b a^\dagger, b^\dagger b] = [b, b^\dagger] b a^\dagger = b a^\dagger.$$
From this we can read off $[a b^\dagger + b a^\dagger, N_{a,b}] \ne 0$ but $[a b^\dagger + b a^\dagger, N] = 0$.

2. We have $\lambda x_a x_b \propto a b + a^\dagger b + b^\dagger a + a^\dagger b^\dagger$.
Again calculate
$$[a b, a^\dagger a] = [a, a^\dagger] a b = a b, \qquad [a b, b^\dagger b] = a [b, b^\dagger] b = a b,$$
$$[a^\dagger b^\dagger, a^\dagger a] = a^\dagger [a^\dagger, a] b^\dagger = -a^\dagger b^\dagger, \qquad [a^\dagger b^\dagger, b^\dagger b] = a^\dagger b^\dagger [b^\dagger, b] = -a^\dagger b^\dagger.$$
Since the contributions don't cancel, neither $N_a$, $N_b$ nor $N_c$ is conserved.
}
\end{question}

In this case the system is actually simple enough that we can solve it in a simple case by just plugging straight into the Schr\"odinger equation.

\begin{question}
Add $\lambda \hbar (a b^\dagger + a^\dagger b)$ to $H$.
We now want to calculate the time evolution of the state $\ket{1,0}$.
Start by calculating $H \ket{1,0}$ and $H \ket{0,1}$.
Use the time-dependent Schr\"odinger equation to determine differential equations for the time evolution of $\ket{\psi(t)} = \alpha(t) \ket{1,0} + \beta(t) \ket{0,1}$.
Solve them, e.g.\ by a trial solution $\alpha(t) = \alpha_1 e^{\im \omega_1 t} + \alpha_2 e^{\im \omega_2 t}$ and $\beta(t) = \beta_1 e^{\im \omega_1 t} + \beta_2 e^{\im \omega_2 t}$.
Find $\ket{\psi(t)}$ for the initial conditions $\alpha(0) = 1$ and $\beta(0) = 0$.
\result{$\ket{\psi(t)} = e^{-2\im\omega t} \left(\cos(\lambda t) \ket{1,0} - \im \sin(\lambda t) \ket{0,1}\right).$}
\solution{
$$H \ket{1,0} = 2\hbar \omega \ket{1,0} + \hbar \lambda \ket{0,1}$$
$$H \ket{0,1} = 2\hbar \omega \ket{0,1} + \hbar \lambda \ket{1,0}$$
Now the time-dependent Schr\"odinger equation is
$$H \ket{\psi} = \im \hbar \pfrac{\ket{\psi}}{t}$$
Hence
$$\alpha'(t) \ket{1,0} + \beta'(t) \ket{0,1} = -2\im \omega (\alpha(t) \ket{1,0} + \beta(t) \ket{0,1}) - \im \lambda (\alpha(t) \ket{0,1} + \beta(t) \ket{1,0})$$
which we can separate into
$$\alpha'(t) = -2\im \omega \alpha(t) - \im \lambda \beta(t), \qquad \beta'(t) = -2\im \omega \beta(t) - \im \lambda \alpha(t).$$
Inserting the trial solutions and separating the different exponentials gives
$$\im \omega_1 \alpha_1 = -2 \im \omega \alpha_1 - \im \lambda \beta_1, \qquad \im \omega_2 \alpha_2 = -2\im \omega \alpha_2 - \im \lambda \beta_2,$$
$$\im \omega_1 \beta_1 = -2 \im \omega \beta_1 - \im \lambda \alpha_1, \qquad \im \omega_2 \beta_2 = -2\im \omega \beta_2 - \im \lambda \alpha_2.$$
You can either solve these equation manually, or you can notice that if you set $\alpha_1 = \beta_1$ you get $\omega_1 = -2\omega - \lambda$.
To get an independent second solution you can try $\alpha_2 = -\beta_2$ to find $\omega_2 = -2\omega + \lambda$.

The initial conditions now correspond to $\alpha_1 + \alpha_2 = 1$ and $\alpha_1 - \alpha_2 = 0$, which leads to
$$\ket{\psi(t)} = e^{-2\im\omega t} \left(\cos(\lambda t) \ket{1,0} - \im \sin(\lambda t) \ket{0,1}\right).$$
}
\end{question}

We can interpret the result from the last question:
Now the particle is constantly moving back and forth between both oscillators.
We have successfully created the second most boring type of particle!
It can now do $a \to b$ and $b \to a$ processes.
You can read this directly off the Hamiltonian, actually.
$b^\dagger a$ means ``destroy an $a$ and create a $b$ particle'' -- hence $a \to b$.

We can also solve $x_a x_b$ type terms, but it's not as straightforward, since now $H \ket{1,0}$ will create $\ket{1,1}$ terms which will create $\ket{2,0}$ terms and so on (try it!).

Our naive interpretation doesn't seem to make a lot of sense here.
$ab$ would be a process destroying an $a$ and a $b$ -- which would violate conservation of energy.

\begin{question}
(optional)
Rewrite the Hamiltonian
$$H = \frac{{p_a}^2 + {p_b}^2}{2m} + \frac{1}{2} m \omega^2 (x_a^2 + x_b^2) + m \lambda^2 x_a x_b$$
using new variables $y_a$ and $y_b$ (linear combinations of $x_a$ and $x_b$), picking coefficients so that there is no $q_a q_b$ cross term.
We now need to change the momenta $p_a$ and $p_b$ to new variables $q_a$ and $q_b$ as well, since we desire the same commutation relations as before, i.e.\
$$[y_a, q_a] = \im \hbar, \quad [y_b, q_b] = \im \hbar, \quad [y_a, q_b] = 0, \quad [y_b, q_a] = 0.$$
Find a suitable definition for $q_a$ and $q_b$.

Calculate the energy of the ground state and expand in $\lambda$ to second lowest order.
Find an expression for the wavefunction of the ground state $\ket{\Omega}$ in terms of $x_a$ and $x_b$.
Expand in $\lambda$ to find $\braket{1,1}{\Omega}$ to lowest order.
Show by qualitative arguments that $\braket{n,n}{\Omega}$ is non-zero for all $n$.
\hint{To find the $\braket{1,1}{\Omega}$ contribution write $x_a$ and $x_b$ in terms of $a$ and $b$.}
\result{$\braket{1,1}{\Omega} = -\lambda^2/(4 \omega^2) + O(\lambda^4).$}
\solution{
Use $y_a = (x_a + x_b)/\sqrt{2}$ and $y_b = (x_a - x_b)/\sqrt{2}$ to get
$$H = \frac{{p_a}^2 + {p_b}^2}{2m} + \frac{1}{2} m \omega^2 (y_a^2 + y_b^2) + \frac{1}{2} m \lambda^2 (y_a^2 - y_b^2).$$
To transform $p_a$ and $p_b$ use $q_a = (p_a + p_b)/\sqrt{2}$ and $q_b = (p_a - p_b)/\sqrt{2}$ to get
$$H = \frac{{q_a}^2 + {q_b}^2}{2m} + \frac{1}{2} m \omega^2 (y_a^2 + y_b^2) + \frac{1}{2} m \lambda^2 (y_a^2 - y_b^2).$$
This is now two independent oscillators at frequencies $\sqrt{\omega^2 \pm \lambda^2}$.
The ground state energy is
$$\frac{1}{2} \hbar \left(\sqrt{\omega^2 + \lambda^2} + \sqrt{\omega^2 - \lambda^2}\right) = \frac{1}{2} \hbar \left(2\omega - \frac{\lambda^4}{4 \omega^3} + O(\lambda^8)\right).$$
The ground state wavefunction is just
$$\psi(q_a, q_b) = \sqrt{\frac{m (\omega^4 - \lambda^4)^{1/4}}{\pi \hbar}}\exp\left(-\frac{m (\sqrt{\omega^2 + \lambda^2}~{q_a}^2 + \sqrt{\omega^2 - \lambda^2}~{q_b}^2)}{2\hbar}\right),$$
which we can translate into $x_a$ and $x_b$ by substituting in their definitions, arriving at
$$\psi(x_a, x_b) = \sqrt{\frac{m (\omega^4 - \lambda^4)^{1/4}}{\pi \hbar}}\exp\left(-\frac{m (\sqrt{\omega^2 + \lambda^2}~(x_a + x_b)^2 + \sqrt{\omega^2 - \lambda^2}~(x_a - x_b)^2)}{4\hbar}\right).$$
Expanding in $\lambda$ we get the previous ground state for $\lambda=0$ and a contribution proportional to $\lambda^2$, from expanding $\sqrt{\omega^2\pm\lambda^2} = \omega (1 \pm \lambda^2/(2\omega^2))$, i.e.\
$$\psi(x_a, x_b) = \left(1-\frac{m\lambda^2 x_a x_b}{2\omega \hbar} + O(\lambda^4)\right) \psi_0(x_a, x_b).$$
The trick is now to write $x_a = \sqrt{\frac{\hbar}{2m \omega}} (a + a^\dagger)$ and the same for $x_b$.
The $a$ will vanish when applied to the vacuum and hence
$$\ket{\Omega} = \left(1 - \frac{\hbar}{2m \omega} \frac{m \lambda^2}{2\omega \hbar} a^\dagger b^\dagger + O(\lambda^4)\right) \ket{0,0}.$$
From this we can read off
$$\braket{1,1}{\Omega} = -\frac{\lambda^2}{4 \omega^2} + O(\lambda^4).$$

Carrying this process further would lead to higher order terms, proportional to $(a^\dagger b^\dagger)^n$, and hence all the $\ket{n,n}$ states contribute.
}
\end{question}

We have only looked at the ground state but we can already see that things are complicated\dots at least in terms of $a$ and $b$.
The presence of $\ket{1,1}$ in the ground state suggests that the process ``$\mbox{nothing} \to a + b$'' predicted by the naive rules \emph{does} occur.
This is puzzling -- shouldn't energy by conserved?
We will later see in perturbation theory that we can form processes such as ``$\mbox{nothing} \to a + b \to \mbox{nothing}$'' with unphysical (``virtual'') intermediate states.
One way to look at this is to argue that energy is like frequency and so it's not well defined for a short-lived state and hence we can temporarily violate conservation of energy (``time-energy uncertainty principle'').

\begin{question}
(optional)
Another thing we can do is to build $c$ and $d$ operators in terms of the $y$ and $q$ variables.
Write down their definition and find an expression for $H$.
Find an expression for $c^\dagger$ in terms of $a$ and $b$ operators and expand $c^\dagger \ket{\Omega}$ in $a,b$ states to order $\lambda^2$.
\solution{
Defining
$$c = \sqrt{\frac{m \sqrt{\omega^2 + \lambda^2}}{2\hbar}} \left(y_a + \frac{\im q_a}{m \sqrt{\omega^2 + \lambda^2}}\right),$$
$$d = \sqrt{\frac{m \sqrt{\omega^2 - \lambda^2}}{2\hbar}} \left(y_b + \frac{\im q_b}{m \sqrt{\omega^2 - \lambda^2}}\right)$$
we get
$$H = \hbar \sqrt{\omega^2 + \lambda^2}~c^\dagger c + \hbar \sqrt{\omega^2 - \lambda^2}~d^\dagger d + \mbox{const}.$$

To find $c^\dagger$ start with
$$x_a = \sqrt{\frac{\hbar}{2m\omega}} (a + a^\dagger),\quad x_b = \sqrt{\frac{\hbar}{2m\omega}} (b + b^\dagger),$$
$$p_a = -\im \sqrt{\frac{\hbar m \omega}{2}} (a - a^\dagger), \quad p_b = -\im \sqrt{\frac{\hbar m \omega}{2}} (b - b^\dagger)$$
and then substitute into the definition of $c^\dagger$, which evaluate to
$$c^\dagger = \sqrt{\frac{\sqrt{\lambda^2 + \omega^2}}{8\omega}} \left(\left(1 + \frac{\omega}{\sqrt{\omega^2 + \lambda^2}}\right) (a^\dagger + b^\dagger) + \left(1 - \frac{\omega}{\sqrt{\omega^2 + \lambda^2}}\right) (a + b)\right).$$
$c^\dagger \ket{\Omega}$ expands to
$$c^\dagger \ket{\Omega} = \left(\frac{a^\dagger + b^\dagger}{\sqrt{2}} + \frac{\lambda^2}{4\omega^2} \frac{a + b}{\sqrt{2}} + O(\lambda^4)\right) \left(1 - \frac{\lambda^2}{4\omega^2} a^\dagger b^\dagger + O(\lambda^4)\right) \ket{0,0}$$
$$= \frac{\ket{1,0} + \ket{0,1}}{\sqrt{2}} - \frac{\lambda^2}{4\omega^2} (\ket{1,2} + \ket{2,1}) + O(\lambda^4).$$
}
\end{question}

The $c$ and $d$ operators now offer an alternative view of the system.
In the $c$/$d$ world the two types of particles have energies $\hbar \sqrt{\omega^2\pm\lambda^2}$ and they don't interact.
We saw earlier that the ground state is messy in terms of $a/b$.
In $c/d$ terms it's the same empty state we're used to!
We just calculated that the ``1 $c$ particle state'' $c^\dagger \ket{\Omega}$ is also messy in $a$/$b$ terms.
One way to look at this is to say that $c$ is a ``dressed excitation'' composed of some combination of $a$ and $b$ particles.
Another term is ``quasiparticle'' -- which is of course a bit silly here, since $a$ and $b$ are artificial constructs as well.

\subsection{Baby steps in quantum electrodynamics}
(This section is optional and requires the Fourier transform)

We started with photons and now we can finally see where photons come from.
To do so we need to quantise the electromagnetic field.
The total energy is
$$H = \int \frac{1}{2} \varepsilon_0 (E^2 + c^2 B^2) \D{^3 \vec x}.$$
This looks very much like an infinite set of harmonic oscillators!
The problem is that $\vec E$ and $\vec B$ are not independent variables, they are constrained and coupled through the Maxwell equations
$$\nabla \cdot \vec E = 0, \quad \nabla \cdot \vec B = 0, \quad \nabla \times \vec E = -\pfrac{\vec B}{t}, \quad \nabla \times \vec B = \frac{1}{c^2} \pfrac{\vec E}{t}.$$
The trick is to take the Fourier transform to get rid of the spatial coupling.

\begin{question}
Write the equations in terms of the Fourier transforms $\tilde E(\vec k, t)$ and $\tilde B(\vec k, t)$ (these are still vectors).
Use two orthonormal polarisation vectors $\vec n_1(\vec k)$ and $\vec n_2(\vec k)$ (obeying $\vec n_i(-\vec k) = \vec n_i(\vec k)$) to define vector components $\tilde E_i(\vec k, t)$ and $\tilde B_i(\vec k, t)$ such that they follow the time evolution of the simple harmonic oscillator
$$\dot x = \frac{p}{m}, \quad \dot p = -m \omega^2 x,$$
with $\tilde E_i$ corresponding to $x$ and $\tilde B_i$ corresponding to $p$ (you will need to impose suitable conditions on $\vec n_i(\vec k)$).
Find the energy in terms of $\tilde E_i$ and $\tilde B_i$.
\solution{
$$\vec k \cdot \tilde E = 0, \quad \vec k \cdot \tilde B = 0,\quad \im \vec k \times \tilde E = -\pfrac{\tilde B}{t}, \quad \im \vec k \times \tilde B = \frac{1}{c^2} \pfrac{\tilde E}{t}.$$
If we demand that $\vec k \cdot \vec n_i = 0$ then we can define
$$\tilde E = \tilde E_1 \vec n_1 + \tilde E_2 \vec n_2,$$
$$\tilde B = \frac{\im \vec k}{k} \times (\tilde B_1 \vec n_1 + \tilde B_1 \vec n_2).$$
With these definitions the third and fourth Maxwell equations become
$$\im \vec k \times (\tilde E_1 \vec n_1 + \tilde E_2 \vec n_1) = \frac{-\im \vec k}{k} \times \left(\pfrac{\tilde B_1}{t} \vec n_1 + \pfrac{\tilde B_1}{t} \vec n_2\right),$$
$$\tilde B_1 k \vec n_1 + \tilde B_2 k \vec n_2 = \frac{1}{c^2} \pfrac{\tilde E_1}{t} \vec n_1 + \frac{1}{c^2} \pfrac{\tilde E_2}{t} \vec n_2;$$
thus
$$\pfrac{\tilde E_i}{t} = c^2 k \tilde B_i,\quad \pfrac{\tilde B_i}{t} = -k \tilde E_i.$$
The energy can be found from Parseval's theorem as
$$H = \int \frac{1}{2} \varepsilon_0 (|\tilde E|^2 + c^2 |\tilde B|^2) \frac{\D{^3 \vec k}}{(2\pi)^3} = \int \frac{1}{2} \varepsilon_0 \sum_i (|\tilde E_i|^2 + c^2 |\tilde B_i|^2) \frac{\D{^3 \vec k}}{(2\pi)^3}$$
}
\end{question}

\begin{question}
The Fourier transform is actually suboptimal here because of the integral over $\vec k$ -- it'd be nicer if it was a sum.
Show that demanding that $\vec E$ and $\vec B$ are periodic in all three coordinates with period $L$ (say we live in a cubic universe with the ends connected) we can write the energy (redefined to be the energy inside one cube) as
$$H = V \sum_{\vec k,i} \frac{1}{2} \varepsilon_0 (|\tilde E_i|^2 + c^2 |\tilde B_i|^2),$$
where the sum is over all allowed $\vec k$ and we have redefined $\tilde E$ etc. using a Fourier series.
\solution{
Periodicity in the three coordinates implies that $\vec k$ must be $(i, j, k)~2\pi/L$ for integers $i,j,k$.
The Fourier series corresponds to
$$\tilde E = \frac{1}{V} \int \vec E(\vec x, t)~e^{\im \vec k \cdot \vec x} \D{^3 \vec x}.$$
$E^2$ now evaluates to
$$E^2 = \int_V \D{^3 \vec x} \sum_{\vec k, \vec k', i} \tilde E(\vec k) \tilde E(\vec k') e^{\im (\vec k + \vec k') \cdot \vec x}  = \sum_{\vec k, \vec k', i} \tilde E(\vec k) \tilde E(\vec k') V \delta_{\vec k, -\vec k'}  = V \sum_{\vec k, i} |\tilde E(\vec k)|^2$$
using the identity
$$\int_V \D{^3 \vec x}~e^{\im \vec k \cdot \vec x} = V \delta_{\vec k, 0}.$$
}
\end{question}

\begin{question}
$\tilde E_i$ and $\tilde B_i$ look very promising now but they still can't be used to construct a quantum theory in analogy with the harmonic oscillator.
The harmonic oscillator had $[x,p]=\im \hbar$.
We would like to similarly impose $[\tilde E_i, \tilde B_i] = \im \hbar$, but that would lead to an inconsistency:
You previously showed that $[x,p]=\im \hbar$ implies
$$\pfrac{\langle x \rangle}{t} = \left\langle \pfrac{H}{p}\right\rangle, \qquad
\pfrac{\langle p \rangle}{t} = -\left\langle \pfrac{H}{q}\right\rangle.$$
Since this holds for expectation values it suggests a relation of the classical variables
$$\dot x = \pfrac{H}{p}, \qquad \dot p = -\pfrac{H}{q}$$
(these equations also appear in classical Hamiltonian mechanics and are called \emph{Hamilton's equations}).

Show that with the Hamiltonian
$$H = \sum_{\vec k, i} \frac{1}{2} \varepsilon_0 (\tilde E_i^* \tilde E_i + \tilde B_i^* \tilde B_i)$$
and the variables $\tilde E_i$ and $\tilde B_i$ (taking the place of $x$ and $p$, respectively) these equations do not lead to the equations of motion
$$\pfrac{\tilde E_i}{t} = c^2 k \tilde B_i,\quad \pfrac{\tilde B_i}{t} = -k \tilde E_i$$
(they are not ``canonical variables'').
Show that $\tilde E_i$ and $\tilde B_i^*$ work with an extra constant factor.
Redefine them appropriately and find $\vec E$, $\vec B$ and $H$ in terms of the new definitions.

(Take care when calculating $\partial H/\partial \tilde E_i(\vec k)$ -- remember that $\tilde E_i^*(-\vec k) = \tilde E_i(\vec k)$.)
\solution{
$\tilde E_i$ and $\tilde B_i$ don't work because of
$$\pfrac{H}{\tilde E_i} = V \varepsilon_0 \tilde E_i^* \ne k \tilde E_i = -\pfrac{\tilde B_i}{t}, \quad \pfrac{H}{\tilde B_i} = V \varepsilon_0 c^2 \tilde B_i^* \ne c^2 k \tilde B_i = \pfrac{\tilde E_i}{t}.$$
We can fix it by using $\tilde E_i$ and $\tilde B_i^*$ instead and by adding a factor $\sqrt{k/(V\varepsilon_0)}$ to both of them, i.e.\ to define
$$\tilde E = \sqrt{\frac{k}{V \varepsilon_0}} (\tilde E_1 \vec n_1 + \tilde E_2 \vec n_2),\quad \tilde B = \sqrt{\frac{k}{V \varepsilon_0}} \frac{\im \vec k}{k} \times (\tilde B_1 \vec n_1 + \tilde B_2 \vec n_2).$$
We can write $\vec E$ and $\vec B$ as
$$\vec E = \sum_{\vec k, i} \sqrt{\frac{k}{V \varepsilon_0}} \tilde E_i(\vec k) \vec n_i(\vec k) e^{\im \vec k \cdot \vec x},$$
$$\vec B = \sum_{\vec k, i} \sqrt{\frac{k}{V \varepsilon_0}} \tilde B_i(\vec k) \frac{\im \vec k}{k} \times \vec n_i(\vec k) e^{\im \vec k \cdot \vec x}.$$
$H$ is now
$$H = \sum_{\vec k,i} \frac{1}{2} k ({\tilde E_i^*} \tilde E_i + c^2 \tilde B_i^* \tilde B_i).$$
}
\end{question}

We can now take the system into the quantum realm.
We convert $\tilde E_i$ and $\tilde B_i^*$ into operators obeying
$$[\tilde E_i(\vec k), \tilde B_j^\dagger(\vec k')] = \im \hbar \delta_{i,j} \delta_{\vec k, \vec k'},\quad [\tilde E_i(\vec k), \tilde E_j(\vec k')] = 0, \quad [\tilde B_i(\vec k), \tilde B_j(\vec k')] = 0.$$
Never forget that the $\vec E$ and $\vec B$ fields are real -- which means that in Fourier space $\tilde E_i(-\vec k) = \tilde E_i^\dagger(\vec k)$ and $\tilde B_i(-\vec k) = \tilde B_i^\dagger(\vec k)$!
Things are a bit scary because we have an oscillator with complex variables, but it mostly leads to extra $^\dagger$ appearing in places.

Now $\vec E$ and $\vec B$ become operators as well -- they are \emph{quantum fields}.
Note that, in our current formalism (the ``Schr\"odinger picture''), the time dependence is moved into the states $\ket{\psi(t)}$, hence $\vec E = \vec E(\vec r)$ and $\vec B = \vec B(\vec r)$.

\begin{question}
Remember we solved the normal harmonic oscillator by defining a complex quantity $a$ which classically followed $\dot a = -\im \omega a$ (for some $\omega > 0$) and quantum mechanically obeyed the commutation relations $[a,a^\dagger] = 1$.
Define $a_i(\vec k)$ to get these properties, with the additional constraint that operators of different $i$ or $\vec k$ should commute.
Express the Hamiltonian in terms of $a_i$ and $a_i^\dagger$, dropping the constant zero-point energy (which is actually infinite summing over all $\vec k$).
\solution{
The simplest thing to try is
$$a_i(\vec k) = \alpha \tilde E_i(\vec k) + \beta \tilde B_i(\vec k).$$
This leads to
$$[a_i(\vec k), a_i(\vec k')] = \alpha \beta [\tilde E_i(\vec k), \tilde B_i^\dagger(\vec k')] + \alpha \beta [\tilde B_i(\vec k), \tilde E_i^\dagger(\vec k')] = 0$$
($[A,B]^\dagger = [B^\dagger, A^\dagger]$ and hence $[\tilde B_i, \tilde E_i^\dagger] = -[\tilde E_i, \tilde B_i^\dagger]$).
We also find
$$[a_i(\vec k), a_i^\dagger(\vec k')] = \alpha \beta^* [\tilde E_i(\vec k), \tilde B_i^\dagger(\vec k')] + \alpha^* \beta [\tilde B_i, \tilde E_i^\dagger(\vec k')] = \alpha \beta^* \im \hbar - \alpha^* \beta \im \hbar = 2 \Im(\alpha^* \beta) \hbar.$$
The classical time evolution is
$$\dot a_i = \alpha c^2 k \tilde B_i - \beta k \tilde E_i = -\im \omega (\alpha \tilde E_i + \beta \tilde B_i).$$
This implies $\beta k = \im \omega \alpha$ and $\alpha c^2 k = -\im\omega \beta$.
Since we want $\omega > 0$ the only solution is $\omega = c k$ and $\beta = \im c \alpha$.
If we pick $\alpha>0$ the earlier condition gives $\alpha^2=1/(2\hbar c)$ and thus
$$a_i(\vec k) = \sqrt{\frac{1}{2 \hbar c}} \left(\tilde E_i(\vec k) + \im c \tilde B_i(\vec k)\right),\quad a_i^\dagger(\vec k) = \sqrt{\frac{1}{2 \hbar c}} \left(\tilde E_i^\dagger(\vec k) - \im c \tilde B_i^\dagger(\vec k)\right).$$
To find the Hamiltonian calculate
$$a_i^\dagger a_i = \frac{1}{2 \hbar c} \left(\tilde E_i^\dagger - \im c \tilde B_i^\dagger\right) \left(\tilde E_i + \im c \tilde B_i\right) = \frac{1}{2\hbar c} \left(\tilde E_i \tilde E_i^\dagger - \im c \tilde B_i^\dagger E_i + \im c E_i^\dagger B_i + c^2 \tilde B_i \tilde B_i^*\right).$$
Note that substituting $\vec k \to -\vec k$ gives
$$a_i^\dagger(-\vec k) a_i(-\vec k) = \frac{1}{2\hbar c} \left(\tilde E_i(\vec k) \tilde E_i^\dagger(\vec k) - \im c \tilde B_i(\vec k) E_i^\dagger(\vec k) + \im c E_i(\vec k) B_i^\dagger(\vec k) + c^2 \tilde B_i(\vec k) \tilde B_i^*(\vec k)\right);$$
adding the $\vec k$ and $-\vec k$ contributions will turn the two middle terms into commutators, which are constants.
Hence
$$H = \sum_{\vec k, i} \hbar \omega a_i^\dagger(\vec k) a_i(\vec k) + \mbox{const}.$$
}
\end{question}

We can now identify the \emph{ground state} $\ket{0}$ by the condition $a_i(\vec k) \ket{0} = 0$ for all $i$ and $\vec k$.
We can also construct states such as $a_i^\dagger(\vec k) \ket{0}$ that contain quantised excitations, i.e.\ \emph{photons}.

\begin{question}
Find the energy of a photon of wavevector $\vec k$.
\solution{We know from the harmonic oscillator that $a_i^\dagger a_i$ is the number operator.
Adding a photon to a state increases the number of photons with that $\vec k$, $i$ combination by one, hence it increases the energy by $\hbar \omega$, which is just the energy of one photon.}
\end{question}

Since we have solved the harmonic oscillator, we have now solved the free electromagnetic field as well!

\begin{question}
What operator creates a photon of wavevector $\vec k$ and Jones vector $\ket{J} = (J_1, J_2)$ (using the axes $\vec n_1(\vec k)$ and $\vec n_2(\vec k)$)?
\solution{$a_1^\dagger(\vec k) J_1 + a_2^\dagger(\vec k) J_2$.}
\end{question}

\begin{question}
Write down the stationary states and their energies.
\solution{
The stationary states are just
$$a_{i_1}^\dagger(\vec k_1) a_{i_2}^\dagger(\vec k_2) \dots a_{i_n}^\dagger(\vec k_n) \ket{0}$$
for some $i_1, i_2, \dots, i_n$ and $\vec k_1, \dots, \vec k_n$.
The energy is just
$$E = \sum_i \hbar c k_i.$$
}
\end{question}

\begin{question}
Show that the expectation values $\langle \vec E \rangle$ and $\langle \vec B \rangle$ obey Maxwell's equation.
\solution{
By construction we have
$$\pfrac{\langle \tilde E_i\rangle}{t} = c^2 k \langle \tilde B_i \rangle,\quad \pfrac{\langle \tilde B_i\rangle}{t} = -k \langle \tilde E_i \rangle.$$
We can actually conclude from this directly that Maxwell's equations are satisfied -- by construction.
We can also explicitly calculate
$$\nabla \cdot \langle \vec E \rangle = \sum_{\vec k, i} \sqrt{\frac{k}{V \varepsilon_0}} \langle \tilde E_i(\vec k)\rangle \im \vec k \cdot \vec n_i(\vec k) e^{\im \vec k \cdot \vec x} = 0,$$
$$\nabla \cdot \langle \vec B \rangle = \sum_{\vec k, i} \im \sqrt{\frac{k}{V \varepsilon_0}} \langle\tilde B_i(\vec k)\rangle \im \vec k \cdot \vec n_i(\vec k) e^{\im \vec k \cdot \vec x} = 0,$$
$$\nabla \times \langle \vec E \rangle = \sum_{\vec k, i} \sqrt{\frac{k}{V \varepsilon_0}} \langle\tilde E_i(\vec k)\rangle \im \vec k \times \vec n_i(\vec k) e^{\im \vec k \cdot \vec x} = -\pfrac{}{t} \langle \vec B \rangle,$$
$$\nabla \times \langle \vec B\rangle = \sum_{\vec k, i} \sqrt{\frac{k}{V \varepsilon_0}} \langle \tilde B_i(\vec k) \rangle \vec n_i(\vec k) e^{\im \vec k \cdot \vec x} = c^2 \pfrac{}{t} \langle \vec E \rangle.$$
}
\end{question}

\begin{question}
Write $\tilde E_i$, $\tilde B_i$, $\vec E$ and $\vec B$ in terms of $a_i$ and $a_i^\dagger$.
Show that, for the ground state, $\langle \vec E \rangle = 0$ and $\langle E^2 \rangle$ is infinite (can you imagine why?).
Show that $\langle \vec E \rangle = 0$ for any stationary state.
\hint{You need to combine different $\vec k$ states.}
\solution{
$$\tilde E_i = \sqrt{\frac{\hbar c}{2}} (a_i(\vec k) + a_i^\dagger(-\vec k)), \quad \tilde B_i = -\im \sqrt{\frac{\hbar}{2 c}} (a_i(\vec k) - a_i^\dagger(-\vec k)),$$
$$\vec E = \sum_{\vec k, i} \sqrt{\frac{\hbar c k}{2 V \varepsilon_0}} (a_i(\vec k) + a_i^\dagger(-\vec k)) \vec n_i(\vec k) e^{\im \vec k \cdot \vec x},$$
$$\vec B = \sum_{\vec k, i} \sqrt{\frac{\hbar k}{2 c V \varepsilon_0}} (a_i(\vec k) - a_i^\dagger(-\vec k)) \frac{\vec k}{k} \times \vec n_i(\vec k) e^{\im \vec k \cdot \vec x}.$$
We can see straight away that $\bra{0} \vec E \ket{0}$ must be zero ($a_i$ acting on $\ket{0}$ produces 0, ditto for $a_i^\dagger$ acting on $\bra{0}$).
$\varepsilon_0 \langle E^2 \rangle$ is just the zero-point energy per unit volume, which is $\hbar \omega/(2V)$ per mode.
Since there are modes with arbitrarily high $\omega$, the result is infinite.
This is a sign that our theory should break down at high $\omega$ (which corresponds to short length scales, using $\lambda = 2\pi c/\omega$).

For an arbitrary stationary state $\ket{\psi}$ the number of photons is well-defined.
But the states $\vec a_i(\vec k) \ket{\psi}$ and $\vec a_i^\dagger(-\vec k) \ket{\psi}$ will have a different number of photons.
Since the number operator is Hermitian, they must be orthogonal to $\ket{\psi}$ (being eigenstates of different eigenvalue).
Hence $\bra{\psi} \vec E \ket{\psi} = 0$.
}
\end{question}

The stationary states seem nice, with their well defined $\hbar \omega$ energy, but we actually just found that they are rather bizarre classically -- since $\langle \vec E \rangle = 0$.
There are other states that aren't stationary states (so their energy is not well defined) but that will lead to more familiar forms for $\langle \vec E \rangle$.

\begin{question}
Find $\langle \vec E(t) \rangle$ for a state $\ket{\psi(t)}$ obeying $a_j(\vec k_0) \ket{\psi(0)} = \alpha \ket{\psi(0)}$ for one particular $j$ and $\vec k_0$ (and no photons with any other $i$ or $\vec k$).
You can assume $\alpha$ is real.

Find the expected number of photons in this state.
\hint{To find the time evolution, use the formula $e^{b} a e^{-b} = a + [b,a] + [b,[b,a]]/2! + [b,[b,[b,a]]]/3! + \dots$}
\result{$\langle \vec E \rangle = \sqrt{2 \hbar c k_0/(V \varepsilon_0)} \vec n_j(\vec k_0) \alpha \cos(\vec k_0 \cdot \vec x - c k_0 t)$.}
\solution{
To find the time evolution we can use
$$e^{\im H t/\hbar} a e^{-\im H t/\hbar} = a + [\im H t/\hbar,a] + [\im H t/\hbar,[\im H t/\hbar,a]]/2 + \dots = e^{-\im c k_0 t} a.$$
From this it folllows
$$a \ket{\psi(t)} = a e^{-\im H t/\hbar} \ket{\psi(0)} = e^{-\im c k_0 t} e^{-\im H t/\hbar} a \ket{\psi(0)} = e^{-\im c k_0 t} \alpha \ket{\psi(t)}.$$
Hence $\ket{\psi(t)}$ remains an eigenstate, but the eigenvalue $\alpha$ rotates through the complex plane with angular frequency $c k_0$.

To find $\langle \vec E \rangle$ use the expression
$$\langle \vec E \rangle = \sum_{\vec k, i} \sqrt{\frac{\hbar c k}{2 V \varepsilon_0}} \langle a_i(\vec k) + a_i^\dagger(-\vec k)\rangle \vec n_i(\vec k) e^{\im \vec k \cdot \vec x}.$$
Since $a_j(\vec k_0) \ket{\psi} = \alpha e^{-\im c k_0 t} \ket{\psi}$ and $\bra{\psi} a_j^\dagger(\vec k_0) = \alpha e^{\im c k_0 t} \bra{\psi}$, the expectation value is non-zero for $\vec k = \vec k_0$ and $\vec k = -\vec k_0$.
This leads to
$$\langle \vec E \rangle = \sqrt{\frac{\hbar c k_0}{2 V \varepsilon_0}} \vec n_j(\vec k_0) \left(\alpha e^{\im (\vec k_0 \cdot \vec x -c k_0 t)} + \alpha e^{-\im (\vec k_0 \cdot \vec x -c k_0 t)}\right) = \sqrt{\frac{2\hbar c k_0}{V \varepsilon_0}} \vec n_j(\vec k_0) \alpha \cos(\vec k_0 \cdot \vec x -c k_0 t),$$
this is a travelling wave of wavevector $\vec k_0$!

The expected number of photons is
$$\langle a_i^\dagger a_i \rangle = \bra{\psi} \alpha^* \alpha \ket{\psi} = |\alpha|^2.$$
}
\end{question}

We see that it is the eigenstates of $a$, not $H$, that corresponds most closely to typical classical states.
These states are called \emph{coherent states}.

\begin{question}
(optional) Continuing from the last question, calculate the energy using
$$\int_V \frac{1}{2} \varepsilon_0 \left(\langle \vec E\rangle^2  + c^2 \langle \vec B\rangle^2\right) \D{^3 \vec x}.$$
\result{$\hbar c k_0 \alpha^2$.}
\solution{
We can use one of the Maxwell equations to calculate
$$\langle \vec B\rangle = \int \D{t}~\nabla \times \langle \vec E\rangle = -\sqrt{\frac{2 \hbar c k_0}{V \varepsilon_0}} \alpha \frac{\vec k_0}{c k_0} \times\vec n_j(\vec k_0) \cos(\vec k_0 \cdot \vec x - c k_0 t).$$
Hence
$$\frac{1}{2} \varepsilon_0 \left(\langle \vec E\rangle^2 + c^2 \langle \vec B\rangle^2\right) = \frac{2\hbar c k_0 \alpha^2}{V \varepsilon_0} \cos^2(\vec k_0 \cdot \vec x - c k_0 t).$$
$\cos^2$ and $\sin^2$ average to $1/2$ and so the integral over all space is $\hbar c k_0 \alpha^2$.
Since the number of photons is $\alpha^2$, this is exactly what we expect from $E=\hbar \omega$.
}
\end{question}

Planck originally proposed the $E=\hbar \omega$ relation to explain his blackbody radiation formula, which we can now formally derive.
To derive this formula we will start with a slightly strange scenario:
A box (which should really have reflecting walls but we will use periodic boundary conditions instead) at temperature $T$.
The energy in the box at a given frequency $\omega$ is then given by
$$\mbox{energy per oscillator} \times \mbox{number of oscillators at $\omega$}.$$

\begin{question}
In thermal equilibrium states are occupied with probability proportional to $\exp(-E/(k_B T))$ (Boltzmann distribution).
Find a formula for the average energy of a single harmonic oscillator of frequency $\omega$ at temperature $T$.
\hint{You may find the geometric sum formula $1+q+q^2+\dots = 1/(1-q)$ useful.}
\hint{You may also want to take derivatives of geometric sum formula.}
\result{$\langle E \rangle = \hbar \omega / (e^{\hbar \omega/(k_B T)} - 1).$}
\solution{
The states have energy $0, \hbar \omega, 2 \hbar \omega, \dots$ (the zero-point energy has no effect here).
To normalise the probabilities we need to calculate
$$Z = \sum_n e^{-E_n/(k_B T)} = \sum_n e^{-n \hbar \omega/(k_B T)} = \sum_n \left(e^{-\hbar \omega/(k_B T)}\right)^n = \frac{1}{1 - e^{-\hbar \omega/(k_B T)}}.$$
We want to evaluate
$$\langle E \rangle = \sum_n \frac{n \hbar \omega e^{-n \hbar \omega/(k_B T)}}{Z}.$$
The quick way to evaluate this is to realise that it's just
$$\langle E \rangle = \frac{1}{Z} \pfrac{Z}{\beta}$$
with $\beta=1/(k_B T)$, which works out as
$$\langle E \rangle = \frac{1}{Z} \frac{\hbar \omega e^{-\hbar \omega/(k_B T)}}{(1 - e^{-\hbar \omega/(k_B T)})^2} = \frac{\hbar \omega}{e^{\hbar \omega/(k_B T)} - 1}.$$
}
\end{question}

\begin{question}
Now to evaluate the ``number of oscillators'' bit.
Calculate the number of $\vec k$ values with frequencies in the range $\omega$ to $\omega+\D{\omega}$.
Assume that $\omega$ is much greater than the spacing of $\vec k$ values $2\pi/L$.
From this, calculate the number of oscillators (electromagnetic field modes) in that energy range.
\result{The number of oscillators is $\omega^2 \D{\omega} V/(\pi^2 c^3).$}
\solution{
The appropriate values of $\vec k$ form a spherical shell in $\vec k$-space.
It has radius $\omega/c$ and thickness $\D{\omega}/c$.
Hence its volume is
$$4 \pi (\omega/c)^2 \D{\omega}/c.$$
The allowed $\vec k$ values are distributed over cubes of volume $(2\pi)^3/V$, hence the number of allowed $\vec k$ values is
$$\frac{\omega^2 \D{\omega} V}{2 \pi^2 c^3}.$$

There are two oscillators per $\vec k$ value (two polarisations), hence the final result is
$$\frac{\omega^2 \D{\omega} V}{\pi^2 c^3}.$$
}
\end{question}

\begin{question}
Calculate the energy per unit volume $u(\omega) \D{\omega}$ of the electromagnetic field with frequencies between $\omega$ and $\omega+\D{\omega}$ in the box at temperature $T$.

To apply the result to a black body (a body that absorbs 100\% of the incident radiation), place the black body in thermal equilibrium with the field.
Relate the power incident on the body with the power radiated to find the power $p(\omega) \D{\omega} \D{A}$ radiated by an area $\D{A}$ of the black body at temperature $T$ at frequencies between $\omega$ and $\omega+\D{\omega}$.
\hint{For the second part, how fast does electromagnetic energy move? Calculate the energy incident on the area within time $\D{t}$. Don't forget that it might move at an angle to the surface -- you'll need to integrate over all solid angles $\D{\Omega} = 2\pi \D{(\cos \theta)}$.}
\result{$p(\omega) = \hbar \omega^3/(4\pi^2 c^2 (e^{\hbar \omega/(k_B T)} - 1)).$}
\solution{
Multiplying the two previous results and dividing by $V$ gives the answer as
$$u(\omega) = \frac{\hbar \omega^3}{\pi^2 c^3} \frac{1}{e^{\hbar \omega/(k_B T)} - 1}.$$

For the black body case, remember that in equilibrium we must have
$$\mbox{power incident} = \mbox{power radiated}.$$
Hence we want to calculate the energy incident in a time $\D{t}$.
If $\theta$ is the angle to the surface, the speed relative to it is just $c \cos \theta$.
At an angle $\theta$ the amount of energy incident is then $c \cos \theta \D{A} \D{t} u$.
Now we need to average $\cos \theta$.
Only radiation moving towards the body counts and hence we only integrate from $\theta=0$ to $\theta=\pi/2$.
The average is
$$\frac{1}{4\pi} \int_0^1 \cos(\theta) 2\pi \D{(\cos(\theta))} = \frac{1}{4}$$
(a quicker way to see this is to realise that the normal component is uniformly distributed between $-1$ and 1).
Hence the answer we want (after dividing $\D{A}$ and $\D{\omega}$) is just $c u/4$, i.e.\
$$p(\omega) = \frac{\hbar \omega^3}{4 \pi^2 c^2} \frac{1}{e^{\hbar \omega/(k_B T)} - 1}.$$
}
\end{question}

\begin{question}
(optional)\\
1. (Rayleigh-Jeans law) Find an approximate expression for $p(\omega)$ at large $T/\omega$.\\
2. (Wien approximation) Repeat for small $T/\omega$.\\
3. (Wien's displacement law) Show that $p(\omega,T_1) = \alpha p(\gamma \omega, T_2)$ for appropriate $\alpha(T_1, T_2)$ and $\gamma(T_1, T_2)$. Use this result to find a simple expression for the coordinates $(\omega, p(\omega))$ of the maximum as a function of $T$ (with undetermined constants).
\result{1. $\omega^2 k_B T/(4 \pi^2 c^2)$.\\
2. $\hbar \omega^3/(4\pi^2 c^2)~e^{-\hbar \omega/(k_B T)}$.\\
3. $\alpha=(T_1/T_2)^3$ and $\gamma=T_2/T_1$. The maximum is at $(b T, d T^3)$.}
\solution{
1. $e^{\hbar \omega/(k_B T)} = 1 + \hbar \omega/(k_B T) + O(\omega^2/T^2)$ and hence
$$p(\omega) = \frac{\hbar \omega^3}{4\pi^2 c^2} \frac{k_B T}{\hbar \omega} + O(1) = \frac{\omega^2}{4\pi^2 c^2} k_B T + O(1).$$
Note that (if we take out the $c/4$ factor from the black body) the result is just $k_B T$ per mode (as expected from equipartition).\\
2.
At large $\omega/T$ we have $e^{\hbar \omega/(k_B T)} \gg 1$ and hence
$$p(\omega) \approx \frac{\hbar \omega^3}{4\pi^2 c^2} e^{-\hbar \omega/(k_B T)}.$$
3. 
$p(\omega,T_1) = \alpha p(\gamma \omega, T_2)$ leads to
$$\frac{\hbar \omega^3}{4\pi^2 c^2} \frac{1}{e^{\hbar \omega/(k_B T_1)} - 1} = \alpha \gamma^3 \frac{\hbar \omega^3}{4\pi^2 c^2} \frac{1}{e^{\hbar \gamma \omega/(k_B T_2)} - 1}.$$
To match up the exponents we need $\gamma = T_2/T_1$ and thus $\alpha = (T_1/T_2)^3$ and hence $p(\omega, T_1) = (T_1/T_2)^3 p((T_2/T_1) \omega, T_2)$.
If the maximum is at $(\omega_0, p_0)$ at $T_1$, it shifts to $((T_2/T_1) \omega_0, (T_2/T_1)^3 p_0)$ at $T_2$, which we can simplify to $(\omega, p(\omega)) = (b T, d T^3)$ with two constants $b$ and $d$.
Numerically one can evaluate
$$b = 2.82 \frac{k_B}{\hbar}, \quad d = 0.036 \frac{k_B^3}{c^2 \hbar^2}.$$
}
\end{question}

\begin{question}
(optional)
Calculate the total power radiated per unit area (over all frequencies) for a black body at temperature $T$.
\hint{You may find the formula $\sum 1/n^4 = \pi^4/90$ (that you may have derived in the complex analysis section) useful.}
\hint{Expand $1/(e^{\hbar \omega/(k_B T)} - 1)$ in a power series in $e^{-\hbar \omega/(k_B T)}$.}
\solution{
Define $\beta = k_B T$.
$$\int_0^\infty \frac{\omega^3 \D{\omega}}{e^{\beta \hbar \omega} - 1} = \int_0^\infty \D{\omega} \omega^3 e^{-\beta \hbar \omega} \sum_{n=0}^\infty (e^{-\beta \hbar \omega})^n = \sum_{n=1}^\infty \int_0^\infty \omega^3 e^{-n \beta \hbar \omega} \D{\omega}$$
$$= -\sum_{n=1}^\infty \frac{1}{n^3 \hbar^3} \pfrac{^3}{\beta^3} \int_0^\infty e^{-n \beta \hbar \omega} \D{\omega} = \frac{6}{\hbar^4 \beta^4} \sum_{n=1}^\infty \frac{1}{n^4} = \frac{\pi^4}{15 \hbar^4 \beta^4}$$
(using $\sum 1/n^4 = \pi^4/90$ in the last step).
Hence the total power radiated is
$$\int_0^\infty p(\omega) \D{\omega} = \frac{\pi^2 k_B^4}{60 \hbar^3 c^2} T^4.$$
This is the \emph{Stefan-Boltzmann law} $P = \sigma T^4$.
}
\end{question}

\subsection{Particles in three dimensions and angular momentum}
Consider a particle moving in three dimensions.
We can treat it as three combined 1D particles.
Rather than basis states $\ket{x}$ we now have basis states $\ket{x} \ket{y} \ket{z}$ which we can write as $\ket{\vec r}$ with a vector $\vec r = (x, y, z)$.
In 1D we had $\ket{\psi}$ as a linear combination of $\ket{x}$, i.e.\
$$\ket{\psi} = \int \psi(x)\ket{x}\D{x},$$
whereas in 3D we have
$$\ket{\psi} = \int \psi(x,y,z) \ket{x}\ket{y}\ket{z} \D{x}\,\D{y}\,\D{z} = \int \psi(\vec r) \ket{\vec r} \D{^3 \vec r}.$$

Each ``1D particle'' has a momentum operator, which means we have three momentum operators $p_x$, $p_y$, $p_z$ (which we could also derive directly from symmetry, using displacements in the $x$, $y$ and $z$ directions).
We can write this as a single ``vector operator'' $\vec p = (p_x, p_y, p_z)$.
This notation is a bit mindboggling: We have a 3D vector where each component is an operator, which acts on the infinite dimensional state vector!
We can similarly define a position operator $\vec r = (x,y,z)$.

Remember that we combining systems the operators corresponding to different systems permute, which means here $[x, y] = 0$, $[x, p_y] = 0$, etc.
We also preserve the commutation rules of operators within one system, hence $[x, p_x] = \im \hbar$.
We can write concisely
$$[r_i, r_j] = 0, \quad [p_i, p_j] = 0, \quad [r_i, p_j] = \im \hbar \delta_{i,j}.$$

\begin{question}
Write $\vec p$ as a differential operator and write down the time-independent Schr\"odinger equation for a particle moving in a 3D potential $V(\vec r)$.
\solution{
$$\vec p = -\im \hbar \nabla$$
$$-\frac{\hbar^2}{2m} \nabla^2 \psi(\vec r) + (V(\vec r) - E) \psi(\vec r) = 0$$
}
\end{question}

Things get more interesting now since we have a new symmetry to play with: rotations.

\begin{question}
Write down the equation for a rotation by $\theta$ around the $z$ axis, in the form
$$T_\theta \ket{x,y,z} = \ket{R_x(x,y,z,\theta), R_y(x,y,z,\theta), R_z(x,y,z,\theta)}.$$
Note that we use definitions such that rotating by $+\pi/2$ transforms $\ket{x,0,0}$ into $\ket{0,y,0}$.
Find the generator of rotation around $z$ and use it to find the operator $L_z$ corresponding to the $z$ component of angular momentum.
Write down $L_x$ and $L_y$.

Rewrite your answer using $\vec r$ and $\vec p$ operators.
\hint{Look back at how we derived $p$.}
\hint{To find $L_x$ and $L_y$ you can just permute $x \to y \to z \to x$.}
\solution{
$$R_x(x,y,z,\theta) = \cos \theta~x - \sin \theta~y,\quad R_y(x,y,z,\theta) = \sin \theta~x + \cos \theta~x,\quad R_z(x,y,z,\theta) = z.$$
To find the generator write
$$\bra{\vec r} T_\theta \ket{\psi} = \bra{\vec R(\vec r,-\theta)} \ket{\psi} = \psi(\vec R(\vec r, -\theta))$$
and hence
$$\bra{\vec r} G \ket{\psi} = \lim_{\theta \to 0} \frac{\psi(R(\vec r, -\theta)) - \psi(\vec r)}{\theta}.$$
For small $\theta$ we can expand
$$R_x(x,y,z,-\theta) = x + \theta y + O(\theta^2), \quad R_y(x,y,z,\theta) = y - \theta x + O(\theta^2), \quad R_z(x,y,z,\theta) = z$$
and thus
$$\psi(\vec R(\vec r, -\theta)) = \psi(\vec r) + \pfrac{\psi}{x} \theta y - \pfrac{\psi}{y} \theta x + O(\theta^2)$$
The effect of the generator is then
$$\bra{x,y,z} G \ket{\psi} = y \pfrac{\psi}{x} - x \pfrac{\psi}{y},$$
which implies we can write $G = y~\partial/\partial x - x~\partial/\partial y$.
Using our symmetry principle we then have
$$L_z = \im \hbar \left(y \pfrac{}{x} - x \pfrac{}{y}\right).$$
We can now permute $x \to y \to z \to x$ to write down
$$L_x = \im \hbar \left(z \pfrac{}{y} - y \pfrac{}{z}\right),\quad L_y = \im \hbar \left(x \pfrac{}{z} - z \pfrac{}{x}\right).$$
Since $\vec p = -\im \hbar \nabla$ we can write
$$L_x = y p_z - z p_y, \quad L_y = z p_x - x p_z, \quad L_z = x p_y - y p_x.$$
In vector notation $\vec L = \vec r \times \vec p$.
}
\end{question}

$\vec L$ turns out to be a unusual quantity.
First something relatively harmless that you may have seen in the old Bohr model of the atom.

\begin{question}
Using an earlier result we can write an arbitrary rotation around $z$ as an operator $e^{-\im L_z \theta/\hbar}$.
Show that $L_z$ is \emph{quantised}, i.e.\ that its eigenvalues are multiples of some quantity.
\hint{What happens when $\theta=2\pi$?}
\solution{
$\psi(\vec r)$ is an ordinary function and must return to itself after a rotation by $2\pi$.
Hence
$$e^{-2\pi \im L_z/\hbar} \ket{\psi} = \ket{\psi}.$$
If $\psi$ is an eigenvector of eigenvalue $\lambda$ this implies the eigenvalue obeys $e^{-2\pi \im \lambda/\hbar} = 1$, which means $\lambda$ is a multiple of $\hbar$.
}
\end{question}

\begin{question}
Find the commutator $[L_i, L_j]$.
Express your answer using the \emph{Levi-Civita tensor} $\varepsilon_{ijk}$ defined to be perfectly antisymmetric (i.e.\ change sign when two indices are switched) and to obey $\varepsilon_{123} = 1$.
\solution{
Start with
$$[L_x, L_y] = [y p_z - z p_y, z p_x - x p_z] = y [p_z, z] p_x - x [z, p_z] p_y = \im \hbar (x p_y - y  p_x) = \im \hbar L_z.$$
Note that switching $x$ and $y$ changes sign, whereas rotating $x \to y \to z \to x$ leaves the sign unchanged.
Hence
$$[L_i, L_j] = \im \hbar \varepsilon_{ijk} L_k$$
(where $k$ is implicitly summed over).
}
\end{question}

Note that $[L_x, L_y] \ne 0$ implies that $L_x$ and $L_y$ can't be measured simultaneously!
There is one exception:
\begin{question}
Define $L^2 = {L_x}^2 + {L_y}^2 + {L_z}^2$.
Show that $\langle L^2 \rangle = 0$ implies that the state is an eigenstate of $L_x$, $L_y$ and $L_z$ with eigenvalue 0.
Additionally, show that $\psi(\vec r)$ is spherically symmetrical.
\solution{
We know that
$$0 = \langle L^2 \rangle = \langle {L_x}^2 \rangle + \langle {L_y}^2 \rangle + \langle {L_z}^2 \rangle.$$
Since $\langle {L_i}^2 \rangle \ge 0$ this implies they must all be zero.
But $\bra{\psi} L_i L_i \ket{\psi} = \braket{\phi}{\phi}$ for $\ket{\phi} = L_i \ket{\psi}$.
Hence $\langle L_i \rangle = 0$ implies $0 = \ket{\phi} = L_i \ket{\phi}$.

Since angular momentum is the generator of rotations, for a finite rotation we have $e^{-\im L_i \theta/\hbar} \ket{\psi} = \ket{\psi}$ and thus the state is spherically symmetrical.
}
\end{question}

\begin{question}
Find $[L^2, L_i]$.
\solution{
You can either do this writing the commutator out in terms out $L_x$, $L_y$, $L_z$ or by writing
$$[L_j L_j, L_i] = 2 L_j [L_j, L_i] = 2 \im \hbar L_j \varepsilon_{jik} L_k.$$
Since $\varepsilon_{jik} = -\varepsilon_{kij}$ the result is equal to $= -2\im\hbar \varepsilon_{kij} L_k L_j$.
Now $k$ and $j$ are just summation indices and so can be swapped, hence the result must be 0.
}
\end{question}

We can thus use the pair $(L^2, L_z)$ to label angular momentum eigenstates (while we can use any direction, the $z$ axis is the conventional choice).
To find the eigenvalues we can do tricks similar to what we did with the harmonic oscillator.
We write $\ket{\ell,m}$ for the simultaneous eigenstate of $(L^2, L_z)$.
$m$ is just the eigenvalue of $L_z/\hbar$ (which we showed is an integer earlier).
$\ell$ is related a function of the eigenvalue of $L^2$ (which we will determine later).

If the arbitrary direction creeps you out, remember that an arbitrary state is \emph{not} an eigenstate of $L_z$ -- we can however \emph{expand} any state in terms of $(L^2, L_z)$ eigenstates and we should get the same physics whichever axis we expand about (it's exactly analogous to choosing a coordinate system in 3D space -- different choices will lead to different coordinates but the underlying physics is unchanged).

\begin{question}
Find a linear combination $L_+$ of the $L_x, L_y, L_z$ operators such that
$$L_+ \ket{\ell,m} \propto \ket{\ell, m+1}, \quad L_- \ket{\ell,m} \propto \ket{\ell, m-1},$$
where $L_- = L_+^\dagger$ (these relations still allow an overall factor. Fix it by setting the $L_x$ coefficient to 1).

Calculate $[L_+, L_-]$.
\hint{Calculate $[L_z, L_+] \ket{\ell,m}$.}
\solution{
Note that any operator based on $L_x$, $L_y$, $L_z$ must leave $\ell$ alone (they all commute with $L^2$).
$L_z$ would contribute a $m \hbar \ket{\ell, m}$ component, hence it's not part of $L_+$.
We want
$$[L_z, L_+] \ket{\ell,m} = L_z L_+ \ket{\ell,m} - L_+ L_z \ket{\ell,m}= \hbar L_+ \ket{\ell,m}$$
and similarly $[L_z, L_-] = -\hbar L_-$ (these relations are direct equivalents of the harmonic oscillator $[H, a^\dagger] = \hbar \omega a^\dagger$ and $[H, a] = -\hbar \omega a$).
Writing $L_+ = L_x + \beta L_y$ we have
$$[L_z, L_+] = [L_z, L_x] + \beta [L_z, L_y] = \im \hbar L_y - \im \hbar \beta L_x \stackrel{!}{=} \hbar L_x + \hbar \beta L_y,$$
$$[L_z, L_-] = [L_z, L_x] + \beta^* [L_z, L_y] = \im \hbar L_y - \im \hbar \beta^* L_x \stackrel{!}{=} -\hbar L_x - \hbar \beta^* L_y.$$
Comparing coefficients of $L_x$ and $L_y$ we have $\beta = \im$ and hence
$$L_+ = L_x + \im L_y.$$

Using this definition
$$[L_+, L_-] = [L_x + \im L_y, L_x - \im L_y] = \im [L_y, L_x] - \im [L_x, L_y] = -2\hbar L_z.$$
}
\end{question}

\begin{question}
Show that there is a solution to $L_+ \ket{\ell, m} = 0$ for every value of $\ell$.
Now \emph{define} $\ell$ by demanding that $L_+ \ket{\ell, \ell} = 0$ (i.e.\ $\ell$ is the maximum value of $m$).
Express $L^2$ in terms of $L_+$, $L_-$ and $L_z$ to find the eigenvalue of $L^2$.
\hint{What does $\langle L^2 \rangle = \langle L_x^2 \rangle + \langle L_y^2 \rangle + \langle L_z^2 \rangle$ imply about the eigenvalues of $L_z^2$ for a fixed value of $L^2$?}
\hint{For the last part, use $\ket{\ell,\ell}$ and the commutator $[L_+, L_-]$.}
\result{$\hbar^2 \ell(\ell+1).$}
\solution{
We have $\langle L_i^2 \rangle \ge 0$ and so
$$\langle L_z^2 \rangle = \langle L^2 \rangle - \langle L_x^2 \rangle - \langle L_y^2 \rangle \le \langle L^2 \rangle,$$
hence $m^2 \hbar^2$ must be smaller than the eigenvalue of $L^2$.
Thus it can't be infinite and $L_+ \ket{\ell,m}$ must terminate for some $m$.
We can write $L_x = (L_+ + L_-)/2$ and $L_y = -\im (L_+ - L_-)/2$ and so 
$$L^2 = (L_+ + L_-)^2 - (L_+ - L_-)^2 + L_z^2 = \frac{1}{2} (L_+ L_- + L_- L_+) + L_z^2.$$
Since $[L_+, L_-] = -2\hbar L_z$ we can rewrite this as
$$L^2 = L_- L_+ + \hbar L_z + L_z^2.$$
Since $L_+ \ket{\ell,\ell} = 0$ this implies
$$L^2 \ket{\ell,\ell} = (\ell \hbar^2 + \ell^2 \hbar^2) \ket{\ell,\ell} = \hbar^2 \ell(\ell+1) \ket{\ell,\ell}.$$
}
\end{question}

\begin{question}
Normalise all the $\ket{\ell,m}$ states and find the proportionality factors in
$$L_+ \ket{\ell,m} \propto \ket{\ell,m+1},\quad L_- \ket{\ell,m} \propto \ket{\ell,m-1}.$$
Find the minimum value of $m$.
\hint{Write $\bra{\ell,m} L_- L_+ \ket{\ell,m}$ and use $L^2 \ket{\ell,m} = \hbar^2 \ell (\ell+1) \ket{\ell,m}$.}
\result{
$L_+ \ket{\ell,m} = \hbar \sqrt{\ell(\ell+1) - m (m+1)} \ket{\ell,m+1}$,\\
$L_- \ket{\ell,m} = \hbar \sqrt{\ell(\ell+1) - m (m-1)} \ket{\ell,m-1}.$
}
\solution{
$$\bra{\ell,m} L_- L_+ \ket{\ell,m} = \bra{\ell,m} (L^2 - {L_z}^2 - \hbar L_z) \ket{\ell,m} = \hbar^2 \ell (\ell+1) - \hbar^2 m (m+1)$$
and hence
$$L_+ \ket{\ell,m} = \hbar \sqrt{\ell(\ell+1) - m (m+1)} \ket{\ell,m+1}$$
(note that this gives zero for $m=\ell$).
To find $L_- \ket{\ell,m}$ we can now use
$$\bra{\ell,m} L_+ L_- \ket{\ell,m} = \bra{\ell,m} (L_- L_+ + 2 \hbar L_z) \ket{\ell,m} = \hbar^2 (\ell(\ell+1) - m(m-1)),$$
thus
$$L_- \ket{\ell,m} = \hbar \sqrt{\ell(\ell+1) - m (m-1)} \ket{\ell,m-1}.$$
This gives zero for $m=-\ell$, which must be the minimum value (this result is also expected since we can just turn the system upside down to invert the value of $L_z$).

}
\end{question}

\begin{question}
Suppose we only care about $\ell=1$.
We then have a finite number of $L_z$ eigenstates and we can pretend they form a basis (mathematically we say that we operate in the ``subspace'' generated by the eigenstates).
Write $L_z$, $L_+$, $L_-$, $L_x$ and $L_y$ in this basis and find the $L_x$ and $L_y$ eigenstates (order the eigenstates of $L_z$ from highest to lowest $m$).
\hint{Remember that the columns of a matrix are just the results of applying the matrix to each basis vector.}
\solution{
We have three eigenstates $\ket{1,1}$, $\ket{1,0}$ and $\ket{1,-1}$.
$L_z$ is trivial, it's just
$$L_z = \hbar \begin{pmatrix}1 & 0 & 0\\0 & 0 & 0\\0 & 0 & -1\end{pmatrix}.$$
$L_+$ can be found using the formula
$$L_+ \ket{\ell,m} = \hbar \sqrt{\ell(\ell+1) - m (m+1)} \ket{\ell,m+1},$$
which can be evaluated to give $L_+ \ket{1,1} = 0$, $L_+ \ket{1,0} = \hbar \sqrt{2} \ket{1,1}$ and $L_+ \ket{1,-1} = \hbar \sqrt{2} \ket{1,0}$.
We thus have
$$L_+ = \hbar \begin{pmatrix}0 & \sqrt{2} & 0\\0 & 0 & \sqrt{2}\\0 & 0 & 0\end{pmatrix}, \quad
L_- = \hbar \begin{pmatrix}0 & 0 & 0\\\sqrt{2} & 0 & 0\\0 & \sqrt{2} & 0\end{pmatrix}.$$
We also know that $L_x = (L_+ + L_-)/2$ and $L_y = (L_+ - L_-)/(2\im)$ and hence
$$L_x = \frac{\hbar}{2} \begin{pmatrix}0 & \sqrt{2} & 0\\\sqrt{2} & 0 & \sqrt{2}\\0 & \sqrt{2} & 0\end{pmatrix}, \quad
L_y = \frac{\im \hbar}{2} \begin{pmatrix}0 & -\sqrt{2} & 0\\\sqrt{2} & 0 & -\sqrt{2}\\0 & \sqrt{2} & 0\end{pmatrix}.$$
The $L_x$ eigenstates are $(1, \sqrt{2}, 1)/2$, $(1,0,-1)/\sqrt{2}$ and $(1, -\sqrt{2}, 1)/2$.
The $L_y$ eigenstates are $(1, \sqrt{2}, 1)/2$, $(1,0,-1)/\sqrt{2}$ and $(1, -\sqrt{2}, 1)/2$.
}
\end{question}

Note that with the exception of our proof that $m$ must be an integer we have used \emph{only} the commutation relations.
The results are thus valid for \emph{any} three operators satisfying $[J_i, J_j] = \im \hbar \varepsilon_{ijk} J_k$.

\begin{question}
Let $J_i$ be such an operator, with no further restrictions.
Write eigenstates as $\ket{j,m}$ with $J^2 \ket{j,m} = \hbar^2 j(j+1) \ket{j,m}$ and $J_z \ket{j,m} = \hbar m \ket{j,m}$.
Use our previously derived results to show that $m$ (and thus $j$) must be either integer or half-integer (e.g.\ $1/2$, $3/2$, etc.).
\hint{How does $J_-$ affect $m$? Can you use $J_-$ to get from maximum $m$ to minimum $m$?}
\solution{
$J_- J_- J_- \dots \ket{j,j}$ has to give 0 for some number of $J_-$, hence
$$J_- \ket{j,j-n} = 0.$$
But we also know that
$$J_- \ket{j,j-n} = \hbar \sqrt{j(j+1) - (j-n)(j-n-1)} \ket{j,j-n-1}.$$
We know that this gives 0 for $j-n=-j$, hence $n = 2j$.
But $n$ must be an integer and hence $j$ is either integer of half-integer.
}
\end{question}

Going back to $L$ we can actually find the eigenstates as wavefunctions (usually called spherical harmonics).
Finding the actual functional form is rather tedious.
The next few questions go through the derivation but feel free to skip them.
The important thing to remember is that the spherical harmonics $Y_\ell^m(\theta, \phi)$ are defined as normalised eigenstates of $J^2$ and $J_z$.
When needed, you can usually look up the exact functional form for some specific value of $\ell$ and $m$.

\begin{question}
(optional)
Use spherical polar coordinates $(r,\theta,\phi)$ defined by $(x,y,z) = (r\sin \theta \cos \phi, r \sin \theta \sin \phi, r \cos \phi)$.
Find expressions for $L_x$, $L_y$, $L_z$ and $L^2$ using the differential operators $\partial/\partial r$, $\partial/\partial \theta$ and $\partial/\partial \phi$.
\hint{You can find $L_i$ from their definition as the generators of rotations. To find $L^2$ you may find the expression
$$\nabla^2 = \frac{1}{r^2} \pfrac{}{r} r^2 \pfrac{}{r} + \frac{1}{r^2 \sin \theta} \pfrac{}{\theta} \sin \theta \pfrac{}{\theta} + \frac{1}{r^2 \sin^2 \theta} \pfrac{}{\phi}.$$
useful.}
\result{
$L_z = -\im \hbar \pfrac{}{\theta},$
$$L^2 = -\hbar^2 \left(\frac{1}{\sin \theta} \pfrac{}{\theta} \sin \theta \pfrac{}{\theta} + \frac{1}{\sin^2 \theta} \pfrac{}{\phi}\right).$$
}
\solution{
Finding $L_x$, $L_y$ and $L_z$ is easiest from their definition as the generator of rotations.
Rotating $\varepsilon$ around $z$ in polar coordinates just means $\phi \to \phi + \varepsilon$.
Hence
$$L_z = -\im \hbar \pfrac{}{\phi}.$$
An infinitesimal rotation $\varepsilon$ around $x$ corresponds to $(x,y,z) \to (x,y-\varepsilon z, z+\varepsilon y)$.
On spherical polars the effect is to go to
$$(r \sin \theta \cos \phi, r \sin \theta \sin \phi - \varepsilon r \cos \theta, r \cos \theta + \varepsilon r \sin \theta \sin \phi)$$
We can reproduce the change in $z$ by going $\theta \to \theta - \varepsilon \sin \phi$.
This changes $x$ by $-\varepsilon r \cos \theta \sin \phi \cos \phi$, which we can cancel by doing $\phi \to \phi - \varepsilon \cot \theta \cos \phi$.
Hence
$$L_x = \im \hbar \left(\sin \phi \pfrac{}{\theta} + \cot \theta \cos \phi \pfrac{}{\phi}\right).$$
We can obtain the expression for $L_y$ by substituting $\phi \to \phi - \pi/2$, which gives
$$L_y = \im \hbar \left(-\cos \phi \pfrac{}{\theta} + \cot \theta \sin \phi \pfrac{}{\phi}\right).$$
To find $L^2 = -\hbar^2 (\vec r \times \nabla)^2$ one way is to realise that the equation for an infinitesimal length element is $\D{s}^2 = \D{r}^2 + (r \D{\theta})^2 + (r \sin \theta \D{\phi})^2$
and we can hence regard $\partial/\partial r$, $(1/r)\,\partial/\partial \theta$ and $(1/(r\sin\theta))\,\partial/\partial \phi$ as components of a vector $\nabla$.
The length of the vector $\vec r \times \nabla$ then corresponds to just the length of the angular portion of $r \nabla$, i.e.\ once squared we get
$$(\vec r \times \nabla)^2 = -(\vec r \times \nabla)^\dagger (\vec r \times \nabla) = -\left(\left(\pfrac{}{\theta}\right)^\dagger \pfrac{}{\theta} + \left(\pfrac{}{\phi}\right)^\dagger \frac{1}{\sin^2 \theta} \pfrac{}{\phi}\right),$$
where we have to be careful since $^\dagger \ne -$ for the spherical derivatives, as can be seen from
$$\int \pfrac{\psi^*}{\theta} \psi~r^2 \sin \theta \D{r} \D{\theta} \D{\phi} = -\int \psi^* \pfrac{(\psi \sin \theta)}{r} r^2 \D{r} \D{\theta} \D{\phi}$$
and so
$$\left(\pfrac{}{\theta}\right)^\dagger = -\frac{1}{\sin \theta} \pfrac{}{\theta} \sin \theta, \quad \left(\pfrac{}{\phi}\right)^\dagger = -\pfrac{}{\phi}.$$
Thus
$$L^2 = -\hbar^2 \left(\frac{1}{\sin \theta} \pfrac{}{\theta} \sin \theta \pfrac{}{\theta} + \frac{1}{\sin^2 \theta} \pfrac{}{\phi}\right).$$
}
\end{question}

\begin{question}
(optional) Write $L^2 \ket{\psi} = \hbar^2 \ell(\ell+1) \ket{\psi}$ and $L_z \ket{\psi} = \hbar m \ket{\psi}$ as differential equations in $\psi(r,\theta,\phi)$.
Make the substitution $\psi(r, \theta, \psi) = f(r) g_m(\phi) P^m_\ell(\cos \theta)$ (the $m$ in $P^m_\ell$ is just a label, not an exponent).
Solve for $g_m(\phi)$ and write down a differential equation for $P^m_\ell(x)$ (the ``associated Legendre polynomial''; the name is common even though it's not a polynomial for odd $m$).
\result{
$$\Dfrac{}{x} \left((1-x^2) \Dfrac{P^m_\ell}{x}\right) + \left(\ell(\ell+1) - \frac{m^2}{1-x^2}\right) P^m_\ell(x) = 0.$$
}
\solution{
The differential equations come out as
$$\pfrac{\psi}{\phi} = \im \psi, \quad \frac{1}{\sin \theta} \pfrac{}{\theta} \sin \theta \pfrac{}{\theta} \psi + \frac{1}{\sin^2 \theta} \pfrac{^2}{\phi^2} \psi + \ell(\ell+1) \psi = 0.$$
Making the substitution they separate into $\D{g_m}/\D{\phi} = \im m \phi$, which just means $g_m(\phi) = e^{\im m \phi}$ and
$$\frac{1}{\sin \theta} \Dfrac{}{\theta} \sin \theta \Dfrac{}{\theta} P^m_\ell(\cos \theta) + \left(\ell(\ell+1) - \frac{m^2}{\sin^2 \theta}\right) P^m_\ell(\cos \theta) = 0.$$
In terms of $x = \cos \theta$ we have
$$\Dfrac{}{\theta} = \Dfrac{x}{\theta} \Dfrac{}{x} = -\sin \theta \Dfrac{}{x} = -\sqrt{1-x^2} \Dfrac{}{x}$$
and thus
$$\Dfrac{}{x} \left((1-x^2) \Dfrac{P^m_\ell}{x}\right) + \left(\ell(\ell+1) - \frac{m^2}{1-x^2}\right) P^m_\ell(x) = 0.$$
}
\end{question}

\begin{question}
(optional) We can now use the ladder operators $L_+$ and $L_-$ to go between diffferent $m$ states.
Write down $L_+$ and apply it to $\psi$ to express $P^{m+1}_\ell$ in terms of $P^m_\ell$.
Your answer should have the form $f(x,m) \Dfrac{}{x} (g(x,m) P^{m+1}_\ell(x))$.
Repeat for $L_-$ (expressing $P^{m-1}_\ell$ in terms of $P^m_\ell$).
Ignore overall normalisation factors.
\result{ (Constant factors dropped)
$$P_\ell^{m+1}(x) = (\sqrt{1 - x^2})^{m+1} \Dfrac{}{x} (\sqrt{1 - x^2})^{-m} P_\ell^{m}(x).$$
$$P_\ell^{m-1}(x) = (\sqrt{1 - x^2})^{-m+1} \Dfrac{}{x} (\sqrt{1 - x^2})^{m} P_\ell^{m}(x).$$
}
\solution{
$$L_+ = \im \hbar \left(-\im e^{\im \phi} \pfrac{}{\theta} + e^{\im \phi} \cot \theta \pfrac{}{\phi}\right)$$
The effect on $\psi = f(r) e^{\im m \phi} P_\ell^m$ is
$$L_+ \psi = \hbar f(r) e^{\im (m+1)\phi} \left(\Dfrac{P_\ell^m}{\theta} - m \cot \theta P_\ell^m\right)$$
Hence we can write (dropping some overall factors)
$$P_\ell^{m+1}(x) = \sqrt{1 - x^2} \Dfrac{P_\ell^m(x)}{x} + m \frac{x}{\sqrt{1 - x^2}} P_\ell^{m}(x).$$
We can rewrite this as
$$P_\ell^{m+1}(x) = (\sqrt{1 - x^2})^{m+1} \Dfrac{}{x} (\sqrt{1 - x^2})^{-m} P_\ell^{m}(x).$$
In exactly the same way you can derive
$$P_\ell^{m-1}(x) = (\sqrt{1 - x^2})^{-m+1} \Dfrac{}{x} (\sqrt{1 - x^2})^{m} P_\ell^{m}(x).$$
}
\end{question}

\begin{question}
(optional) Using the results from the previous question, determine $P^{-\ell}_\ell$ (up to an overall factor).
From this, deduce a formula (containing differential operators) for $P_\ell^m$ for arbitrary $m$.
\hint{What happens when you do $L_- \ket{\ell, -\ell}$?}
\result{
$$P_\ell^{-\ell}(x) = (\sqrt{1-x^2})^{\ell},$$
$$P_\ell^m(x) = (\sqrt{1 - x^2})^{m} \Dfrac{^{\ell+m}}{x^{\ell+m}} (1-x^2)^{\ell}.$$
}
\solution{
We know that $L_- P^\ell_\ell = 0$
Hence
$$(\sqrt{1 - x^2})^{\ell+1} \Dfrac{}{x} (\sqrt{1 - x^2})^{-\ell} P_\ell^{\ell}(x) = 0,$$
which implies
$$P_\ell^{-\ell}(x) = (\sqrt{1-x^2})^{\ell}$$
(setting the overall constant to 1).
We can then find all the other $P_\ell^m$ by repeated application of
$$P_\ell^{m+1}(x) = (\sqrt{1 - x^2})^{m+1} \Dfrac{}{x} (\sqrt{1 - x^2})^{-m} P_\ell^{m}(x).$$
Applying the formula to $-\ell$, $-\ell+1$, \dots, $m-1$, the square root factors between differential operators disappear and we arrive at
$$P_\ell^m(x) = (\sqrt{1 - x^2})^{m} \Dfrac{^{\ell+m}}{x^{\ell+m}} (1-x^2)^{\ell}$$
(this formula is known as Rodrigues' formula; it's the same name as the one for the harmonic oscillator, though this one came first).
}
\end{question}

\begin{question}
(super optional, tricky) All that's left to do now is to specify an overall normalisation.
We add a factor $(-1)^m c_\ell$ to the Rodrigues' formula from the previous question.
Find $c_\ell$ so that $P^0_\ell(1) = 1$.
Define $Y^m_\ell(\theta,\phi) = d_{\ell,m} e^{\im m \phi} P^m_\ell(\cos \theta)$ and find $d_{\ell,m} > 0$ so that
$$\int |Y^m_\ell|^2 \sin^2 \theta \D{\theta} = 1.$$
\hint{To find $c_\ell$ think about what happens when you do the differentiation in Rodrigues' formula and then set $x=1$.}
\hint{To normalise $Y_\ell^m$ normalise it for $m=-\ell$ and then use the earlier formul\ae\ for $\ket{\ell,m+1}$ using ladder operators.}
\hint{To get the integral for $m=-\ell$ integrate by parts repeatedly until you get back to $\ell=0$. Some useful formul\ae: $1 \cdot 3 \cdots (2\ell+1) = (2\ell+2)!/(2^{\ell+1} (\ell+1)!)$ and $2 \cdot 4 \cdot 6 \cdots 2\ell = 2^\ell \ell!$.}
\result{
$$P_\ell^m(x) = \frac{(-1)^{\ell+m}}{2^\ell \ell!} (\sqrt{1 - x^2})^{m} \Dfrac{^{\ell+m}}{x^{\ell+m}} (1-x^2)^{\ell}.$$
$$Y_\ell^m(\theta, \phi) = \sqrt{\frac{(2\ell+1)(\ell-m)!}{4\pi(\ell+m)!}} e^{\im m \phi} P_\ell^m(\cos \theta).$$
}
\solution{
If you differentiate $(1-x^2)^\ell$, you get $\ell (1-x^2)^{\ell-1} (-2x)$.
If you now differentiate a second time, you get $\ell (\ell-1) (1-x)^{\ell-2} (-2x)^2 + \mbox{stuff}\times (1-x^2)^{\ell-1}$.
Continuing this way $\ell$ times we get
$$\ell! (-2x)^\ell + \mbox{terms with $(1-x^2)^i$ in them}.$$
Setting $x=1$ makes all the secondary terms disappear.
Hence if we set $c_\ell = (-1)^\ell/(2^\ell \ell!)$ we get $P^0_\ell(1) = 1$ as desired.
To normalise $Y_\ell^m$ we start with $m=-\ell$.
In that case we know that
$$P_\ell^{-\ell} = \frac{1}{2^\ell \ell!} (1-x^2)^{\ell/2}$$
and hence
$$\int_0^\pi (P_\ell^{-\ell}(\cos \theta))^2 \sin \theta \D{\theta} = \int_{-1}^1 (P_\ell^{-\ell}(x))^2 \D{x} = \frac{1}{(2^\ell \ell!)^2} \int_{-1}^1 (1-x^2)^\ell \D{x}$$
To find the integral integrate by parts to get
$$\int_{-1}^1 (1-x^2)^\ell \D{x} = \int_{-1}^1 2\ell x^2 (1-x^2)^\ell \D{x} = 2 \ell \int_{-1}^1 (1-x^2)^{\ell-1} \D{x} - 2 \ell \int_{-1}^1 (1-x^2)^\ell \D{x}$$
and hence
$$\int_{-1}^1 (1-x^2)^\ell \D{x} = \frac{2 \ell}{2\ell+1} \int_{-1}^1 (1-x^2)^{\ell-1} \D{x} = 2 \frac{(2\ell)!!}{(2\ell+1)!!} = 2 \frac{2^\ell \ell!}{(2\ell+2)!/(2^{\ell+1} (\ell+1)!)},$$
since the integral is 2 for $\ell=0$.
Now, since we dropped the factors when deriving $P_\ell^m$, we know that the integral should get bigger by $\ell(\ell+1)-m(m+1) = (\ell - m)(\ell+m+1)$ every time we raise with $m$.
The product of that from $m=-\ell$ to $m-1$ is $((2\ell)!/(\ell-m)!) (\ell+m)!$.
Hence we get
$$\int_{-1}^1 (P_\ell^{m}(x))^2 \D{x} = 2 \frac{1}{2^\ell \ell!} \frac{2^{\ell+1} (\ell+1)!}{(2\ell+2)!} \frac{(2\ell)! (\ell+m)!}{(\ell-m)!} = \frac{2}{2\ell+1} \frac{(\ell+m)!}{(\ell-m)!}.$$
The $\phi$ integral just gives $2\pi$ and so the correctly normalised spherical harmonics are
$$Y_\ell^m(\theta, \phi) = \sqrt{\frac{(2\ell+1)(\ell-m)!}{4\pi(\ell+m)!}} e^{\im m \phi} P_\ell^m(\cos \theta).$$
}
\end{question}

Now back to physics.
Consider a particle moving in a spherically symmetrical potential $V(\vec r) = V(r)$.
\begin{question}
Show that the stationary states can be written as $\ket{n,\ell,m}$ with the corresponding wavefunction $u_{n,\ell}(r) Y_\ell^m(\theta, \phi)$ and find a differential equation for $u(r)$.
Can you interpret the extra $\ell$-dependent term?
\hint{You may find the expression
$$\nabla^2 = \frac{1}{r^2} \pfrac{}{r} r^2 \pfrac{}{r} - \frac{L^2}{\hbar^2 r^2}$$
useful.
}
\hint{To interpret the extra term, write it in terms of $L$. Treating it as a classical potential, find the force and write it in terms of $m$, $v$ and $r$.}
\solution{
$H$ commutes with rotations and hence with $L_z$ and $L^2$.
This means they are conserved quantities and we can label the eigenstates with them, i.e.\ we can demand
$$L_z \ket{n,\ell,m} = m \hbar \ket{n,\ell,m},\quad L^2 \ket{n,\ell,m} = \ell(\ell+1) \hbar^2 \ket{n,\ell,m}.$$
This means for fixed $r$ the wavefunction $\psi$ is proportional to spherical harmonic, i.e. $\psi = u_{n,\ell,m}(r) Y_\ell^m(\theta,\phi)$.
The Schr\"odinger equation is
$$-\frac{\hbar^2}{2\mu} \nabla^2 \psi + (V(r) - E) \psi = 0$$
(writing $\mu$ for the mass, since $m$ is taken).
Using the expansion
$$\nabla^2 = \frac{1}{r^2} \pfrac{}{r} r^2 \pfrac{}{r} - \frac{L^2}{\hbar^2 r^2}$$
we can write
$$-\frac{\hbar^2}{2\mu} \frac{1}{r^2} \pfrac{}{r} r^2 \pfrac{}{r} u(r) + \frac{\hbar^2 \ell(\ell+1)}{2\mu r^2} u(r) + (V(r) - E) u(r) = 0.$$
This equation is called the \emph{radial equation}.
Note that there is no $m$ in this equation and hence neither $E$ nor $u(r)$ can depend on it.
To interpret the $L^2$ term calculate the classical force $\D{V}/\D{r}$ to find $L^2/(\mu r^3) = \mu v^2/r$: the centrifugal force!
}
\end{question}

\begin{question}
Show that $Y_\ell^m$ has parity $(-1)^\ell$, i.e.\ that
$$P\ket{\ell,m} = (-1)^\ell \ket{\ell,m},$$
where $P$ maps $(x,y,z)$ to $(-x,-y,-z)$.
\hint{All you need for this question is the form $Y_\ell^m \propto e^{\im m \phi} P_\ell^m(\cos \theta)$ and
$$P_\ell^m(x) = (\sqrt{1 - x^2})^{m} \Dfrac{^{\ell+m}}{x^{\ell+m}} (1-x^2)^{\ell}.$$
}
\hint{It might also be useful to remember that derivatives flip the parity.}
\solution{
$P$ corresponds to $\phi \to \phi + \pi$ and $\theta \to \pi - \theta$,which corresponds to $x \to -x$.
$(1-x^2)^\ell$ is an even function and so taking $\ell+m$ derivatives, $P_\ell^m(x)$ has parity $(-1)^{\ell+m}$.
$e^{\im m \phi}$ has parity $(-1)^m$ and so $Y_\ell^m$ has parity $(-1)^\ell$.
}
\end{question}

\begin{question}
(optional)
Show that the impure state
$$\varrho = \sum_{m=-\ell}^\ell \ket{\ell,m} \bra{\ell,m},$$
is invariant under rotations.
Use the result to find an expression for
$$\sum_{\ell=-m}^m Y_\ell^{m*}(\vec r\,') Y_\ell^m(\vec r),$$
where $\vec r$ and $\vec r\,'$ are unit vectors.
\hint{Remember that a transformation $U$ on a density matrix changes $\varrho$ to $U \varrho U^\dagger$. Use this result with infinitesimal rotations.}
\hint{Also remember $L_+$ and $L_-$.}
\hint{To do the second part, note that the rotational invariance means that you can rotate $\vec r\,'$ to an arbitrary direction (hopefully one that makes your life easier). You might find it useful to know that $P_\ell^m(1) = \delta_{m0}$. If you write your answer using vectors, you can argue that it will hold universally.}
\result{
$$\sum_{\ell=-m}^m Y_\ell^{m*}(\vec r\,') Y_\ell^m(\vec r) = \frac{2\ell+1}{4\pi} P_\ell^0(\vec r \cdot \vec r\,').$$
}
\solution{
Under an infinitesimal rotation $\varrho$ changes to
$$(1+\im \varepsilon L_i/\hbar) \varrho (1 - \im \varepsilon L_i/\hbar) = \varrho + \im \varepsilon [L_i, \varrho]/\hbar + O(\varepsilon^2).$$
We thus need to show that $\varrho$ commutes with $L_i$.
It obviously commutes with $L_z$, but we also have
$$L_+ \varrho = \sum_{m=-\ell}^\ell L_+ \ket{\ell,m} \bra{\ell,m} = \hbar \sum_{m=-\ell}^{\ell-1} \sqrt{\ell(\ell+1)-m(m+1)} \ket{\ell,m+1} \bra{\ell,m}.$$
Here we can shift $m \to m-1$ to arrive at
$$= \hbar \sum_{m=1-\ell}^\ell \sqrt{\ell(\ell+1)-(m-1)} \ket{\ell,m} \bra{\ell,m-1} = \varrho L_+.$$
Hence $L_+$ commutes with $\varrho$.
Since $(L_- \varrho)^\dagger = \varrho L_+$, so does $L_-$, as well as the linear combinations $L_x$ and $L_y$.

We can now take the expression
$$\sum_{\ell=-m}^m Y_\ell^{m*}(\vec r\,') Y_\ell^m(\vec r)$$
(which is just $\varrho$ expressed in terms of wavefunctions) and rotate it so that $\vec r\,'$ points in the $+z$ direction.
For $m \ne 0$, $Y_\ell^m$ rotates in the complex plane as $\phi$ is increased.
Hence at north pole ($\theta=0$), it must be 0, since $\phi$ is ill-defined there.
For $m = 0$, by definition, $P_\ell^0 = 1$.
We therefore have
$$\sum_{\ell=-m}^m Y_\ell^{m*}(\vec r\,') Y_\ell^m(\vec r) \propto Y_\ell^0(\cos \theta, 0) \propto P_\ell^0(\cos \theta) = P_\ell^0(\vec r \cdot \vec r\,').$$
Now the magic happens: since the left hand side is invariant under rotations and the right hand side is invariant under rotations and they are both equal in one coordinate system, \emph{the two must be equal in all coordinate systems}.
Note that this is not true for the two expressions in the middle, which are not invariant under rotations.
To find the overall normalisation substitute $\vec r=\vec r'$ and integrate $\vec r$ over the unit sphere. The right-hand side evaluates to $4\pi$ (since $P_\ell^0(1) = 1$) and the left-hand side evaluates to $2\ell+1$ (since we assume each $Y_\ell^m$ is normalised).
This leads to
$$\sum_{\ell=-m}^m Y_\ell^{m*}(\vec r\,') Y_\ell^m(\vec r) = \frac{2\ell+1}{4\pi} P_\ell^0(\vec r \cdot \vec r\,').$$
}
\end{question}
